{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phillipguo/opt/anaconda3/envs/pyt/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from calculate_mask import calculate_mask\n",
    "from inverse_hessian import calc_inverse_hessian\n",
    "from input_prehooks import put_input_hooks\n",
    "from testing_module import calculate_perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_size = \"opt-125m\"\n",
    "\n",
    "model_name = f\"facebook/{model_size}\"\n",
    "# model_name = \"facebook/opt-1.3b\"\n",
    "\n",
    "#Load dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', streaming=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calibrate model (get inputs to each layer with calibration data)\n",
    "\n",
    "calibration_size=4\n",
    "token_length=512\n",
    "calibration_batch_size=2\n",
    "\n",
    "EPSILON = 1e-8\n",
    "B = 4\n",
    "Bs = 2\n",
    "\n",
    "# run model on batches of calibration data, then concatenate inputs\n",
    "def split_model_calibration(model):\n",
    "    batch_sentences = []\n",
    "    for i, data in tqdm(enumerate(iter(dataset['train'])), total=calibration_size):\n",
    "        if i < calibration_size + 1:\n",
    "            if len(batch_sentences) >= calibration_batch_size:\n",
    "                with torch.no_grad():\n",
    "                    encoded_input = tokenizer(batch_sentences, return_tensors=\"pt\",\n",
    "                                              padding=\"max_length\", max_length=token_length,\n",
    "                                              truncation=True).to(device=device)\n",
    "                    model(**encoded_input, labels=encoded_input.input_ids)\n",
    "                    batch_sentences = []\n",
    "            batch_sentences.append(data['text'])\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get module name from parameter name\n",
    "def get_module_name(param_name):\n",
    "    if param_name[-5:] == \".bias\":\n",
    "        return param_name[:-5], \"bias\"\n",
    "    elif param_name[-7:] == \".weight\":\n",
    "        return param_name[:-7], \"weight\"\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "\n",
    "'''\n",
    "W is weights matrix for one layer\n",
    "H_inv is inverse hessian for one layer\n",
    "p is proportion of weights we're pruning (top p%)\n",
    "B is lazy block size (low B helps to reduce memory use)\n",
    "Bs is inverse of how often to make masks (e.g. when Bs is 4, make new masks with specified sparseness for every 4 columns)\n",
    "'''\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def prop_zeros(tens):\n",
    "    return torch.sum(tens == 0) / torch.numel(tens)\n",
    "\n",
    "# calculate new mask of previously masked model\n",
    "def calculate_mask(\n",
    "    W,\n",
    "    H_inv,\n",
    "    p,\n",
    "    B,\n",
    "    Bs,\n",
    "    ):\n",
    "\n",
    "    # Get the number of rows and columns in W\n",
    "    (d_row, d_col) = W.shape\n",
    "\n",
    "    # Initialize the pruning mask M and block quantization errors E to all zeros\n",
    "\n",
    "    M = torch.zeros(d_row, d_col, dtype=torch.bool).to(device=device)\n",
    "    E = torch.zeros(d_row, B, dtype=torch.float64).to(device=device)\n",
    "\n",
    "    # previous mask, check where weights are 0 (don't want to update/select these weights)\n",
    "    prev_mask = (W != 0)\n",
    "    # add up tot number of weights already masked\n",
    "    prev_num_masked = torch.sum(~prev_mask)\n",
    "    # print(f\"num weights masked: {prev_num_masked}\")\n",
    "    # print(f\"total weights: {W.numel}\")\n",
    "    # update proportion to prune, accounting for not pruning masked weights\n",
    "    # print(f\"original p: {p}\")\n",
    "    p *= (torch.numel(W) - prev_num_masked)/torch.numel(W)\n",
    "    # p = 1 - (1-p) * (torch.numel(W) - prev_num_masked)/torch.numel(W)\n",
    "    # print(f\"new p: {p}\")\n",
    "\n",
    "    # only need to calculate w_square and h_square once\n",
    "    # Loop over blocks of columns of W (as specified by B)\n",
    "\n",
    "    for i in range(0, d_col, B):\n",
    "\n",
    "        # Loop over columns within a block\n",
    "        for j in range(i, min(i + B, d_col)):\n",
    "\n",
    "            # If j is a multiple of Bs, prune a portion of the weights\n",
    "            if j % Bs == 0:\n",
    "                # Get the mask for the largest (1 - p)% of weights based on squared value and inverse hessian\n",
    "                # ASTERISK: prune_values is matrix of w^2/H^(-1)_cc\n",
    "\n",
    "                # Finding respective sections of hessian and weights matrix\n",
    "                w_square_section = torch.square(W[:, j:j + Bs])\n",
    "                h_square_section = torch.square(H_inv[j:j + Bs, j:j + Bs]).diag() # 1 dimensional vector\n",
    "\n",
    "                prev_mask_section = prev_mask[:, j:j + Bs]\n",
    "\n",
    "                # getting the prune values matrix from W and H^-1 sections\n",
    "                prune_values = w_square_section / h_square_section.unsqueeze(0)\n",
    "\n",
    "                # set prune_values of already-pruned weights to 0 (to not select them)\n",
    "                prune_values = prune_values * prev_mask_section\n",
    "\n",
    "                num_el_prune = int(p * prune_values.numel())\n",
    "\n",
    "                cutoff_value = torch.topk(prune_values.flatten(), num_el_prune, largest=True).values[-1]\n",
    "\n",
    "                #getting the final mask\n",
    "                mask = prune_values > cutoff_value\n",
    "\n",
    "                # print(f\"num zeros in new mask: {prop_zeros(mask)}\")\n",
    "                #masking\n",
    "                M[:, j:j + Bs] = mask * prev_mask_section\n",
    "                # print(f\"num zeros in new mask with old mask: {prop_zeros(M[:, j:j + Bs])}\")\n",
    "\n",
    "            # Calculate the pruning error for this column\n",
    "            E[:, j - i] = W[:, j] / H_inv[j, j]\n",
    "\n",
    "            # Freeze the weights that are not pruned by multiplying by the pruning mask\n",
    "            E[:, j - i] = (~M[:, j]) * E[:, j - i]\n",
    "\n",
    "            # Also freeze previously masked weights\n",
    "            # E[:, j - i] = prev_mask[: j] * E[:, j - i]\n",
    "\n",
    "            W[:, j:i + B] -= torch.ger(E[:, j - i], H_inv[j, j:i + B])\n",
    "            \n",
    "        # Update all remaining weights\n",
    "        W[:, i + B:] -= torch.matmul(E, H_inv[i:i + B, i + B:])\n",
    "\n",
    "    # print(f\"Proportion of Mask 0s: {prop_zeros(M)}\")\n",
    "    # return mask\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input_prehooks import get_feature_storage_name\n",
    "import gc\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils import prune\n",
    "from inverse_hessian import calc_inverse_hessian\n",
    "# import calculate_mask\n",
    "# import iterative_calculate_mask\n",
    "\n",
    "opt_blacklist = ['model.decoder.embed_tokens', 'model.decoder.embed_positions']\n",
    "\n",
    "#DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Using calibration data (inputs to each intermediate weight layer)\n",
    "# Iterate through named parameters, calculate inverse hessian and calculate mask\n",
    "\n",
    "def get_module_name(param_name):\n",
    "    if param_name[-5:] == \".bias\":\n",
    "        return param_name[:-5], \"bias\"\n",
    "    elif param_name[-7:] == \".weight\":\n",
    "        return param_name[:-7], \"weight\"\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def sparsegpt_prune(model, feature_hessians, \n",
    "EPSILON, SPARSENESS, B, Bs, module_blacklist=opt_blacklist, iterative=True):\n",
    "    module_dict = {}\n",
    "    for n, m in model.named_modules():\n",
    "        module_dict[n] = m\n",
    "    # print(module_dict.keys())\n",
    "    \n",
    "    param_names = []\n",
    "    param_dict = {}\n",
    "    for n, m in model.named_parameters():\n",
    "        param_names.append(n)\n",
    "        param_dict[n] = m\n",
    "    # print(parameter_list)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # for name in tqdm(param_names):\n",
    "        for param_name in tqdm(param_names, total=len(param_names)):\n",
    "            module_name, param_type = get_module_name(param_name)\n",
    "\n",
    "            # skip bias, embed, etc parameters\n",
    "            if module_name in module_blacklist or module_name is None \\\n",
    "                or param_type is None or param_type!=\"weight\":\n",
    "                continue\n",
    "\n",
    "            if len(param_dict[param_name].shape) < 2:\n",
    "                continue\n",
    "\n",
    "            param = param_dict[param_name]\n",
    "\n",
    "            #print(f\"Doing layer {name}\")\n",
    "            # get layer input from features, key is get_feature_storage_name(module_name)\n",
    "            # get_feature_storage_name(module_name) stores k_proj, v_proj, q_proj together\n",
    "            # since they are the same input\n",
    "            \n",
    "            layer_hessian = feature_hessians[get_feature_storage_name(module_name)].to(device=device)\n",
    "\n",
    "            # calculate inverse hessian\n",
    "            # check if input is flattened e.g. from 8,512,768 to 4096,768\n",
    "            inv_hess = calc_inverse_hessian(layer_hessian, epsilon=EPSILON)\n",
    "\n",
    "            # calculate mask\n",
    "            # if iterative:\n",
    "            #     mask = iterative_calculate_mask.calculate_mask(W=param, H_inv=inv_hess, p=SPARSENESS, B=B, Bs=Bs)\n",
    "            # else:\n",
    "            #     mask = calculate_mask.calculate_mask(W=param, H_inv=inv_hess, p=SPARSENESS, B=B, Bs=Bs)\n",
    "            mask = calculate_mask(W=param, H_inv=inv_hess, p=SPARSENESS, B=B, Bs=Bs)\n",
    "\n",
    "            # get module from lookup dictionary by module name\n",
    "            module = module_dict[module_name]\n",
    "            # apply mask\n",
    "            prune.custom_from_mask(module=module, name=param_type, mask=mask)\n",
    "            prune.remove(module=module, name=param_type)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()           \n",
    "    pruned_model_name = f'opt-350m-test-{SPARSENESS}'\n",
    "\n",
    "    torch.save(model.state_dict(), f'pruned_models/{pruned_model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:05,  1.04s/it]                       \n",
      "100%|██████████| 196/196 [01:15<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# initial prune\n",
    "model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True,\n",
    "                                           output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "SPARSENESS=.8\n",
    "\n",
    "feature_hessians = {}\n",
    "#put_input_hooks(model=model, features=feature_hessians, storage_dir=storage_dir, offload_freq=10000, feature_storage_device='cpu')\n",
    "put_input_hooks(model=model, features=feature_hessians, feature_storage_device='cpu')\n",
    "split_model_calibration(model)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "sparsegpt_prune(model=model, feature_hessians=feature_hessians, # type: ignore\n",
    "EPSILON=EPSILON, SPARSENESS=SPARSENESS, B=B, Bs=Bs)\n",
    "# print(f\"Proportion of 0s: {get_prop_zeros(model)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [01:12<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of 0s: 0.3619791567325592\n",
      "------------\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [01:15<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of 0s: 0.490234375\n",
      "------------\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [01:16<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of 0s: 0.5931006669998169\n",
      "------------\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [01:11<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of 0s: 0.67578125\n"
     ]
    }
   ],
   "source": [
    "# Test Iterative Pruning\n",
    "# from SparseGPT_pruning import sparsegpt_prune\n",
    "\n",
    "def get_prop_zeros(model):\n",
    "    return torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) / (torch.numel(model.get_decoder().layers[0].self_attn.k_proj.weight == 0))\n",
    "\n",
    "# model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True,\n",
    "#                                            output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "SPARSENESS=.8\n",
    "import iterative_calculate_mask\n",
    "for i in range(1,5):\n",
    "    \n",
    "    #storage_dir = f'tmp/{model_name}-{SPARSENESS}'\n",
    "    \n",
    "    # First, put in forward hooks\n",
    "    # Don't store inputs, instead store hessians (less data)\n",
    "    # Only store hessians once, as all models take the same hessians\n",
    "    print('------------')\n",
    "    if i == 0:\n",
    "        feature_hessians = {}\n",
    "        #put_input_hooks(model=model, features=feature_hessians, storage_dir=storage_dir, offload_freq=10000, feature_storage_device='cpu')\n",
    "        put_input_hooks(model=model, features=feature_hessians, feature_storage_device='cpu')\n",
    "        split_model_calibration(model)\n",
    "        torch.cuda.empty_cache()\n",
    "    print('-----------')\n",
    "    sparsegpt_prune(model=model, feature_hessians=feature_hessians, # type: ignore\n",
    "    EPSILON=EPSILON, SPARSENESS=SPARSENESS, B=B, Bs=Bs)\n",
    "    print(f\"Proportion of 0s: {get_prop_zeros(model)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input_prehooks import get_feature_storage_name\n",
    "import gc\n",
    "from SparseGPT_pruning import sparsegpt_prune\n",
    "layer_blacklist = ['model.decoder.embed_tokens.weight', 'model.decoder.embed_tokens.bias', 'model.decoder.embed_positions.weight']\n",
    "\n",
    "# Using calibration data (inputs to each intermediate weight layer)\n",
    "# Iterate through named parameters, calculate inverse hessian and calculate mask\n",
    "\n",
    "SPARSENESS_LIST = [0.5]#0.1, 0.2, 0.3, 0.5, 0.7, 0.9\n",
    "for i, SPARSENESS in enumerate(SPARSENESS_LIST):\n",
    "    \n",
    "    # Load model with pre-trained head\n",
    "    model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True,\n",
    "                                           output_hidden_states=True).to(device=device) # type: ignore\n",
    "    \n",
    "    #storage_dir = f'tmp/{model_name}-{SPARSENESS}'\n",
    "    \n",
    "    # First, put in forward hooks\n",
    "    # Don't store inputs, instead store hessians (less data)\n",
    "    # Only store hessians once, as all models take the same hessians\n",
    "    print('------------')\n",
    "    if i == 0:\n",
    "        feature_hessians = {}\n",
    "        #put_input_hooks(model=model, features=feature_hessians, storage_dir=storage_dir, offload_freq=10000, feature_storage_device='cpu')\n",
    "        put_input_hooks(model=model, features=feature_hessians, feature_storage_device='cpu')\n",
    "        split_model_calibration(model)\n",
    "        torch.cuda.empty_cache()\n",
    "    print('-----------')\n",
    "    \n",
    "    sparsegpt_prune(model=model, feature_hessians=feature_hessians, # type: ignore\n",
    "    EPSILON=EPSILON, SPARSENESS=SPARSENESS, B=B, Bs=Bs)\n",
    "\n",
    "    pruned_model_name = f'{model_size}-test-{SPARSENESS}'\n",
    "\n",
    "    torch.save(model.state_dict(), f'pruned_models/{pruned_model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from save_pruned_model import load_unmasked_model, load_masked_model\n",
    "\n",
    "loaded_model = OPTForCausalLM.from_pretrained(f'facebook/{model_size}', output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "# loaded_model = torch.nn.DataParallel(loaded_model, device_ids=[0,1,2,3])\n",
    "load_unmasked_model(loaded_model, f'pruned_models/{model_size}-test-0.5.pt')\n",
    "\n",
    "loaded_model_2 = OPTForCausalLM.from_pretrained(f'facebook/{model_size}', output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "load_masked_model(loaded_model_2, f'pruned_models/{model_size}-test-0.5.pt')\n",
    "\n",
    "def get_prop_zeros(model):\n",
    "    return torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) / (torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) + torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight != 0))\n",
    "\n",
    "print(get_prop_zeros(loaded_model))\n",
    "print(get_prop_zeros(model))\n",
    "print(get_prop_zeros(loaded_model_2))\n",
    "# loaded_model.eval()\n",
    "# _ = loaded_model(torch.randint(high=20, size=(1,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_list = []\n",
    "layer_blacklist = ['', 'module','module.model','module.model.decoder',\n",
    "                   'module.model.decoder.embed_tokens',\n",
    "                   'module.model.decoder.embed_tokens',\n",
    "                   'module.model.decoder.embed_positions']\n",
    "for name, module in model.named_modules():\n",
    "    # skip the embed layer or skip norms which have 1 dimension\n",
    "    if name in layer_blacklist or 'norm' in name or not isinstance(module, torch.nn.Module):\n",
    "        continue\n",
    "    prune_list.append((module, 'weight'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prune to 0s\n",
    "class ThresholdPruning(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "    # default threshold is 0, prunes weights that are already 0 (for training)\n",
    "    def __init__(self, threshold=1e-8):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def compute_mask(self, tensor, default_mask):\n",
    "        return torch.abs(tensor) >= self.threshold\n",
    "    \n",
    "prune.global_unstructured(prune_list,\n",
    "                          pruning_method=ThresholdPruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "01ca8e50707366aa71ffa41d1a1f13415d8b38eae741916c87480058f12910d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phill\\anaconda3\\envs\\pyt-exp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from calculate_mask import calculate_mask\n",
    "from inverse_hessian import inverse_hessian\n",
    "from input_prehooks import put_input_hooks\n",
    "from testing_module import calculate_perp\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_summary(name=\"\"):\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    f = r-a  # free inside reserved\n",
    "    print(f\"{name}, memory allocated: {a/1024/1024/1024} gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = \"facebook/opt-350m\"\n",
    "\n",
    "#Load dataset\n",
    "dataset = load_dataset('c4', 'en', streaming=True)\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, memory allocated: 2.7602057456970215 gb\n",
      "On SPASENESS 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:21,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, memory allocated: 1.5034546852111816 gb\n",
      "HI\n",
      "SPARSIFYING SPASENESS 0.3\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4101929664611816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.3940796852111816 gb\n",
      "after hessian, memory allocated: 1.3960328102111816 gb\n",
      "after mask, memory allocated: 1.3945679664611816 gb\n",
      "after pruning, memory allocated: 1.3940796852111816 gb\n",
      "2, memory allocated: 1.3940796852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "2, memory allocated: 1.5034546852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4106812477111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.4097046852111816 gb\n",
      "after hessian, memory allocated: 1.4175171852111816 gb\n",
      "after mask, memory allocated: 1.4136109352111816 gb\n",
      "after pruning, memory allocated: 1.4097046852111816 gb\n",
      "2, memory allocated: 1.4097046852111816 gb\n",
      "before hessian, memory allocated: 1.5034546852111816 gb\n",
      "after hessian, memory allocated: 1.6284546852111816 gb\n",
      "after mask, memory allocated: 1.5073609352111816 gb\n",
      "after pruning, memory allocated: 1.5034546852111816 gb\n",
      "SAVING SPASENESS 0.3\n"
     ]
    }
   ],
   "source": [
    "# Calibrate model (get inputs to each layer with calibration data)\n",
    "calibration_size=16\n",
    "token_length=512\n",
    "calibrate_on_cpu = False\n",
    "calibration_batch_size=2\n",
    "EPSILON = 1e-8\n",
    "B = 8\n",
    "Bs = 4\n",
    "layer_blacklist = ['model.decoder.embed_tokens.weight', 'model.decoder.embed_tokens.bias',\n",
    "'model.decoder.embed_positions.weight']\n",
    "\n",
    "# run model on batches of calibration data, then concatenate inputs\n",
    "def split_model_calibration(model):\n",
    "    batch_sentences = []\n",
    "    for i, data in tqdm(enumerate(iter(dataset['train']))):\n",
    "        if i < calibration_size + 1:\n",
    "            if len(batch_sentences) >= calibration_batch_size:\n",
    "                encoded_input = tokenizer(batch_sentences, return_tensors=\"pt\", padding=\"max_length\", max_length=token_length, truncation=True).to(device=device)\n",
    "                with torch.no_grad():\n",
    "                    model(**encoded_input, labels=encoded_input.input_ids)\n",
    "                batch_sentences = []\n",
    "            batch_sentences.append(data['text'])\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "# function to get module name from parameter name\n",
    "def get_module_name(param_name):\n",
    "    if param_name[-5:] == \".bias\":\n",
    "        return param_name[:-5], \"bias\"\n",
    "    elif param_name[-7:] == \".weight\":\n",
    "        return param_name[:-7], \"weight\"\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "for SPARSENESS in [0.3]:#0.2,\n",
    "    print_memory_summary(0)\n",
    "    print(f'On SPASENESS {SPARSENESS}')\n",
    "    # Load model with pre-trained head\n",
    "    model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True,\n",
    "                                           output_hidden_states=True).to(device=device) # type: ignore\n",
    "    \n",
    "    model.eval()\n",
    "    # First, put in forward hooks\n",
    "    features = {}\n",
    "    put_input_hooks(model=model, features=features, feature_storage_device='cpu')\n",
    "    split_model_calibration(model)\n",
    "    # make a dictionary to access module by name\n",
    "    module_lookup_dict = {}\n",
    "    for module_name, module_iter in model.named_modules():\n",
    "        module_lookup_dict[module_name] = module_iter\n",
    "        \n",
    "    #Iterate through named parameters, calculate inverse hessian and mask\n",
    "    param_lookup_dict = {}\n",
    "    param_names = []\n",
    "    for name, param in model.named_parameters():\n",
    "        param_names.append(name)\n",
    "        param_lookup_dict[name] = param\n",
    "    print_memory_summary(1)\n",
    "    print(\"HI\")\n",
    "    print(f'SPARSIFYING SPASENESS {SPARSENESS}')\n",
    "    with torch.no_grad():\n",
    "        for name in param_names:\n",
    "            param = param_lookup_dict[name]\n",
    "\n",
    "            # skip the embed layer\n",
    "            if name in layer_blacklist:\n",
    "                continue\n",
    "\n",
    "            # skip norms which have 1 dimension\n",
    "            if len(param.shape) < 2:\n",
    "                continue\n",
    "\n",
    "            module_name, param_type = get_module_name(name)\n",
    "            print_memory_summary(2)\n",
    "            # apply to weight and bias layers\n",
    "            if param_type == \"weight\" or param_type == \"bias\":\n",
    "                # input to parameter, move to gpu\n",
    "                layer_input = features[module_name].to(device=device)\n",
    "                # calculate inverse hessian\n",
    "                # check if input is flattened e.g. from 8,512,768 to 4096,768\n",
    "                print_memory_summary(\"before hessian\")\n",
    "                if len(layer_input.shape) == 2:\n",
    "                    inv_hess = inverse_hessian(torch.transpose(layer_input, 0, 1), epsilon=EPSILON, \n",
    "                    flattened=True).to(device=device)\n",
    "                else:\n",
    "                    inv_hess = inverse_hessian(torch.transpose(layer_input, 1, 2), epsilon=EPSILON,\n",
    "                    flattened=False).to(device=device)\n",
    "                print_memory_summary(\"after hessian\")\n",
    "                #No need for layer input now\n",
    "                del features[module_name]\n",
    "                # calculate mask\n",
    "                mask = calculate_mask(W=param.to(device=device), H_inv=inv_hess, p=SPARSENESS, B=B, Bs=Bs)\n",
    "                del inv_hess\n",
    "                del param\n",
    "                # mask.to(device=device)\n",
    "                print_memory_summary(\"after mask\")\n",
    "                # get module from lookup dictionary by module name\n",
    "                module = module_lookup_dict[module_name]\n",
    "                # apply mask\n",
    "                prune.custom_from_mask(module=module, name=param_type, mask=mask)\n",
    "\n",
    "                # masks add memory, remove the mask and replace masked weights with 0\n",
    "                prune.remove(module=module, name=param_type)\n",
    "                del module\n",
    "                del mask\n",
    "                print_memory_summary(\"after pruning\")\n",
    "        print(f'SAVING SPASENESS {SPARSENESS}')\n",
    "        pruned_model_name = f'opt-350m-{SPARSENESS}'\n",
    "        torch.save(model.state_dict(), f'pruned_models/{pruned_model_name}.pt')\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phill\\anaconda3\\envs\\pyt-exp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39m# loaded_model = OPTForCausalLM.from_pretrained('facebook/opt-350m', output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[39m# load_unmasked_model(loaded_model, 'pruned_models/opt-350m-0.3.pt')\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loaded_model2 \u001b[39m=\u001b[39m OPTForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mfacebook/opt-350m\u001b[39m\u001b[39m'\u001b[39m, output_attentions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice) \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m load_masked_model(loaded_model2, \u001b[39m'\u001b[39;49m\u001b[39mpruned_models/opt-350m-0.3.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     17\u001b[0m loaded_model2\u001b[39m.\u001b[39meval()\n\u001b[0;32m     18\u001b[0m loaded_model2(torch\u001b[39m.\u001b[39mrandint(high\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, size\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m10\u001b[39m))\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice))\n",
      "File \u001b[1;32mc:\\Users\\phill\\Documents\\Deep Learning\\Sparse-GPT-Finetuning\\save_pruned_model.py:39\u001b[0m, in \u001b[0;36mload_masked_model\u001b[1;34m(existing_model, state_dict_path)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_masked_model\u001b[39m(existing_model, state_dict_path):\n\u001b[0;32m     35\u001b[0m     \u001b[39m# apply_identity_prune(model=existing_model)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     existing_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(state_dict_path))\n\u001b[1;32m---> 39\u001b[0m     prune\u001b[39m.\u001b[39;49mglobal_unstructured(\n\u001b[0;32m     40\u001b[0m         existing_model\u001b[39m.\u001b[39;49mparameters(), pruning_method\u001b[39m=\u001b[39;49mThresholdPruning, threshold\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[0;32m     41\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\phill\\anaconda3\\envs\\pyt-exp\\lib\\site-packages\\torch\\nn\\utils\\prune.py:1086\u001b[0m, in \u001b[0;36mglobal_unstructured\u001b[1;34m(parameters, pruning_method, importance_scores, **kwargs)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mglobal_unstructured(): importance_scores must be of type dict\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1084\u001b[0m \u001b[39m# flatten importance scores to consider them all at once in global pruning\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m relevant_importance_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mparameters_to_vector(\n\u001b[1;32m-> 1086\u001b[0m     [\n\u001b[0;32m   1087\u001b[0m         importance_scores\u001b[39m.\u001b[39mget((module, name), \u001b[39mgetattr\u001b[39m(module, name))\n\u001b[0;32m   1088\u001b[0m         \u001b[39mfor\u001b[39;00m (module, name) \u001b[39min\u001b[39;00m parameters\n\u001b[0;32m   1089\u001b[0m     ]\n\u001b[0;32m   1090\u001b[0m )\n\u001b[0;32m   1091\u001b[0m \u001b[39m# similarly, flatten the masks (if they exist), or use a flattened vector\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[39m# of 1s of the same dimensions as t\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m default_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mparameters_to_vector(\n\u001b[0;32m   1094\u001b[0m     [\n\u001b[0;32m   1095\u001b[0m         \u001b[39mgetattr\u001b[39m(module, name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_mask\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mones_like(\u001b[39mgetattr\u001b[39m(module, name)))\n\u001b[0;32m   1096\u001b[0m         \u001b[39mfor\u001b[39;00m (module, name) \u001b[39min\u001b[39;00m parameters\n\u001b[0;32m   1097\u001b[0m     ]\n\u001b[0;32m   1098\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\phill\\anaconda3\\envs\\pyt-exp\\lib\\site-packages\\torch\\nn\\utils\\prune.py:1086\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mglobal_unstructured(): importance_scores must be of type dict\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1084\u001b[0m \u001b[39m# flatten importance scores to consider them all at once in global pruning\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m relevant_importance_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mparameters_to_vector(\n\u001b[1;32m-> 1086\u001b[0m     [\n\u001b[0;32m   1087\u001b[0m         importance_scores\u001b[39m.\u001b[39mget((module, name), \u001b[39mgetattr\u001b[39m(module, name))\n\u001b[0;32m   1088\u001b[0m         \u001b[39mfor\u001b[39;00m (module, name) \u001b[39min\u001b[39;00m parameters\n\u001b[0;32m   1089\u001b[0m     ]\n\u001b[0;32m   1090\u001b[0m )\n\u001b[0;32m   1091\u001b[0m \u001b[39m# similarly, flatten the masks (if they exist), or use a flattened vector\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[39m# of 1s of the same dimensions as t\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m default_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mparameters_to_vector(\n\u001b[0;32m   1094\u001b[0m     [\n\u001b[0;32m   1095\u001b[0m         \u001b[39mgetattr\u001b[39m(module, name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_mask\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mones_like(\u001b[39mgetattr\u001b[39m(module, name)))\n\u001b[0;32m   1096\u001b[0m         \u001b[39mfor\u001b[39;00m (module, name) \u001b[39min\u001b[39;00m parameters\n\u001b[0;32m   1097\u001b[0m     ]\n\u001b[0;32m   1098\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# load pruned model\n",
    "\n",
    "from save_pruned_model import load_unmasked_model, load_masked_model\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# loaded_model = OPTForCausalLM.from_pretrained('facebook/opt-350m', output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "# load_unmasked_model(loaded_model, 'pruned_models/opt-350m-0.3.pt')\n",
    "\n",
    "loaded_model2 = OPTForCausalLM.from_pretrained('facebook/opt-350m', output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "load_masked_model(loaded_model2, 'pruned_models/opt-350m-0.3.pt')\n",
    "\n",
    "loaded_model2.eval()\n",
    "loaded_model2(torch.randint(high=20, size=(1,10)).to(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7005, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_prop_zeros(model):\n",
    "    return torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) / (torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) + torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight != 0))\n",
    "\n",
    "print(get_prop_zeros(loaded_model2))\n",
    "print(get_prop_zeros(loaded_model))\n",
    "# print(get_prop_zeros(model2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3598d1fcb68810e1ab8d19050593974172dcc02c5f12874feb8b1a070749563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

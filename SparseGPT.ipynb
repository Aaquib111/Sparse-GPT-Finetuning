{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# from calculate_mask import calculate_mask\n",
    "# from inverse_hessian import inverse_hessian\n",
    "from input_prehooks import put_input_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "#Load dataset\n",
    "dataset = load_dataset('c4', 'en', streaming=True)\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "# Load model with pre-trained head\n",
    "model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True)\n",
    "# Load genrator\n",
    "generator = pipeline('text-generation', model=\"facebook/opt-125m\")\n",
    "# Create calibration data\n",
    "calibration_data = []\n",
    "for i, data in enumerate(iter(dataset['train'])):\n",
    "    if i > 15:\n",
    "        break\n",
    "    tokenized = tokenizer.encode(data['text'], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "    calibration_data.append(tokenized)\n",
    "calibration_data = torch.squeeze(torch.stack(calibration_data)).to(device=device)\n",
    "calibration_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, put in forward hooks\n",
    "features = {}\n",
    "put_input_hooks(model=model, features=features)\n",
    "\n",
    "# Run calibration data through model at first to calculate features dictionary with\n",
    "# input tensors to each intermediate layer\n",
    "model(calibration_data)\n",
    "\n",
    "# function to get module name from parameter name\n",
    "def get_module_name(param_name):\n",
    "    if param_name[-5:] == \".bias\":\n",
    "        return param_name[:-5], \"bias\"\n",
    "    elif param_name[-7:] == \".weight\":\n",
    "        return param_name[:-7], \"weight\"\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    2, 48290,  7130,  ...,     1,     1,     1],\n",
       "         [    2, 48763,    11,  ...,     1,     1,     1],\n",
       "         [    2,   597, 14189,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    2,   100,    33,  ...,     1,     1,     1],\n",
       "         [    2, 16750, 37123,  ...,     1,     1,     1],\n",
       "         [    2,   113,   387,  ...,     6,  4753,     4]]),)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['model.decoder.embed_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 512])\n",
      "num zeros in hessian: 0\n",
      "Determinant is inf\n",
      "Hessian Diagonal is tensor([1.2800e+02, 2.1211e+10, 4.4277e+09, 4.0502e+09, 6.9439e+09, 6.9146e+09,\n",
      "        3.8837e+09, 2.8223e+09, 3.6256e+09, 5.8754e+09, 8.0317e+09, 7.6373e+09,\n",
      "        1.8665e+10, 7.1441e+09, 9.2101e+08, 4.8976e+09, 8.7763e+09, 8.6306e+09,\n",
      "        2.9042e+08, 6.2431e+08, 2.2291e+09, 1.1374e+09, 7.0100e+09, 7.1680e+09,\n",
      "        4.2909e+09, 5.3398e+09, 1.1318e+10, 2.9786e+09, 4.3516e+09, 6.2345e+09,\n",
      "        8.3428e+09, 8.5852e+09, 5.9779e+09, 6.3430e+09, 1.2358e+09, 4.2279e+08,\n",
      "        5.6564e+09, 1.3808e+09, 6.9630e+09, 1.1036e+10, 1.3557e+10, 4.8947e+09,\n",
      "        4.9650e+09, 2.3144e+09, 9.7131e+09, 7.7709e+09, 3.7450e+09, 5.0588e+09,\n",
      "        5.7672e+09, 1.3213e+10, 5.6264e+09, 3.1772e+08, 5.2932e+09, 7.2946e+09,\n",
      "        5.2837e+09, 6.2821e+09, 7.5086e+09, 3.7343e+09, 1.7969e+09, 3.6061e+09,\n",
      "        5.5131e+09, 2.1430e+08, 6.1203e+09, 3.1141e+09, 5.3713e+09, 8.1849e+08,\n",
      "        6.7303e+08, 6.8726e+09, 7.3958e+09, 1.2630e+10, 8.7682e+09, 3.8449e+09,\n",
      "        1.5297e+09, 1.0514e+09, 8.7549e+09, 1.1526e+10, 5.2480e+09, 1.0409e+10,\n",
      "        5.0639e+09, 9.1248e+08, 1.2107e+09, 8.5707e+09, 4.3252e+09, 1.0480e+10,\n",
      "        6.7787e+09, 3.2531e+09, 1.5818e+09, 4.8651e+09, 5.1505e+09, 2.4209e+09,\n",
      "        8.2320e+08, 2.7378e+09, 1.8885e+09, 4.0385e+08, 1.2399e+09, 2.2973e+08,\n",
      "        6.3008e+05, 3.8263e+09, 1.6741e+09, 9.8013e+09, 4.3444e+09, 5.7915e+08,\n",
      "        1.0184e+10, 3.2508e+09, 8.7252e+08, 4.7374e+09, 2.6517e+09, 3.9360e+09,\n",
      "        2.5196e+09, 7.1398e+09, 3.8875e+09, 7.4322e+09, 3.6579e+08, 4.7260e+08,\n",
      "        1.2824e+09, 1.9189e+10, 7.2964e+09, 4.1600e+09, 9.2167e+09, 3.5375e+09,\n",
      "        1.2190e+09, 4.1377e+09, 4.2752e+07, 6.4563e+09, 2.6258e+08, 2.7843e+09,\n",
      "        7.6264e+07, 1.0439e+08, 1.1475e+08, 2.1425e+07, 1.0048e+09, 1.0344e+09,\n",
      "        2.5201e+09, 4.5717e+09, 5.6114e+09, 3.5740e+09, 5.4947e+08, 1.9136e+09,\n",
      "        8.9820e+09, 2.0074e+09, 1.1645e+07, 1.2098e+10, 1.4733e+08, 1.7721e+09,\n",
      "        2.9034e+09, 1.1104e+09, 1.9746e+07, 7.1010e+09, 1.6486e+08, 2.1055e+08,\n",
      "        1.5960e+09, 6.0761e+09, 4.1426e+09, 7.5500e+09, 1.5155e+09, 3.8535e+09,\n",
      "        4.2953e+08, 2.3579e+09, 5.2493e+07, 5.6161e+09, 1.2086e+09, 1.3264e+08,\n",
      "        1.3490e+08, 1.9603e+09, 7.8135e+08, 1.5991e+08, 1.7757e+09, 1.0065e+10,\n",
      "        3.9450e+09, 1.2268e+09, 1.8700e+09, 1.4526e+08, 1.6513e+09, 3.1682e+09,\n",
      "        5.7065e+07, 3.4311e+08, 3.8556e+08, 1.2893e+08, 5.9236e+08, 2.6944e+09,\n",
      "        1.4497e+06, 2.6885e+09, 9.6085e+08, 2.9086e+08, 5.2616e+06, 5.0640e+09,\n",
      "        2.7788e+08, 3.7312e+09, 2.7760e+08, 3.6384e+09, 5.1191e+09, 6.7236e+07,\n",
      "        5.1469e+09, 1.5451e+08, 5.0461e+09, 1.1341e+09, 2.3647e+09, 1.8708e+09,\n",
      "        3.2344e+08, 7.8777e+08, 3.5199e+07, 3.2724e+09, 2.0603e+08, 3.8836e+08,\n",
      "        3.4010e+09, 1.3244e+09, 5.0579e+09, 1.6551e+08, 8.5775e+08, 2.4010e+08,\n",
      "        1.7036e+07, 5.1286e+09, 2.6292e+05, 2.9167e+09, 6.0549e+07, 4.0823e+05,\n",
      "        5.2552e+07, 1.2617e+08, 6.3137e+09, 2.8423e+08, 4.2956e+06, 2.3449e+09,\n",
      "        2.5286e+08, 2.1425e+08, 1.9050e+08, 5.1213e+09, 6.4605e+07, 3.9093e+08,\n",
      "        3.3636e+07, 2.1368e+08, 4.3103e+09, 6.2755e+08, 6.0295e+07, 5.2443e+09,\n",
      "        7.9517e+08, 1.8050e+07, 8.5391e+08, 6.2750e+09, 2.2872e+08, 6.8481e+07,\n",
      "        2.7026e+07, 9.2361e+08, 4.5211e+07, 5.4364e+07, 3.0779e+08, 1.0945e+08,\n",
      "        3.5293e+09, 1.3809e+08, 3.5939e+08, 5.3213e+06, 4.0553e+09, 2.2426e+09,\n",
      "        4.1254e+04, 5.6740e+09, 1.6222e+09, 2.6642e+08, 1.1035e+08, 5.1326e+09,\n",
      "        1.5165e+09, 1.2711e+07, 3.1186e+06, 6.4853e+07, 2.0530e+07, 1.1122e+08,\n",
      "        3.5073e+09, 3.1684e+09, 1.5495e+09, 7.7272e+08, 1.2379e+09, 2.3610e+08,\n",
      "        1.4957e+07, 1.1419e+09, 5.3703e+09, 2.0879e+07, 8.0781e+05, 1.4755e+07,\n",
      "        6.8691e+05, 2.7703e+05, 2.6543e+08, 8.3180e+08, 2.9864e+09, 3.2545e+09,\n",
      "        7.4010e+07, 4.0290e+04, 3.2490e+05, 1.5426e+06, 2.8923e+09, 7.3433e+07,\n",
      "        1.0761e+08, 4.9906e+05, 5.9431e+06, 3.7045e+08, 4.0147e+09, 4.4700e+03,\n",
      "        2.5616e+09, 6.6474e+04, 3.6128e+08, 3.1172e+05, 5.0237e+09, 6.1367e+07,\n",
      "        7.4038e+07, 2.4006e+08, 4.5448e+08, 1.4961e+07, 1.4680e+09, 1.5074e+06,\n",
      "        6.0914e+09, 1.5644e+06, 2.5046e+09, 1.0928e+08, 3.4595e+07, 5.0270e+09,\n",
      "        3.9233e+09, 1.6981e+09, 4.4034e+06, 8.6958e+06, 4.5740e+03, 2.4065e+08,\n",
      "        1.5113e+06, 1.3733e+08, 4.5182e+07, 1.1511e+08, 6.2923e+09, 9.7564e+08,\n",
      "        1.3200e+08, 2.5547e+08, 4.1043e+05, 9.1080e+03, 1.6284e+06, 3.7899e+06,\n",
      "        1.0552e+09, 9.1845e+05, 1.1773e+06, 2.1196e+09, 4.9030e+05, 1.2933e+06,\n",
      "        2.4774e+07, 1.5350e+06, 1.4964e+07, 2.8622e+09, 4.3170e+05, 2.8023e+07,\n",
      "        1.3262e+08, 2.2126e+06, 4.3374e+04, 6.5310e+06, 4.3436e+04, 4.1561e+08,\n",
      "        1.4403e+08, 6.3773e+07, 2.5400e+02, 1.2388e+09, 2.2964e+08, 2.1464e+07,\n",
      "        1.0047e+10, 4.7394e+08, 7.0706e+09, 5.9751e+06, 4.8489e+06, 9.0354e+07,\n",
      "        3.0075e+07, 1.0565e+09, 4.6091e+05, 7.9662e+05, 8.3700e+04, 1.0416e+08,\n",
      "        2.3098e+08, 7.5156e+08, 1.9104e+04, 3.1000e+02, 1.1073e+08, 3.7660e+03,\n",
      "        1.0245e+07, 8.8780e+04, 1.2449e+09, 2.2814e+08, 2.0715e+08, 4.0849e+05,\n",
      "        2.4670e+07, 5.0236e+09, 2.9330e+08, 1.2100e+07, 1.5500e+06, 4.5663e+05,\n",
      "        1.0444e+08, 1.5966e+08, 5.9097e+07, 4.1922e+04, 5.6826e+05, 2.0855e+07,\n",
      "        1.2160e+04, 3.7613e+08, 5.2800e+02, 3.4745e+06, 4.0620e+03, 4.6860e+03,\n",
      "        1.6290e+06, 1.4830e+07, 3.3217e+07, 1.7580e+03, 7.4600e+03, 7.7730e+06,\n",
      "        1.5000e+02, 1.7185e+09, 2.2812e+08, 3.6105e+07, 7.8780e+03, 2.9713e+07,\n",
      "        8.1365e+07, 6.7333e+05, 3.3260e+03, 2.3247e+07, 1.0600e+08, 5.0236e+09,\n",
      "        4.2227e+09, 1.5496e+06, 1.4120e+03, 5.0236e+09, 7.9403e+08, 4.7852e+04,\n",
      "        1.3800e+03, 1.1422e+08, 4.4993e+08, 4.7622e+04, 1.5270e+04, 1.8878e+04,\n",
      "        1.7071e+07, 1.3838e+04, 1.0258e+07, 4.1438e+04, 1.3800e+03, 2.0358e+05,\n",
      "        5.0236e+09, 3.1255e+08, 3.1800e+02, 2.6079e+07, 1.2094e+07, 1.5348e+06,\n",
      "        4.3776e+06, 3.5745e+05, 8.2518e+04, 1.4060e+03, 2.5508e+04, 5.7200e+02,\n",
      "        1.0321e+06, 4.0896e+04, 1.1533e+05, 9.3740e+03, 2.6057e+07, 3.7980e+03,\n",
      "        1.6143e+07, 1.0260e+08, 9.2000e+01, 5.0236e+09, 6.0065e+05, 5.2292e+07,\n",
      "        6.0697e+08, 3.0157e+07, 2.1449e+08, 1.3892e+07, 2.0299e+07, 3.6280e+03,\n",
      "        2.5811e+08, 4.4502e+04, 9.8288e+05, 1.2812e+05, 3.1800e+02, 4.7276e+07,\n",
      "        6.9292e+04, 1.9268e+04, 1.0026e+07, 6.9920e+03, 2.1248e+04, 1.7560e+06,\n",
      "        4.9613e+06, 1.9200e+02, 8.7392e+04, 1.2325e+06, 3.2300e+03, 1.1910e+05,\n",
      "        4.4480e+03, 1.6188e+07, 7.4720e+03, 7.2733e+06, 3.2300e+03, 4.0527e+08,\n",
      "        2.4230e+04, 4.0518e+07, 6.2000e+01, 5.0236e+09, 2.9957e+05, 5.2286e+07,\n",
      "        2.2812e+08, 8.2220e+03, 1.5980e+03, 4.8702e+04, 1.9520e+03, 3.6983e+05,\n",
      "        7.5252e+07, 1.9200e+02, 2.0263e+07, 9.4208e+04, 8.2947e+07, 1.0200e+02,\n",
      "        3.2789e+07, 1.0200e+02, 1.1616e+08, 1.0200e+02, 2.7217e+07, 1.0200e+02,\n",
      "        4.5182e+07, 6.2000e+01], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 129\u001b[0m\n\u001b[1;32m    126\u001b[0m inv_hess \u001b[39m=\u001b[39m inverse_hessian(torch\u001b[39m.\u001b[39mtranspose(layer_input[\u001b[39m0\u001b[39m], \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m), epsilon\u001b[39m=\u001b[39mEPSILON)\n\u001b[1;32m    128\u001b[0m \u001b[39m# calculate mask\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m mask \u001b[39m=\u001b[39m calculate_mask(W\u001b[39m=\u001b[39;49mparam, H_inv\u001b[39m=\u001b[39;49minv_hess, p\u001b[39m=\u001b[39;49mSPARSENESS, B\u001b[39m=\u001b[39;49mB, Bs\u001b[39m=\u001b[39;49mBs)\n\u001b[1;32m    131\u001b[0m \u001b[39m# get module from lookup dictionary by module name\u001b[39;00m\n\u001b[1;32m    132\u001b[0m module \u001b[39m=\u001b[39m model_lookup_dict[module_name]\n",
      "Cell \u001b[0;32mIn[27], line 69\u001b[0m, in \u001b[0;36mcalculate_mask\u001b[0;34m(W, H_inv, p, B, Bs)\u001b[0m\n\u001b[1;32m     65\u001b[0m         W[:, j:i \u001b[39m+\u001b[39m B] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mger(E[:, j \u001b[39m-\u001b[39m i], H_inv[j, j:i \u001b[39m+\u001b[39m B])\n\u001b[1;32m     67\u001b[0m     \u001b[39m# Update all remaining weights\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     W[:, i \u001b[39m+\u001b[39m B:] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(E, H_inv[i:i \u001b[39m+\u001b[39;49m B, i \u001b[39m+\u001b[39;49m B:])\n\u001b[1;32m     71\u001b[0m \u001b[39m# return mask\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39mreturn\u001b[39;00m M\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "def calculate_mask(\n",
    "    W,\n",
    "    H_inv,\n",
    "    p,\n",
    "    B,\n",
    "    Bs,\n",
    "    ):\n",
    "\n",
    "    # Get the number of rows and columns in W\n",
    "\n",
    "    (d_row, d_col) = W.shape\n",
    "\n",
    "    # Initialize the pruning mask M and block quantization errors E to all zeros\n",
    "\n",
    "    M = torch.zeros(d_row, d_col, dtype=torch.bool)\n",
    "    E = torch.zeros(d_row, B)\n",
    "\n",
    "    # only need to calculate w_square and h_square once\n",
    "    # Loop over blocks of columns of W (as specified by B)\n",
    "\n",
    "    for i in range(0, d_col, B):\n",
    "\n",
    "        # Loop over columns within a block\n",
    "\n",
    "        for j in range(i, min(i + B, d_col)):\n",
    "\n",
    "            # If j is a multiple of Bs, prune a portion of the weights\n",
    "\n",
    "            if j % Bs == 0:\n",
    "\n",
    "                # Get the mask for the largest (1 - p)% of weights based on squared value and inverse hessian\n",
    "\n",
    "                # ASTERISK: prune_values is matrix of w^2/H^(-1)_cc\n",
    "\n",
    "                # Finding respective sections of hessian and weights matrix\n",
    "                w_square_section = torch.square(W[:, j:j + Bs])\n",
    "                h_square_section = torch.square(H_inv[j:j + Bs, j:j\n",
    "                        + Bs]).diag()  # 1 dimensional vector\n",
    "\n",
    "                # getting the prune values matrix from W and H^-1 sections\n",
    "                prune_values = w_square_section \\\n",
    "                    / h_square_section.unsqueeze(0)\n",
    "\n",
    "                #calulating cutoff for the weights\n",
    "                cutoff_value = torch.kthvalue(prune_values, int((1 - p)\n",
    "                        * d_row), dim=0)[0]\n",
    "\n",
    "                #getting the final mask\n",
    "                mask = prune_values > cutoff_value\n",
    "\n",
    "                #masking\n",
    "                M[:, j:j + Bs] = mask\n",
    "\n",
    "            # Calculate the pruning error for this column\n",
    "\n",
    "            E[:, j - i] = W[:, j] / H_inv[j, j]\n",
    "\n",
    "            # Freeze the weights that are not pruned by multiplying by the pruning mask\n",
    "            # Invert mask (~M equivalent to 1 - M < might be -(M + 1))\n",
    "\n",
    "            E[:, j - i] = ~M[:, j] * E[:, j - i]\n",
    "\n",
    "            # Update the weights in this block based on the pruning error and inverse hessian information\n",
    "\n",
    "            W[:, j:i + B] -= torch.ger(E[:, j - i], H_inv[j, j:i + B])\n",
    "\n",
    "        # Update all remaining weights\n",
    "\n",
    "        W[:, i + B:] -= torch.matmul(E, H_inv[i:i + B, i + B:])\n",
    "\n",
    "    # return mask\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "def inverse_hessian(X, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Calculate the inverse of a positive-definite matrix using the Cholesky decomposition.\n",
    "    Args:\n",
    "    - X (torch.Tensor): dxn tensor\n",
    "    - epsilon (float): small constant to prevent Hessian from being singular\n",
    "    Returns:\n",
    "    - torch.Tensor: inverted matrix\n",
    "    \"\"\"\n",
    "    X = X.float()\n",
    "    X_T = torch.transpose(X, 0, 1)\n",
    "    identity = torch.eye(X.shape[0], dtype=torch.float64)\n",
    "    H = 2 * (X @ X_T + (epsilon * identity))\n",
    "    # print(torch.linalg.eig(H)[0])\n",
    "    print(H.shape)\n",
    "    print(f\"num zeros in hessian: {torch.sum(H == 0)}\")\n",
    "    print(f\"Determinant is {torch.linalg.det(H)}\")\n",
    "    print(f\"Hessian Diagonal is {H.diag()}\")\n",
    "    H_inv = torch.inverse(H)\n",
    "    \n",
    "    #H_inv = torch.cholesky(H_inv).T\n",
    "    H_inv = torch.lu(H_inv)[0].T\n",
    "    \n",
    "    return H_inv\n",
    "\n",
    "\n",
    "# Re-load model with pre-trained head\n",
    "model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "# make a dictionary to access module by name\n",
    "model_lookup_dict = {}\n",
    "for module_name, module_iter in model.named_modules():\n",
    "    model_lookup_dict[module_name] = module_iter\n",
    "\n",
    "EPSILON = 1e-8\n",
    "SPARSENESS = .2\n",
    "B = 32\n",
    "Bs = 16\n",
    "\n",
    "# Using calibration data (inputs to each intermediate weight layer)\n",
    "# Iterate through named parameters, calculate inverse hessian and calculate mask\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        module_name, param_type = get_module_name(name)\n",
    "\n",
    "        # apply to weight and bias layers\n",
    "        if param_type == \"weight\" or param_type == \"bias\":\n",
    "            # input to parameter\n",
    "            layer_input = features[module_name]\n",
    "            # calculate inverse hessian\n",
    "            inv_hess = inverse_hessian(torch.transpose(layer_input[0], 0, 1), epsilon=EPSILON)\n",
    "\n",
    "            # calculate mask\n",
    "            mask = calculate_mask(W=param, H_inv=inv_hess, p=SPARSENESS, B=B, Bs=Bs)\n",
    "            \n",
    "            # get module from lookup dictionary by module name\n",
    "            module = model_lookup_dict[module_name]\n",
    "            # apply mask\n",
    "            prune.custom_from_mask(module=module, name=param_type, mask=mask)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('pyt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01ca8e50707366aa71ffa41d1a1f13415d8b38eae741916c87480058f12910d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from calculate_mask import calculate_mask\n",
    "from inverse_hessian import inverse_hessian\n",
    "from input_prehooks import put_input_hooks\n",
    "from testing_module import calculate_perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 685/685 [00:00<00:00, 474kB/s]\n",
      "Downloading: 100%|██████████| 653/653 [00:00<00:00, 529kB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 7.37MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 4.47MB/s]\n",
      "Downloading: 100%|██████████| 441/441 [00:00<00:00, 412kB/s]\n",
      "Downloading: 100%|██████████| 2.63G/2.63G [05:28<00:00, 8.01MB/s]\n"
     ]
    }
   ],
   "source": [
    "#DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model_name = \"facebook/opt-125m\"\n",
    "model_name = \"facebook/opt-1.3b\"\n",
    "\n",
    "#Load dataset\n",
    "dataset = load_dataset('c4', 'en', streaming=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "# Load model with pre-trained head\n",
    "model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "# Load generator\n",
    "generator = pipeline('text-generation', model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name is model.decoder.layers.0.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.0.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.0.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.0.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.0.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.0.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.1.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.1.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.1.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.1.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.1.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.1.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.2.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.2.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.2.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.2.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.2.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.2.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.3.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.3.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.3.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.3.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.3.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.3.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.4.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.4.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.4.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.4.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.4.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.4.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.5.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.5.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.5.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.5.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.5.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.5.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.6.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.6.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.6.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.6.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.6.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.6.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.7.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.7.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.7.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.7.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.7.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.7.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.8.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.8.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.8.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.8.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.8.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.8.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.9.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.9.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.9.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.9.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.9.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.9.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.10.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.10.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.10.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.10.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.10.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.10.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.11.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.11.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.11.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.11.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.11.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.11.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.12.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.12.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.12.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.12.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.12.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.12.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.13.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.13.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.13.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.13.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.13.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.13.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.14.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.14.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.14.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.14.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.14.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.14.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.15.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.15.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.15.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.15.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.15.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.15.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.16.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.16.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.16.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.16.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.16.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.16.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.17.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.17.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.17.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.17.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.17.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.17.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.18.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.18.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.18.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.18.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.18.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.18.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.19.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.19.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.19.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.19.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.19.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.19.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.20.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.20.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.20.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.20.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.20.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.20.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.21.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.21.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.21.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.21.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.21.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.21.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.22.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.22.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.22.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.22.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.22.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.22.fc2.weight, param shape is torch.Size([2048, 8192])\n",
      "name is model.decoder.layers.23.self_attn.k_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.23.self_attn.v_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.23.self_attn.q_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.23.self_attn.out_proj.weight, param shape is torch.Size([2048, 2048])\n",
      "name is model.decoder.layers.23.fc1.weight, param shape is torch.Size([8192, 2048])\n",
      "name is model.decoder.layers.23.fc2.weight, param shape is torch.Size([2048, 8192])\n"
     ]
    }
   ],
   "source": [
    "for n, m in model.named_parameters():\n",
    "    if 'weight' in n and ('proj' in n or 'fc' in n):\n",
    "        print(f\"name is {n}, param shape is {m.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.decoder\n",
      "model.decoder.embed_tokens\n",
      "model.decoder.embed_positions\n",
      "model.decoder.final_layer_norm\n",
      "model.decoder.layers\n",
      "model.decoder.layers.0\n",
      "model.decoder.layers.0.self_attn\n",
      "model.decoder.layers.0.self_attn.k_proj\n",
      "model.decoder.layers.0.self_attn.v_proj\n",
      "model.decoder.layers.0.self_attn.q_proj\n",
      "model.decoder.layers.0.self_attn.out_proj\n",
      "model.decoder.layers.0.activation_fn\n",
      "model.decoder.layers.0.self_attn_layer_norm\n",
      "model.decoder.layers.0.fc1\n",
      "model.decoder.layers.0.fc2\n",
      "model.decoder.layers.0.final_layer_norm\n",
      "model.decoder.layers.1\n",
      "model.decoder.layers.1.self_attn\n",
      "model.decoder.layers.1.self_attn.k_proj\n",
      "model.decoder.layers.1.self_attn.v_proj\n",
      "model.decoder.layers.1.self_attn.q_proj\n",
      "model.decoder.layers.1.self_attn.out_proj\n",
      "model.decoder.layers.1.activation_fn\n",
      "model.decoder.layers.1.self_attn_layer_norm\n",
      "model.decoder.layers.1.fc1\n",
      "model.decoder.layers.1.fc2\n",
      "model.decoder.layers.1.final_layer_norm\n",
      "model.decoder.layers.2\n",
      "model.decoder.layers.2.self_attn\n",
      "model.decoder.layers.2.self_attn.k_proj\n",
      "model.decoder.layers.2.self_attn.v_proj\n",
      "model.decoder.layers.2.self_attn.q_proj\n",
      "model.decoder.layers.2.self_attn.out_proj\n",
      "model.decoder.layers.2.activation_fn\n",
      "model.decoder.layers.2.self_attn_layer_norm\n",
      "model.decoder.layers.2.fc1\n",
      "model.decoder.layers.2.fc2\n",
      "model.decoder.layers.2.final_layer_norm\n",
      "model.decoder.layers.3\n",
      "model.decoder.layers.3.self_attn\n",
      "model.decoder.layers.3.self_attn.k_proj\n",
      "model.decoder.layers.3.self_attn.v_proj\n",
      "model.decoder.layers.3.self_attn.q_proj\n",
      "model.decoder.layers.3.self_attn.out_proj\n",
      "model.decoder.layers.3.activation_fn\n",
      "model.decoder.layers.3.self_attn_layer_norm\n",
      "model.decoder.layers.3.fc1\n",
      "model.decoder.layers.3.fc2\n",
      "model.decoder.layers.3.final_layer_norm\n",
      "model.decoder.layers.4\n",
      "model.decoder.layers.4.self_attn\n",
      "model.decoder.layers.4.self_attn.k_proj\n",
      "model.decoder.layers.4.self_attn.v_proj\n",
      "model.decoder.layers.4.self_attn.q_proj\n",
      "model.decoder.layers.4.self_attn.out_proj\n",
      "model.decoder.layers.4.activation_fn\n",
      "model.decoder.layers.4.self_attn_layer_norm\n",
      "model.decoder.layers.4.fc1\n",
      "model.decoder.layers.4.fc2\n",
      "model.decoder.layers.4.final_layer_norm\n",
      "model.decoder.layers.5\n",
      "model.decoder.layers.5.self_attn\n",
      "model.decoder.layers.5.self_attn.k_proj\n",
      "model.decoder.layers.5.self_attn.v_proj\n",
      "model.decoder.layers.5.self_attn.q_proj\n",
      "model.decoder.layers.5.self_attn.out_proj\n",
      "model.decoder.layers.5.activation_fn\n",
      "model.decoder.layers.5.self_attn_layer_norm\n",
      "model.decoder.layers.5.fc1\n",
      "model.decoder.layers.5.fc2\n",
      "model.decoder.layers.5.final_layer_norm\n",
      "model.decoder.layers.6\n",
      "model.decoder.layers.6.self_attn\n",
      "model.decoder.layers.6.self_attn.k_proj\n",
      "model.decoder.layers.6.self_attn.v_proj\n",
      "model.decoder.layers.6.self_attn.q_proj\n",
      "model.decoder.layers.6.self_attn.out_proj\n",
      "model.decoder.layers.6.activation_fn\n",
      "model.decoder.layers.6.self_attn_layer_norm\n",
      "model.decoder.layers.6.fc1\n",
      "model.decoder.layers.6.fc2\n",
      "model.decoder.layers.6.final_layer_norm\n",
      "model.decoder.layers.7\n",
      "model.decoder.layers.7.self_attn\n",
      "model.decoder.layers.7.self_attn.k_proj\n",
      "model.decoder.layers.7.self_attn.v_proj\n",
      "model.decoder.layers.7.self_attn.q_proj\n",
      "model.decoder.layers.7.self_attn.out_proj\n",
      "model.decoder.layers.7.activation_fn\n",
      "model.decoder.layers.7.self_attn_layer_norm\n",
      "model.decoder.layers.7.fc1\n",
      "model.decoder.layers.7.fc2\n",
      "model.decoder.layers.7.final_layer_norm\n",
      "model.decoder.layers.8\n",
      "model.decoder.layers.8.self_attn\n",
      "model.decoder.layers.8.self_attn.k_proj\n",
      "model.decoder.layers.8.self_attn.v_proj\n",
      "model.decoder.layers.8.self_attn.q_proj\n",
      "model.decoder.layers.8.self_attn.out_proj\n",
      "model.decoder.layers.8.activation_fn\n",
      "model.decoder.layers.8.self_attn_layer_norm\n",
      "model.decoder.layers.8.fc1\n",
      "model.decoder.layers.8.fc2\n",
      "model.decoder.layers.8.final_layer_norm\n",
      "model.decoder.layers.9\n",
      "model.decoder.layers.9.self_attn\n",
      "model.decoder.layers.9.self_attn.k_proj\n",
      "model.decoder.layers.9.self_attn.v_proj\n",
      "model.decoder.layers.9.self_attn.q_proj\n",
      "model.decoder.layers.9.self_attn.out_proj\n",
      "model.decoder.layers.9.activation_fn\n",
      "model.decoder.layers.9.self_attn_layer_norm\n",
      "model.decoder.layers.9.fc1\n",
      "model.decoder.layers.9.fc2\n",
      "model.decoder.layers.9.final_layer_norm\n",
      "model.decoder.layers.10\n",
      "model.decoder.layers.10.self_attn\n",
      "model.decoder.layers.10.self_attn.k_proj\n",
      "model.decoder.layers.10.self_attn.v_proj\n",
      "model.decoder.layers.10.self_attn.q_proj\n",
      "model.decoder.layers.10.self_attn.out_proj\n",
      "model.decoder.layers.10.activation_fn\n",
      "model.decoder.layers.10.self_attn_layer_norm\n",
      "model.decoder.layers.10.fc1\n",
      "model.decoder.layers.10.fc2\n",
      "model.decoder.layers.10.final_layer_norm\n",
      "model.decoder.layers.11\n",
      "model.decoder.layers.11.self_attn\n",
      "model.decoder.layers.11.self_attn.k_proj\n",
      "model.decoder.layers.11.self_attn.v_proj\n",
      "model.decoder.layers.11.self_attn.q_proj\n",
      "model.decoder.layers.11.self_attn.out_proj\n",
      "model.decoder.layers.11.activation_fn\n",
      "model.decoder.layers.11.self_attn_layer_norm\n",
      "model.decoder.layers.11.fc1\n",
      "model.decoder.layers.11.fc2\n",
      "model.decoder.layers.11.final_layer_norm\n",
      "model.decoder.layers.12\n",
      "model.decoder.layers.12.self_attn\n",
      "model.decoder.layers.12.self_attn.k_proj\n",
      "model.decoder.layers.12.self_attn.v_proj\n",
      "model.decoder.layers.12.self_attn.q_proj\n",
      "model.decoder.layers.12.self_attn.out_proj\n",
      "model.decoder.layers.12.activation_fn\n",
      "model.decoder.layers.12.self_attn_layer_norm\n",
      "model.decoder.layers.12.fc1\n",
      "model.decoder.layers.12.fc2\n",
      "model.decoder.layers.12.final_layer_norm\n",
      "model.decoder.layers.13\n",
      "model.decoder.layers.13.self_attn\n",
      "model.decoder.layers.13.self_attn.k_proj\n",
      "model.decoder.layers.13.self_attn.v_proj\n",
      "model.decoder.layers.13.self_attn.q_proj\n",
      "model.decoder.layers.13.self_attn.out_proj\n",
      "model.decoder.layers.13.activation_fn\n",
      "model.decoder.layers.13.self_attn_layer_norm\n",
      "model.decoder.layers.13.fc1\n",
      "model.decoder.layers.13.fc2\n",
      "model.decoder.layers.13.final_layer_norm\n",
      "model.decoder.layers.14\n",
      "model.decoder.layers.14.self_attn\n",
      "model.decoder.layers.14.self_attn.k_proj\n",
      "model.decoder.layers.14.self_attn.v_proj\n",
      "model.decoder.layers.14.self_attn.q_proj\n",
      "model.decoder.layers.14.self_attn.out_proj\n",
      "model.decoder.layers.14.activation_fn\n",
      "model.decoder.layers.14.self_attn_layer_norm\n",
      "model.decoder.layers.14.fc1\n",
      "model.decoder.layers.14.fc2\n",
      "model.decoder.layers.14.final_layer_norm\n",
      "model.decoder.layers.15\n",
      "model.decoder.layers.15.self_attn\n",
      "model.decoder.layers.15.self_attn.k_proj\n",
      "model.decoder.layers.15.self_attn.v_proj\n",
      "model.decoder.layers.15.self_attn.q_proj\n",
      "model.decoder.layers.15.self_attn.out_proj\n",
      "model.decoder.layers.15.activation_fn\n",
      "model.decoder.layers.15.self_attn_layer_norm\n",
      "model.decoder.layers.15.fc1\n",
      "model.decoder.layers.15.fc2\n",
      "model.decoder.layers.15.final_layer_norm\n",
      "model.decoder.layers.16\n",
      "model.decoder.layers.16.self_attn\n",
      "model.decoder.layers.16.self_attn.k_proj\n",
      "model.decoder.layers.16.self_attn.v_proj\n",
      "model.decoder.layers.16.self_attn.q_proj\n",
      "model.decoder.layers.16.self_attn.out_proj\n",
      "model.decoder.layers.16.activation_fn\n",
      "model.decoder.layers.16.self_attn_layer_norm\n",
      "model.decoder.layers.16.fc1\n",
      "model.decoder.layers.16.fc2\n",
      "model.decoder.layers.16.final_layer_norm\n",
      "model.decoder.layers.17\n",
      "model.decoder.layers.17.self_attn\n",
      "model.decoder.layers.17.self_attn.k_proj\n",
      "model.decoder.layers.17.self_attn.v_proj\n",
      "model.decoder.layers.17.self_attn.q_proj\n",
      "model.decoder.layers.17.self_attn.out_proj\n",
      "model.decoder.layers.17.activation_fn\n",
      "model.decoder.layers.17.self_attn_layer_norm\n",
      "model.decoder.layers.17.fc1\n",
      "model.decoder.layers.17.fc2\n",
      "model.decoder.layers.17.final_layer_norm\n",
      "model.decoder.layers.18\n",
      "model.decoder.layers.18.self_attn\n",
      "model.decoder.layers.18.self_attn.k_proj\n",
      "model.decoder.layers.18.self_attn.v_proj\n",
      "model.decoder.layers.18.self_attn.q_proj\n",
      "model.decoder.layers.18.self_attn.out_proj\n",
      "model.decoder.layers.18.activation_fn\n",
      "model.decoder.layers.18.self_attn_layer_norm\n",
      "model.decoder.layers.18.fc1\n",
      "model.decoder.layers.18.fc2\n",
      "model.decoder.layers.18.final_layer_norm\n",
      "model.decoder.layers.19\n",
      "model.decoder.layers.19.self_attn\n",
      "model.decoder.layers.19.self_attn.k_proj\n",
      "model.decoder.layers.19.self_attn.v_proj\n",
      "model.decoder.layers.19.self_attn.q_proj\n",
      "model.decoder.layers.19.self_attn.out_proj\n",
      "model.decoder.layers.19.activation_fn\n",
      "model.decoder.layers.19.self_attn_layer_norm\n",
      "model.decoder.layers.19.fc1\n",
      "model.decoder.layers.19.fc2\n",
      "model.decoder.layers.19.final_layer_norm\n",
      "model.decoder.layers.20\n",
      "model.decoder.layers.20.self_attn\n",
      "model.decoder.layers.20.self_attn.k_proj\n",
      "model.decoder.layers.20.self_attn.v_proj\n",
      "model.decoder.layers.20.self_attn.q_proj\n",
      "model.decoder.layers.20.self_attn.out_proj\n",
      "model.decoder.layers.20.activation_fn\n",
      "model.decoder.layers.20.self_attn_layer_norm\n",
      "model.decoder.layers.20.fc1\n",
      "model.decoder.layers.20.fc2\n",
      "model.decoder.layers.20.final_layer_norm\n",
      "model.decoder.layers.21\n",
      "model.decoder.layers.21.self_attn\n",
      "model.decoder.layers.21.self_attn.k_proj\n",
      "model.decoder.layers.21.self_attn.v_proj\n",
      "model.decoder.layers.21.self_attn.q_proj\n",
      "model.decoder.layers.21.self_attn.out_proj\n",
      "model.decoder.layers.21.activation_fn\n",
      "model.decoder.layers.21.self_attn_layer_norm\n",
      "model.decoder.layers.21.fc1\n",
      "model.decoder.layers.21.fc2\n",
      "model.decoder.layers.21.final_layer_norm\n",
      "model.decoder.layers.22\n",
      "model.decoder.layers.22.self_attn\n",
      "model.decoder.layers.22.self_attn.k_proj\n",
      "model.decoder.layers.22.self_attn.v_proj\n",
      "model.decoder.layers.22.self_attn.q_proj\n",
      "model.decoder.layers.22.self_attn.out_proj\n",
      "model.decoder.layers.22.activation_fn\n",
      "model.decoder.layers.22.self_attn_layer_norm\n",
      "model.decoder.layers.22.fc1\n",
      "model.decoder.layers.22.fc2\n",
      "model.decoder.layers.22.final_layer_norm\n",
      "model.decoder.layers.23\n",
      "model.decoder.layers.23.self_attn\n",
      "model.decoder.layers.23.self_attn.k_proj\n",
      "model.decoder.layers.23.self_attn.v_proj\n",
      "model.decoder.layers.23.self_attn.q_proj\n",
      "model.decoder.layers.23.self_attn.out_proj\n",
      "model.decoder.layers.23.activation_fn\n",
      "model.decoder.layers.23.self_attn_layer_norm\n",
      "model.decoder.layers.23.fc1\n",
      "model.decoder.layers.23.fc2\n",
      "model.decoder.layers.23.final_layer_norm\n",
      "lm_head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:38,  5.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Calibrate model (get inputs to each layer with calibration data)\n",
    "\n",
    "calibration_size=6\n",
    "token_length=512\n",
    "calibrate_on_cpu = False\n",
    "calibration_batch_size=2\n",
    "\n",
    "# First, put in forward hooks\n",
    "features = {}\n",
    "put_input_hooks(model=model, features=features, feature_storage_device='cpu')\n",
    "\n",
    "\n",
    "# run model on batches of calibration data, then concatenate inputs\n",
    "def split_model_calibration(model):\n",
    "    batch_sentences = []\n",
    "    for i, data in tqdm(enumerate(iter(dataset['train']))):\n",
    "        if i < calibration_size + 1:\n",
    "            if len(batch_sentences) >= calibration_batch_size:\n",
    "                encoded_input = tokenizer(batch_sentences, return_tensors=\"pt\", padding=\"max_length\", max_length=token_length, truncation=True)\n",
    "                model(**encoded_input, labels=encoded_input.input_ids)\n",
    "                batch_sentences = []\n",
    "            batch_sentences.append(data['text'])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# print(torch.cuda.memory_allocated(0))\n",
    "split_model_calibration(model)\n",
    "# print(torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.decoder.layers.0.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.0.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.0.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.0.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.1.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.1.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.1.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.1.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.2.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.2.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.2.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.2.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.3.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.3.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.3.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.3.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.4.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.4.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.4.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.4.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.5.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.5.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.5.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.5.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.6.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.6.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.6.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.6.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.7.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.7.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.7.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.7.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.8.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.8.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.8.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.8.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.9.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.9.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.9.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.9.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.10.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.10.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.10.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.10.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.11.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.11.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.11.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.11.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.12.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.12.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.12.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.12.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.13.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.13.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.13.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.13.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.14.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.14.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.14.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.14.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.15.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.15.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.15.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.15.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.16.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.16.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.16.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.16.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.17.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.17.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.17.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.17.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.18.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.18.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.18.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.18.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.19.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.19.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.19.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.19.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.20.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.20.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.20.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.20.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.21.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.21.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.21.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.21.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.22.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.22.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.22.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.22.fc2, shape is torch.Size([3072, 8192])\n",
      "model.decoder.layers.23.self_attn.in_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.23.self_attn.out_proj, shape is torch.Size([6, 512, 2048])\n",
      "model.decoder.layers.23.fc1, shape is torch.Size([3072, 2048])\n",
      "model.decoder.layers.23.fc2, shape is torch.Size([3072, 8192])\n"
     ]
    }
   ],
   "source": [
    "for k in features.keys():\n",
    "    print(f\"{k}, shape is {features[k].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(features[\"model.decoder.layers.1.self_attn.out_proj\"])\n",
    "# print(features[\"model.decoder.layers.1.self_attn_layer_norm\"])\n",
    "print(torch.equal(features[\"model.decoder.layers.5.self_attn.k_proj\"], features[\"model.decoder.layers.5.self_attn.q_proj\"]))\n",
    "# print(torch.equal(features[\"model.decoder.layers.1.final_layer_norm\"], features[\"model.decoder.layers.1.fc1\"]))\n",
    "# print(features[\"model.decoder.layers.1.final_layer_norm\"])\n",
    "# print(features[\"model.decoder.layers.1.fc1\"])\n",
    "# print(features[\"model.decoder.layers.4.self_attn.in_proj\"])\n",
    "# print(features[\"model.decoder.layers.6.self_attn.in_proj\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to access module by name\n",
    "module_lookup_dict = {}\n",
    "for module_name, module_iter in model.named_modules():\n",
    "    module_lookup_dict[module_name] = module_iter\n",
    "EPSILON = 1e-8\n",
    "SPARSENESS = .5\n",
    "B = 2\n",
    "Bs = 2\n",
    "\n",
    "# function to get module name from parameter name\n",
    "def get_module_name(param_name):\n",
    "    if param_name[-5:] == \".bias\":\n",
    "        return param_name[:-5], \"bias\"\n",
    "    elif param_name[-7:] == \".weight\":\n",
    "        return param_name[:-7], \"weight\"\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input_prehooks import get_feature_storage_name\n",
    "\n",
    "layer_blacklist = ['model.decoder.embed_tokens.weight', 'model.decoder.embed_tokens.bias',\n",
    "'model.decoder.embed_positions.weight']\n",
    "\n",
    "# Using calibration data (inputs to each intermediate weight layer)\n",
    "# Iterate through named parameters, calculate inverse hessian and calculate mask\n",
    "\n",
    "# without this\n",
    "param_lookup_dict = {}\n",
    "param_names = []\n",
    "for name, param in model.named_parameters():\n",
    "    param_names.append(name)\n",
    "    param_lookup_dict[name] = param\n",
    "\n",
    "with torch.no_grad():\n",
    "    # for name in tqdm(param_names):\n",
    "    for name in param_names:\n",
    "        param = param_lookup_dict[name]\n",
    "\n",
    "        # skip the embed layer\n",
    "        if name in layer_blacklist:\n",
    "            continue\n",
    "        \n",
    "        # skip norms which have 1 dimension\n",
    "        if len(param.shape) < 2:\n",
    "            continue\n",
    "\n",
    "        module_name, param_type = get_module_name(name)\n",
    "\n",
    "        # apply to weight layers\n",
    "        if param_type == \"weight\":\n",
    "            print(f\"Doing layer {name}\")\n",
    "            # get layer input from features, key is get_feature_storage_name(module_name)\n",
    "            # get_feature_storage_name(module_name) stores k_proj, v_proj, q_proj together\n",
    "            # since they are the same input\n",
    "            layer_input = features[get_feature_storage_name(module_name)].to(device=device)\n",
    "            \n",
    "            # calculate inverse hessian\n",
    "            # check if input is flattened e.g. from 8,512,768 to 4096,768\n",
    "            if len(layer_input.shape) == 2:\n",
    "                inv_hess = inverse_hessian(torch.transpose(layer_input, 0, 1), epsilon=EPSILON, \n",
    "                flattened=True)\n",
    "\n",
    "            else:\n",
    "                inv_hess = inverse_hessian(torch.transpose(layer_input, 1, 2), epsilon=EPSILON,\n",
    "                flattened=False)\n",
    "\n",
    "            # calculate mask\n",
    "            mask = calculate_mask(W=param, H_inv=inv_hess, p=SPARSENESS, B=B, Bs=Bs)\n",
    "            \n",
    "            # get module from lookup dictionary by module name\n",
    "            module = module_lookup_dict[module_name]\n",
    "            # apply mask\n",
    "            prune.custom_from_mask(module=module, name=param_type, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PRUNED MODEL\n",
    "pruned_model_name = f'opt-125m-{SPARSENESS}'\n",
    "# torch.save(model,'pruned_models/' + pruned_model_name)\n",
    "# model.save_pretrained(save_directory = 'pruned_models/' + pruned_model_name)\n",
    "\n",
    "torch.save(model.state_dict(), f'pruned_models/{pruned_model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD SAVED MODEL\n",
    "\n",
    "from save_pruned_model import load_into_model\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "loaded_model = OPTForCausalLM.from_pretrained('facebook/opt-125m', output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "load_into_model(loaded_model, 'pruned_models/opt-125m.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "def get_prop_zeros(model):\n",
    "    return torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) / (torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) + torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight != 0))\n",
    "\n",
    "print(get_prop_zeros(loaded_model))\n",
    "print(get_prop_zeros(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 5])\n",
      "tensor([[3.3387, 1.7186, 2.8955, 3.1412, 2.5768],\n",
      "        [1.7186, 1.4465, 1.6266, 2.0901, 2.3053],\n",
      "        [2.8955, 1.6266, 3.2232, 3.5412, 2.4887],\n",
      "        [3.1412, 2.0901, 3.5412, 4.6508, 3.9475],\n",
      "        [2.5768, 2.3053, 2.4887, 3.9475, 5.1958]], dtype=torch.float64)\n",
      "tensor([[3.5618, 4.1236, 3.2586, 2.8208, 2.7146],\n",
      "        [4.1236, 6.5931, 4.9056, 3.6332, 4.7593],\n",
      "        [3.2586, 4.9056, 4.1975, 2.8734, 3.5105],\n",
      "        [2.8208, 3.6332, 2.8734, 3.3229, 2.9970],\n",
      "        [2.7146, 4.7593, 3.5105, 2.9970, 4.0479]], dtype=torch.float64)\n",
      "tensor([[6.8906, 5.8422, 6.1541, 5.9621, 5.2914],\n",
      "        [5.8422, 8.0296, 6.5322, 5.7233, 7.0647],\n",
      "        [6.1541, 6.5322, 7.4107, 6.4146, 5.9992],\n",
      "        [5.9621, 5.7233, 6.4146, 7.9637, 6.9445],\n",
      "        [5.2914, 7.0647, 5.9992, 6.9445, 9.2337]], dtype=torch.float64)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def hessian(X, epsilon=0.01, flattened=False):\n",
    "    X = X.double()\n",
    "\n",
    "    if flattened:\n",
    "        X_T = torch.transpose(X, 0, 1)\n",
    "        identity = torch.eye(X.shape[0], dtype=torch.float64, device=device)\n",
    "        # print(f\"shape of x @ x_t: {torch.sum(X @ X_T, dim=0).shape}\")\n",
    "        H = 2 * (X @ X_T) + (epsilon * identity)\n",
    "    else:\n",
    "        X_T = torch.transpose(X, 1, 2)\n",
    "        identity = torch.eye(X.shape[1], dtype=torch.float64, device=device)\n",
    "        # print(f\"shape of x @ x_t: {torch.sum(X @ X_T, dim=0).shape}\")\n",
    "        H = 2 * (torch.sum(X @ X_T, dim=0)) + (epsilon * identity)\n",
    "\n",
    "    return H\n",
    "\n",
    "test_tensor = torch.rand(2, 3, 5)\n",
    "flattened_tensor = torch.flatten(test_tensor, start_dim=0, end_dim=1)\n",
    "# print(inverse_hessian(torch.transpose(test_tensor, 1, 2), flattened=False))\n",
    "# print(inverse_hessian(torch.transpose(flattened_tensor, 0, 1), flattened=True))\n",
    "\n",
    "test_tensor_2 = torch.rand(2, 3, 5)\n",
    "flattened_tensor_2 = torch.flatten(test_tensor_2, start_dim=0, end_dim=1)\n",
    "\n",
    "# comb_tensor = torch.cat((test_tensor, test_tensor_2,))\n",
    "comb_tensor = torch.cat((flattened_tensor, flattened_tensor_2,))\n",
    "print(comb_tensor.shape)\n",
    "# flattened_tensor_3 = torch.flatten(comb_tensor, start_dim=0, end_dim=1)\n",
    "\n",
    "# print(inverse_hessian(torch.transpose(comb_tensor, 1, 2), flattened=False))\n",
    "print(hessian(torch.transpose(flattened_tensor, 0, 1), flattened=True))\n",
    "print(hessian(torch.transpose(flattened_tensor_2, 0, 1), flattened=True))\n",
    "print(hessian(torch.transpose(comb_tensor, 0, 1), flattened=True))\n",
    "print(hessian(torch.transpose(comb_tensor, 0, 1), flattened=True) - \n",
    "    (hessian(torch.transpose(flattened_tensor, 0, 1), flattened=True) + hessian(torch.transpose(flattened_tensor_2, 0, 1), flattened=True)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# REGULAR OUTPUT\n",
    "dense_model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True).to(device=device)\n",
    "encoded_test_input = tokenizer('What did you just say to me? I will have you know', return_tensors=\"pt\",\n",
    "                                                                                    padding=\"max_length\", \n",
    "                                                                                    max_length=token_length, \n",
    "                                                                                    truncation=True)\n",
    "#print(encoded_test_input)\n",
    "print('DENSE MODEL:')\n",
    "with torch.no_grad():\n",
    "    generated_ids = dense_model.generate(**encoded_test_input, max_new_tokens=30, num_beams=5, do_sample=True)\n",
    "dense_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(f'\\tOutput: {dense_output}')\n",
    "\n",
    "print('SPARSE MODEL: ')\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**encoded_test_input, max_new_tokens=30, num_beams=5, do_sample=True)\n",
    "sparse_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(f'\\tOutput: {sparse_output}')''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True).to(device=device)\n",
    "encoded_test_input = tokenizer('What did you just say to me? I will have you know', return_tensors=\"pt\",\n",
    "                                                                                    padding=\"max_length\", \n",
    "                                                                                    max_length=token_length, \n",
    "                                                                                    truncation=True)\n",
    "print(torch.exp(dense_model(**encoded_test_input, labels = encoded_test_input.input_ids).loss))\n",
    "print(torch.exp(model(**encoded_test_input, labels = encoded_test_input.input_ids).loss))\n",
    "print(torch.exp(loaded_model(**encoded_test_input, labels = encoded_test_input.input_ids).loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_dataset('wikitext', 'wikitext-2-v1', split='test[:10%]')\n",
    "tokenized_test = tokenizer(test_set['text'])\n",
    "\n",
    "flattened_input_ids = [item for sublist in tokenized_test.input_ids for item in sublist]\n",
    "flattened_input_ids = flattened_input_ids[:(len(flattened_input_ids) - (len(flattened_input_ids) % token_length))]\n",
    "flattened_input_ids = torch.Tensor(flattened_input_ids).reshape(-1, token_length).int()\n",
    "\n",
    "flattened_masks = [item for sublist in tokenized_test.attention_mask for item in sublist]\n",
    "flattened_masks = flattened_masks[:(len(flattened_masks) - (len(flattened_masks) % token_length))]\n",
    "flattened_masks = torch.Tensor(flattened_masks).reshape(-1, token_length).int()\n",
    "\n",
    "test_dict = {'input_ids': flattened_input_ids, 'attention_mask': flattened_masks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dense_model.eval()\n",
    "output = model(**test_dict, labels=test_dict['input_ids'])\n",
    "output2 = dense_model(**test_dict, labels=test_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.exp(output.loss))\n",
    "print(torch.exp(output2.loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('pyt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01ca8e50707366aa71ffa41d1a1f13415d8b38eae741916c87480058f12910d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

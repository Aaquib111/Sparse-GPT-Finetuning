{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from calculate_mask import calculate_mask\n",
    "from inverse_hessian import inverse_hessian\n",
    "from input_prehooks import put_input_hooks\n",
    "from testing_module import calculate_perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "#Load dataset\n",
    "dataset = load_dataset('c4', 'en', streaming=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "# Load model with pre-trained head\n",
    "model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "#model = torch.nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "\n",
    "# Load generator\n",
    "generator = pipeline('text-generation', model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate model (get inputs to each layer with calibration data)\n",
    "\n",
    "calibration_size=128\n",
    "token_length=1024\n",
    "calibrate_on_cpu = False\n",
    "calibration_batch_size=2\n",
    "\n",
    "# First, put in forward hooks\n",
    "features = {}\n",
    "put_input_hooks(model=model, features=features, feature_storage_device='cpu')\n",
    "\n",
    "model.eval()\n",
    "# run model on batches of calibration data, then concatenate inputs\n",
    "def split_model_calibration(model):\n",
    "    batch_sentences = []\n",
    "    for i, data in tqdm(enumerate(iter(dataset['train']))):\n",
    "        if i < calibration_size + 1:\n",
    "            if len(batch_sentences) >= calibration_batch_size:\n",
    "                encoded_input = tokenizer(batch_sentences, return_tensors=\"pt\", padding=\"max_length\", max_length=token_length, truncation=True).to(device=device)\n",
    "                with torch.no_grad():\n",
    "                    model(**encoded_input, labels=encoded_input.input_ids)\n",
    "                batch_sentences = []\n",
    "            batch_sentences.append(data['text'])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "split_model_calibration(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to access module by name\n",
    "module_lookup_dict = {}\n",
    "for module_name, module_iter in model.named_modules():\n",
    "    module_lookup_dict[module_name] = module_iter\n",
    "EPSILON = 1e-8\n",
    "SPARSENESS = .8\n",
    "B = 128\n",
    "Bs = 64\n",
    "\n",
    "# function to get module name from parameter name\n",
    "def get_module_name(param_name):\n",
    "    if param_name[-5:] == \".bias\":\n",
    "        return param_name[:-5], \"bias\"\n",
    "    elif param_name[-7:] == \".weight\":\n",
    "        return param_name[:-7], \"weight\"\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [02:14<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "layer_blacklist = ['model.decoder.embed_tokens.weight', 'model.decoder.embed_tokens.bias',\n",
    "'model.decoder.embed_positions.weight']\n",
    "\n",
    "# Using calibration data (inputs to each intermediate weight layer)\n",
    "# Iterate through named parameters, calculate inverse hessian and calculate mask\n",
    "\n",
    "# without this\n",
    "param_lookup_dict = {}\n",
    "param_names = []\n",
    "for name, param in model.named_parameters():\n",
    "    param_names.append(name)\n",
    "    param_lookup_dict[name] = param\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name in tqdm(param_names):\n",
    "        param = param_lookup_dict[name]\n",
    "\n",
    "        # skip the embed layer\n",
    "        if name in layer_blacklist:\n",
    "            continue\n",
    "        \n",
    "        # skip norms which have 1 dimension\n",
    "        if len(param.shape) < 2:\n",
    "            continue\n",
    "\n",
    "        module_name, param_type = get_module_name(name)\n",
    "\n",
    "        # apply to weight and bias layers\n",
    "        if param_type == \"weight\" or param_type == \"bias\":\n",
    "            # input to parameter\n",
    "            layer_input = features[module_name].to(device=device)\n",
    "            # calculate inverse hessian\n",
    "            # check if input is flattened e.g. from 8,512,768 to 4096,768\n",
    "            if len(layer_input.shape) == 2:\n",
    "                inv_hess = inverse_hessian(torch.transpose(layer_input, 0, 1), epsilon=EPSILON, \n",
    "                flattened=True).to(device=device)\n",
    "\n",
    "            else:\n",
    "                inv_hess = inverse_hessian(torch.transpose(layer_input, 1, 2), epsilon=EPSILON,\n",
    "                flattened=False).to(device=device)\n",
    "\n",
    "            # calculate mask\n",
    "            mask = calculate_mask(W=param, H_inv=inv_hess, p=SPARSENESS, B=B, Bs=Bs)\n",
    "            \n",
    "            # get module from lookup dictionary by module name\n",
    "            module = module_lookup_dict[module_name]\n",
    "            # apply mask\n",
    "            prune.custom_from_mask(module=module, name=param_type, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PRUNED MODEL\n",
    "pruned_model_name = f'opt-125m-{SPARSENESS}'\n",
    "# torch.save(model,'pruned_models/' + pruned_model_name)\n",
    "# model.save_pretrained(save_directory = 'pruned_models/' + pruned_model_name)\n",
    "\n",
    "torch.save(model.state_dict(), f'pruned_models/{pruned_model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:00<00:00, 13909.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# LOAD SAVED MODEL\n",
    "\n",
    "from save_pruned_model import load_into_model\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "loaded_model = OPTForCausalLM.from_pretrained('facebook/opt-125m', output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "load_into_model(loaded_model, f'pruned_models/{pruned_model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model_name = \"facebook/opt-125m\"\n",
    "\n",
    "def get_prop_zeros(model):\n",
    "    return torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) / (torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) + torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight != 0))\n",
    "\n",
    "print(get_prop_zeros(loaded_model))\n",
    "print(get_prop_zeros(model))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pruned_model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin()\n\u001b[0;32m----> 3\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mICLR\u001b[39m\u001b[38;5;124m\"\u001b[39m, name \u001b[38;5;241m=\u001b[39m \u001b[43mpruned_model_name\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pruned_model_name' is not defined"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"ICLR\", name = pruned_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENSE MODEL:\n",
      "\tOutput: [\"What did you just say to me? I will have you know that I am not the only one.\\nI'm not the only one.\"]\n",
      "SPARSE MODEL: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 128 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSPARSE MODEL: \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 16\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_test_input, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m sparse_output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Aaquib\\OneDrive\\Documents\\ICLR\\env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aaquib\\OneDrive\\Documents\\ICLR\\env\\lib\\site-packages\\transformers\\generation\\utils.py:1511\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[0;32m   1503\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1504\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1505\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams \u001b[39m*\u001b[39m generation_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1506\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1507\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1508\u001b[0m     )\n\u001b[0;32m   1510\u001b[0m     \u001b[39m# 14. run beam sample\u001b[39;00m\n\u001b[1;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_sample(\n\u001b[0;32m   1512\u001b[0m         input_ids,\n\u001b[0;32m   1513\u001b[0m         beam_scorer,\n\u001b[0;32m   1514\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1515\u001b[0m         logits_warper\u001b[39m=\u001b[39mlogits_warper,\n\u001b[0;32m   1516\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1517\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m   1518\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m   1519\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m   1520\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1521\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   1522\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1523\u001b[0m     )\n\u001b[0;32m   1525\u001b[0m \u001b[39melif\u001b[39;00m is_group_beam_gen_mode:\n\u001b[0;32m   1526\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[1;32mc:\\Users\\Aaquib\\OneDrive\\Documents\\ICLR\\env\\lib\\site-packages\\transformers\\generation\\utils.py:3048\u001b[0m, in \u001b[0;36mGenerationMixin.beam_sample\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3044\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   3046\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> 3048\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   3049\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   3050\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   3051\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   3052\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   3053\u001b[0m )\n\u001b[0;32m   3055\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   3056\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Aaquib\\OneDrive\\Documents\\ICLR\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[1;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[1;32mc:\\Users\\Aaquib\\OneDrive\\Documents\\ICLR\\env\\lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:932\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    929\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m    931\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 932\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m    933\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    934\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    935\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    936\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    937\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    938\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    939\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    940\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    941\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    942\u001b[0m )\n\u001b[0;32m    944\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    946\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aaquib\\OneDrive\\Documents\\ICLR\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[1;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[1;32mc:\\Users\\Aaquib\\OneDrive\\Documents\\ICLR\\env\\lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:632\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    629\u001b[0m past_key_values_length \u001b[39m=\u001b[39m past_key_values[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 632\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_tokens(input_ids)\n\u001b[0;32m    634\u001b[0m \u001b[39m# embed positions\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Aaquib\\OneDrive\\Documents\\ICLR\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1201\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks:\n\u001b[0;32m   1200\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_pre_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m-> 1201\u001b[0m         result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m)\n\u001b[0;32m   1202\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Aaquib\\OneDrive\\Documents\\ICLR\\input_prehooks.py:27\u001b[0m, in \u001b[0;36mput_input_hooks.<locals>.get_features.<locals>.pre_hook\u001b[1;34m(model, input)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39minput\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     25\u001b[0m     \u001b[39m# concatenate with self\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m features:\n\u001b[1;32m---> 27\u001b[0m         features[name] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((features[name], \u001b[39minput\u001b[39;49m[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mfeature_storage_device)), dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     28\u001b[0m     \u001b[39m# make new entry if not existing\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m         features[name] \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mfeature_storage_device)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 128 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "'''# REGULAR OUTPUT\n",
    "dense_model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True).to(device=device)\n",
    "encoded_test_input = tokenizer('What did you just say to me? I will have you know', return_tensors=\"pt\",\n",
    "                                                                                    padding=\"max_length\", \n",
    "                                                                                    max_length=token_length, \n",
    "                                                                                    truncation=True)\n",
    "#print(encoded_test_input)\n",
    "print('DENSE MODEL:')\n",
    "with torch.no_grad():\n",
    "    generated_ids = dense_model.generate(**encoded_test_input, max_new_tokens=30, num_beams=5, do_sample=True)\n",
    "dense_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(f'\\tOutput: {dense_output}')\n",
    "\n",
    "print('SPARSE MODEL: ')\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**encoded_test_input, max_new_tokens=30, num_beams=5, do_sample=True)\n",
    "sparse_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(f'\\tOutput: {sparse_output}')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9591.2529, grad_fn=<ExpBackward0>)\n",
      "tensor(219081.9531, grad_fn=<ExpBackward0>)\n",
      "tensor(2541604.7500, grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''dense_model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True).to(device=device)\n",
    "encoded_test_input = tokenizer('What did you just say to me? I will have you know', return_tensors=\"pt\",\n",
    "                                                                                    padding=\"max_length\", \n",
    "                                                                                    max_length=token_length, \n",
    "                                                                                    truncation=True).to(device=device)\n",
    "print(torch.exp(dense_model(**encoded_test_input, labels = encoded_test_input.input_ids).loss))\n",
    "print(torch.exp(model(**encoded_test_input, labels = encoded_test_input.input_ids).loss))\n",
    "print(torch.exp(loaded_model(**encoded_test_input, labels = encoded_test_input.input_ids).loss))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.nn.DataParallel(loaded_model, device_ids=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/Aaquib/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "#test_set = load_dataset('wikitext', 'wikitext-2-v1', split='test[:10%]')\n",
    "test_set = load_dataset('wikitext', 'wikitext-2-v1', split='test')\n",
    "tokenized_test = tokenizer(test_set['text'])\n",
    "\n",
    "flattened_input_ids = [item for sublist in tokenized_test.input_ids for item in sublist]\n",
    "flattened_input_ids = flattened_input_ids[:(len(flattened_input_ids) - (len(flattened_input_ids) % token_length))]\n",
    "flattened_input_ids = torch.Tensor(flattened_input_ids).reshape(-1, token_length).type(torch.LongTensor).to(device=device)\n",
    "\n",
    "flattened_masks = [item for sublist in tokenized_test.attention_mask for item in sublist]\n",
    "flattened_masks = flattened_masks[:(len(flattened_masks) - (len(flattened_masks) % token_length))]\n",
    "flattened_masks = torch.Tensor(flattened_masks).reshape(-1, token_length).type(torch.LongTensor).to(device=device)\n",
    "\n",
    "test_dict = {'input_ids': flattened_input_ids, 'attention_mask': flattened_masks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "loaded_model.eval()\n",
    "sum_perp_sparse = 0\n",
    "sum_perp_dense = 0\n",
    "with torch.no_grad():\n",
    "    #for input_id, attention_mask in zip(test_dict['input_ids'], test_dict['attention_mask']):\n",
    "        #batch_dict = {'input_ids': input_id, 'attention_mask': attention_mask}\n",
    "    sparse_output = loaded_model(**test_dict, labels=test_dict['input_ids'])\n",
    "    sum_perp_sparse += torch.exp(sparse_output.loss)\n",
    "#sum_perp_sparse /= len(test_dict['input_ids'])\n",
    "#sum_perp_dense /= len(test_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.log({\"sparse_perplexity\": sum_perp_sparse, \"dense_perplexity\": sum_perp_dense})\n",
    "wandb.log({\"sparse_perplexity\": sum_perp_sparse})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum_perp_sparse)\n",
    "print(sum_perp_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3598d1fcb68810e1ab8d19050593974172dcc02c5f12874feb8b1a070749563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

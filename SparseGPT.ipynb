{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d6f37c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phillipguo/opt/anaconda3/envs/pyt/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "'''\n",
    "  Calculates inverse hessian matrix given input X\n",
    "  X is a dxn matrix, where d is num_features and n is num_examples in dataset\n",
    "'''\n",
    "def inverse_hessian(X, l):\n",
    "  X = X.to(device)\n",
    "  H_inv = torch.inverse(2 * X @ X.t() + (l * torch.eye(X.shape[0])))\n",
    "  return H_inv.to(device)\n",
    "\n",
    "'''\n",
    "W: Layer matrix\n",
    "H_inv: Inverse Hessian\n",
    "p: percent pruning\n",
    "B: batch-update blocksize\n",
    "Bs: adaptive mask selection blocksize\n",
    "'''\n",
    "def prune_model(W, H_inv, p, B, Bs):\n",
    "    # Matrices to device\n",
    "    W = W.to(device)\n",
    "\n",
    "    d_row, d_col = W.shape\n",
    "    M = torch.ones(d_row, d_col, dtype=torch.float32, device=device) # 0/1 pruning mask\n",
    "    E = torch.zeros(d_row, B, dtype=torch.float32, device=device) # block quantization errors\n",
    "    \n",
    "    H_inv_T = torch.transpose(torch.cholesky(H_inv), 0, 1) # Hessian inverse information; upper triangular\n",
    "    for i in range(0, d_col, B):\n",
    "        for j in range(i, min(i + B, d_col)):\n",
    "            if j % Bs == 0:\n",
    "                # mask of (1 - p)% weights wc with largest w^2c / [H_inv]^2cc\n",
    "                block = W[:, j:j+Bs].clone().detach().to(\"cpu\").numpy()\n",
    "                H_inv_block = H_inv_T[j:j+Bs, j:j+Bs].clone().detach().to(\"cpu\").numpy()\n",
    "                weights_squared = np.square(block)\n",
    "                H_inv_squared = np.square(H_inv_block)\n",
    "                weights_normalized = weights_squared / H_inv_squared\n",
    "                k = int(np.floor((1 - p) * Bs))\n",
    "                prune_index = np.argpartition(weights_normalized, -k, axis=1)[:, :k]\n",
    "                prune_mask = np.zeros(block.shape, dtype=np.bool)\n",
    "                prune_mask[np.arange(d_row)[:, None], prune_index] = 1\n",
    "                M[:, j:j+Bs] = torch.from_numpy(prune_mask).to(device).float()\n",
    "            # pruning error\n",
    "            E[:, j-i] = W[:, j] / H_inv[j, j]\n",
    "            # freeze weights that are not pruned\n",
    "            E[:, j-i] *= (1 - M[:, j]).float()\n",
    "            # update weights in block\n",
    "            W[:, j:i+B] -= torch.mm(E[:, j-i][:, None], H_inv_T[j, j:i+B][None, :])\n",
    "        # update all remaining weights\n",
    "        W[:, i+B:] -= torch.mm(E, H_inv_T[i+B:, i+B:])\n",
    "\n",
    "    W *= M.float() # set pruned weights to 0\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = torch.tensor([[2, 0.5], [0.5, 3]], dtype=torch.float32)\n",
    "print(inverse_hessian(H, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf348a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "# Load model with pre-trained head\n",
    "model = AutoModel.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "generator = pipeline('text-generation', model=\"facebook/opt-125m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a4cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(\"Bush did 9/11 because\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5af3ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.embed_tokens.weight\n",
      "torch.Size([50272, 768])\n",
      "decoder.embed_positions.weight\n",
      "torch.Size([2050, 768])\n",
      "decoder.final_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.final_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.0.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.0.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.0.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.0.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.0.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.0.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.0.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.0.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.0.self_attn_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.0.self_attn_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.0.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "decoder.layers.0.fc1.bias\n",
      "torch.Size([3072])\n",
      "decoder.layers.0.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "decoder.layers.0.fc2.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.0.final_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.0.final_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.1.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.1.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.1.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.1.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.1.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.1.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.1.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.1.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.1.self_attn_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.1.self_attn_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.1.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "decoder.layers.1.fc1.bias\n",
      "torch.Size([3072])\n",
      "decoder.layers.1.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "decoder.layers.1.fc2.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.1.final_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.1.final_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.2.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.2.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.2.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.2.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.2.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.2.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.2.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.2.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.2.self_attn_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.2.self_attn_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.2.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "decoder.layers.2.fc1.bias\n",
      "torch.Size([3072])\n",
      "decoder.layers.2.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "decoder.layers.2.fc2.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.2.final_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.2.final_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.3.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.3.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.3.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.3.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.3.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.3.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.3.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.3.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.3.self_attn_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.3.self_attn_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.3.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "decoder.layers.3.fc1.bias\n",
      "torch.Size([3072])\n",
      "decoder.layers.3.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "decoder.layers.3.fc2.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.3.final_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.3.final_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.4.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.4.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.4.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.4.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.4.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.4.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.4.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.4.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.4.self_attn_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.4.self_attn_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.4.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "decoder.layers.4.fc1.bias\n",
      "torch.Size([3072])\n",
      "decoder.layers.4.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "decoder.layers.4.fc2.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.4.final_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.4.final_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.5.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.5.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.5.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.5.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.5.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.5.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.5.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.5.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.5.self_attn_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.5.self_attn_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.5.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "decoder.layers.5.fc1.bias\n",
      "torch.Size([3072])\n",
      "decoder.layers.5.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "decoder.layers.5.fc2.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.5.final_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.5.final_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.6.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.6.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.6.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.6.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.6.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.6.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.6.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.6.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.6.self_attn_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.6.self_attn_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.6.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "decoder.layers.6.fc1.bias\n",
      "torch.Size([3072])\n",
      "decoder.layers.6.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "decoder.layers.6.fc2.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.6.final_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.6.final_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.7.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.7.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.7.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.7.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.7.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.7.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.7.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.7.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.7.self_attn_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.7.self_attn_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.7.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "decoder.layers.7.fc1.bias\n",
      "torch.Size([3072])\n",
      "decoder.layers.7.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "decoder.layers.7.fc2.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.7.final_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.7.final_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.8.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.8.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.8.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.8.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.8.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.8.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.8.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.8.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.8.self_attn_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.8.self_attn_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.8.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "decoder.layers.8.fc1.bias\n",
      "torch.Size([3072])\n",
      "decoder.layers.8.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "decoder.layers.8.fc2.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.8.final_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.8.final_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.9.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.9.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.9.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.9.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.9.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.9.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.9.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.9.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.9.self_attn_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.9.self_attn_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.9.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "decoder.layers.9.fc1.bias\n",
      "torch.Size([3072])\n",
      "decoder.layers.9.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "decoder.layers.9.fc2.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.9.final_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.9.final_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.10.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.10.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.10.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.10.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.10.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.10.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.10.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.10.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.10.self_attn_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.10.self_attn_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.10.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "decoder.layers.10.fc1.bias\n",
      "torch.Size([3072])\n",
      "decoder.layers.10.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "decoder.layers.10.fc2.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.10.final_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.10.final_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.11.self_attn.k_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.11.self_attn.k_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.11.self_attn.v_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.11.self_attn.v_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.11.self_attn.q_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.11.self_attn.q_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.11.self_attn.out_proj.weight\n",
      "torch.Size([768, 768])\n",
      "decoder.layers.11.self_attn.out_proj.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.11.self_attn_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.11.self_attn_layer_norm.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.11.fc1.weight\n",
      "torch.Size([3072, 768])\n",
      "decoder.layers.11.fc1.bias\n",
      "torch.Size([3072])\n",
      "decoder.layers.11.fc2.weight\n",
      "torch.Size([768, 3072])\n",
      "decoder.layers.11.fc2.bias\n",
      "torch.Size([768])\n",
      "decoder.layers.11.final_layer_norm.weight\n",
      "torch.Size([768])\n",
      "decoder.layers.11.final_layer_norm.bias\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d07ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pyt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "01ca8e50707366aa71ffa41d1a1f13415d8b38eae741916c87480058f12910d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

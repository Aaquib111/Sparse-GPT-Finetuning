{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from calculate_mask import calculate_mask\n",
    "from inverse_hessian import inverse_hessian\n",
    "from input_prehooks import put_input_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "#Load dataset\n",
    "dataset = load_dataset('c4', 'en', streaming=True)\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load model with pre-trained head\n",
    "\n",
    "model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "# Load generator\n",
    "generator = pipeline('text-generation', model=model_name)\n",
    "# Create calibration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate model (get inputs to each layer with calibration data)\n",
    "\n",
    "calibration_size=4\n",
    "token_length=512\n",
    "calibrate_on_cpu = False\n",
    "calibration_batch_size=2\n",
    "\n",
    "calibration_data_array = []\n",
    "for i, data in enumerate(iter(dataset['train'])): # type: ignore\n",
    "    if i >= calibration_size:\n",
    "        break\n",
    "    tokenized = tokenizer.encode(data['text'], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=token_length)\n",
    "    calibration_data_array.append(tokenized)\n",
    "calibration_data = torch.squeeze(torch.stack(calibration_data_array)).to(device='cpu')\n",
    "# calibration_data.double()\n",
    "\n",
    "# First, put in forward hooks\n",
    "features = {}\n",
    "store_features_cpu = True # if you want to store features on the cpu or on device, cpu for less vram demand\n",
    "\n",
    "if store_features_cpu:\n",
    "    # store features on cpu device with shared memory so vram isn't hogged\n",
    "    put_input_hooks(model=model, features=features, feature_storage_device='cpu')\n",
    "else:\n",
    "    put_input_hooks(model=model, features=features, feature_storage_device=device)\n",
    "\n",
    "\n",
    "# run model on batches of calibration data, then concatenate inputs\n",
    "def split_model_calibration(model, calibration_data, batch_size=2):\n",
    "    # split into batches of batch_size\n",
    "    split_data = torch.split(calibration_data, split_size_or_sections=batch_size, dim=0)\n",
    "\n",
    "    # iterate through split_data and calibrate\n",
    "    for batch in split_data:\n",
    "        model(batch.detach().to(device=device))\n",
    "\n",
    "\n",
    "# calibrate model on cpu for less vram\n",
    "if calibrate_on_cpu:\n",
    "    model.to(device='cpu')\n",
    "\n",
    "    # Run calibration data through model at first to calculate features dictionary with\n",
    "    # input tensors to each intermediate layer\n",
    "    # at first, run on cpu for less memory?\n",
    "    calibration_data = torch.squeeze(torch.stack(calibration_data_array)).to(device='cpu')\n",
    "    # calibration_data.double()\n",
    "    \n",
    "    split_model_calibration(model, calibration_data, batch_size=calibration_batch_size)\n",
    "\n",
    "    # send model back to cpu\n",
    "    model.to(device=device)\n",
    "else:\n",
    "    split_model_calibration(model, calibration_data, batch_size=calibration_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary to access module by name\n",
    "module_lookup_dict = {}\n",
    "for module_name, module_iter in model.named_modules():\n",
    "    module_lookup_dict[module_name] = module_iter\n",
    "EPSILON = 0.0001\n",
    "SPARSENESS = .9\n",
    "B = 128\n",
    "Bs = 64\n",
    "\n",
    "# function to get module name from parameter name\n",
    "def get_module_name(param_name):\n",
    "    if param_name[-5:] == \".bias\":\n",
    "        return param_name[:-5], \"bias\"\n",
    "    elif param_name[-7:] == \".weight\":\n",
    "        return param_name[:-7], \"weight\"\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "layer_blacklist = ['model.decoder.embed_tokens.weight', 'model.decoder.embed_tokens.bias',\n",
    "'model.decoder.embed_positions.weight']\n",
    "\n",
    "# Using calibration data (inputs to each intermediate weight layer)\n",
    "# Iterate through named parameters, calculate inverse hessian and calculate mask\n",
    "\n",
    "# without this\n",
    "param_lookup_dict = {}\n",
    "param_names = []\n",
    "for name, param in model.named_parameters():\n",
    "    param_names.append(name)\n",
    "    param_lookup_dict[name] = param\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name in tqdm(param_names):\n",
    "        param = param_lookup_dict[name]\n",
    "\n",
    "        # skip the embed layer\n",
    "        if name in layer_blacklist:\n",
    "            continue\n",
    "        \n",
    "        # skip norms which have 1 dimension\n",
    "        if len(param.shape) < 2:\n",
    "            continue\n",
    "\n",
    "        module_name, param_type = get_module_name(name)\n",
    "\n",
    "        # apply to weight and bias layers\n",
    "        if param_type == \"weight\" or param_type == \"bias\":\n",
    "            # input to parameter\n",
    "            layer_input = features[module_name].to(device=device)\n",
    "\n",
    "            # calculate inverse hessian\n",
    "            # check if input is flattened e.g. from 8,512,768 to 4096,768\n",
    "            if len(layer_input.shape) == 2:\n",
    "                inv_hess = inverse_hessian(torch.transpose(layer_input, 0, 1), epsilon=EPSILON, \n",
    "                flattened=True)\n",
    "\n",
    "            else:\n",
    "                inv_hess = inverse_hessian(torch.transpose(layer_input, 1, 2), epsilon=EPSILON,\n",
    "                flattened=False)\n",
    "\n",
    "            # calculate mask\n",
    "            mask = calculate_mask(W=param, H_inv=inv_hess, p=SPARSENESS, B=B, Bs=Bs)\n",
    "            \n",
    "            # get module from lookup dictionary by module name\n",
    "            module = module_lookup_dict[module_name]\n",
    "            # apply mask\n",
    "            prune.custom_from_mask(module=module, name=param_type, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PRUNED MODEL\n",
    "pruned_model_name = 'opt-125m'\n",
    "# torch.save(model,'pruned_models/' + pruned_model_name)\n",
    "# model.save_pretrained(save_directory = 'pruned_models/' + pruned_model_name)\n",
    "\n",
    "torch.save(model.state_dict(), 'pruned_models/opt-125m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:00<00:00, 1122.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# LOAD SAVED MODEL\n",
    "\n",
    "from save_pruned_model import load_into_model\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "loaded_model = OPTForCausalLM.from_pretrained('facebook/opt-125m', output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "load_into_model(loaded_model, 'pruned_models/opt-125m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, my dog is cute, my dog is cute, my dog is cute, my dog is cute, my dog is cute, my dog is cute, my dog is cute, my dog is cute, my dog is cute, my dog is cute, my dog is cute, my dog is cute, my dog is cute, my dog is cute, my dog is cute, my, dog is cute, dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog dog'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "#Load dataset\n",
    "dataset = load_dataset('c4', 'en', streaming=True)\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "input1 = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device=device)\n",
    "input2 = tokenizer(\"What the fuck did you just fucking say about me, you little bitch?\", return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device=device)\n",
    "output = loaded_model.generate(input1.input_ids, max_length=100, num_return_sequences=1, temperature=0.9, top_p=0.95)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1000)\n"
     ]
    }
   ],
   "source": [
    "# Proportion of weights that are 0:\n",
    "\n",
    "def get_prop_zeros(model):\n",
    "    return torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) / (torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) + torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight != 0))\n",
    "\n",
    "print(get_prop_zeros(loaded_model))\n",
    "# print(get_prop_zeros(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGULAR OUTPUT\n",
    "model2 = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20443.5879, grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from testing_module import calculate_perp\n",
    "\n",
    "# def calculate_perp(model, input_data, device):\n",
    "#     input_data = torch.squeeze(torch.stack(input_data)).to(device=device)\n",
    "#     input_data.double()\n",
    "#     outputs = model(input_data)[0] \n",
    "#     log_probs = outputs[0, -1, :].log_softmax(-1)\n",
    "#     neg_log_likelihood = -log_probs.mean()\n",
    "#     perplexity = torch.exp(neg_log_likelihood)      \n",
    "#     return perplexity.item()\n",
    "\n",
    "# print(calculate_perp(model, input_data, device))\n",
    "\n",
    "dataset = load_dataset('c4', 'en', streaming=True)\n",
    "input_data = []\n",
    "for i, data in enumerate(iter(dataset['train'])):\n",
    "    if i > 7:\n",
    "        break\n",
    "    tokenized = tokenizer.encode(data['text'], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "    input_data.append(tokenized)\n",
    "input_data = torch.squeeze(torch.stack(input_data)).to(device=device)\n",
    "input_data.double()\n",
    "\n",
    "# lmao out of memory\n",
    "print(calculate_perp(loaded_model, input_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('pyt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01ca8e50707366aa71ffa41d1a1f13415d8b38eae741916c87480058f12910d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from calculate_mask import calculate_mask\n",
    "from inverse_hessian import calc_inverse_hessian\n",
    "from input_prehooks import put_input_hooks\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OPTForCausalLM.from_pretrained(f'facebook/opt-125m', output_attentions=True,\n",
    "                                           output_hidden_states=True) # type: ignore\n",
    "model = torch.nn.DataParallel(model, device_ids=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 25 00:47:45 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    35W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCI...  On   | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    37W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCI...  On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    34W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PCI...  On   | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    33W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run model on batches of calibration data, then concatenate inputs\n",
    "def split_model_calibration(model):\n",
    "    batch_sentences = []\n",
    "    for i, data in tqdm(enumerate(iter(dataset['train'])), total=calibration_size):\n",
    "        if i < calibration_size + 1:\n",
    "            if len(batch_sentences) >= calibration_batch_size:\n",
    "                with torch.no_grad():\n",
    "                    encoded_input = tokenizer(batch_sentences, return_tensors=\"pt\",\n",
    "                                              padding=\"max_length\", max_length=token_length,\n",
    "                                              truncation=True).to(device=device)\n",
    "                    model(**encoded_input, labels=encoded_input.input_ids)\n",
    "                    del encoded_input\n",
    "                    torch.cuda.empty_cache()\n",
    "                    batch_sentences = []\n",
    "            batch_sentences.append(data['text'])\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparseGPT fine tune loop\n",
    "import torch\n",
    "\n",
    "def get_prop_zeros(model):\n",
    "    return torch.sum(model.module.get_decoder().layers[0].self_attn.k_proj.weight == 0) / (torch.numel(model.module.get_decoder().layers[0].self_attn.k_proj.weight))\n",
    "\n",
    "# now, fine tune loop\n",
    "# sparseness is defined as proportion of nonzeros (opposite of intuitive)\n",
    "# sparseness_sequence = [.9, .8, .7, .6, .5, .4, .3, .2]\n",
    "# sparseness_sequence = [.9, .5, .4]\n",
    "\n",
    "# model is model to iteratively tune and prune, will do so in place\n",
    "# model_size is for naming the save files (like opt-125m)\n",
    "# sparseness sequence is sequence of sparsities (.8 sparseness = 20% proportion of zeros)\n",
    "# training_data and tokenizer are for fine_tuning (should already be preprocessed with torch.format and stuff)\n",
    "def iterative_sparsegpt_prune_tune(model, model_size, sparseness_sequence, feature_hessians, EPSILON, B, Bs, tokenizer, EPOCH_COUNT):\n",
    "    # for sparseness_index in range(len(sparseness_sequence)):\n",
    "    for sparseness in sparseness_sequence:\n",
    "        sparsegpt_prune(model=model, model_name=model_size, feature_hessians=feature_hessians, SPARSENESS=sparseness, EPSILON=EPSILON, B=B, Bs=Bs)\n",
    "        #del model\n",
    "        #torch.cuda.empty_cache()\n",
    "        #fine_tune(model_name=model_name, EPOCH_COUNT=EPOCH_COUNT, tokenizer=tokenizer)\n",
    "\n",
    "        # deactivate masks\n",
    "        #unmask_model(model=model)\n",
    "        #torch.cuda.empty_cache()\n",
    "        #print(f\"proportion of zeros: {get_prop_zeros(model)}\")\n",
    "\n",
    "        #pruned_model_name = f'{model_size}-finetuned-{sparseness}'\n",
    "        #torch.save(model.state_dict(), f'pruned_models/{pruned_model_name}.pt')\n",
    "        #del model\n",
    "def iterative_cerebras_prune_tune(model, model_size, sparseness_sequence, training_data, tokenizer, EPOCH_COUNT):\n",
    "    for sparseness in sparseness_sequence:\n",
    "\n",
    "        mask_lowest(model=model, amount=1-sparseness)\n",
    "\n",
    "        # activate masks\n",
    "        mask_from_pruned(model=model)\n",
    "\n",
    "        fine_tune(model=model, training_data=training_data, EPOCH_COUNT=EPOCH_COUNT, tokenizer=tokenizer)\n",
    "\n",
    "        # deactivate masks\n",
    "        unmask_model(model=model)\n",
    "\n",
    "        print(f\"proportion of zeros: {get_prop_zeros(model)}\")\n",
    "\n",
    "        pruned_model_name = f'{model_size}-cerebras-tune-and-prune-{sparseness}'\n",
    "        torch.save(model.state_dict(), f'pruned_models/{pruned_model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PRUNED MODEL\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# function to get module name from parameter name\n",
    "def get_module_name(param_name):\n",
    "    if param_name[-5:] == \".bias\":\n",
    "        return param_name[:-5], \"bias\"\n",
    "    elif param_name[-7:] == \".weight\":\n",
    "        return param_name[:-7], \"weight\"\n",
    "    \n",
    "    elif param_name[-10:] == \".bias_orig\":\n",
    "        return param_name[:-10], \"bias\"\n",
    "    elif param_name[-12:] == \".weight_orig\":\n",
    "        return param_name[:-12], \"weight\"\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# load model without masks\n",
    "def load_unmasked_model(existing_model, state_dict_path):\n",
    "    existing_model.load_state_dict(torch.load(state_dict_path))\n",
    "\n",
    "# prune 0s to a mask, to make training easier (ostensibly)\n",
    "class ZeroPruning(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "    # default threshold is 0, prunes weights that are already 0 (for training)\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute_mask(self, tensor, default_mask):\n",
    "        return torch.abs(tensor) != 0\n",
    "\n",
    "# apply pytorch mask in place of 0 weights to make backpropagation easier for training\n",
    "default_opt_blacklist = ['model.decoder.embed_tokens', 'model.decoder.embed_positions']\n",
    "def mask_from_pruned(model, module_blacklist=default_opt_blacklist):\n",
    "    module_dict = {}\n",
    "    for n, m in model.named_modules():\n",
    "        module_dict[n] = m\n",
    "    # print(module_dict.keys())\n",
    "    \n",
    "    parameter_list = []\n",
    "    param_dict = {}\n",
    "    for n, m in model.named_parameters():\n",
    "        parameter_list.append(n)\n",
    "        param_dict[n] = m\n",
    "    # print(parameter_list)\n",
    "\n",
    "    for n in parameter_list:\n",
    "        module_name, param_type = get_module_name(n)\n",
    "\n",
    "        # skip bias, embed, etc parameters\n",
    "        if module_name in module_blacklist or module_name is None \\\n",
    "            or param_type is None or param_type!=\"weight\":\n",
    "            continue\n",
    "\n",
    "        if len(param_dict[n].shape) < 2:\n",
    "            continue\n",
    "\n",
    "        ZeroPruning.apply(module=module_dict[module_name], name=param_type)\n",
    "# unmask model with 0s in place\n",
    "def unmask_model(model, module_blacklist=default_opt_blacklist):\n",
    "    module_dict = {}\n",
    "    for n, m in model.named_modules():\n",
    "        module_dict[n] = m\n",
    "    # print(module_dict.keys())\n",
    "    \n",
    "    parameter_list = []\n",
    "    param_dict = {}\n",
    "    for n, m in model.named_parameters():\n",
    "        parameter_list.append(n)\n",
    "        param_dict[n] = m\n",
    "    # print(parameter_list)\n",
    "\n",
    "    for n in parameter_list:\n",
    "        module_name, param_type = get_module_name(n)\n",
    "\n",
    "        # skip bias, embed, etc parameters\n",
    "        if module_name in module_blacklist or module_name is None \\\n",
    "            or param_type is None or param_type!=\"weight\":\n",
    "            continue\n",
    "\n",
    "        if len(param_dict[n].shape) < 2:\n",
    "            continue\n",
    "            \n",
    "        prune.remove(module=module_dict[module_name], name=param_type)\n",
    "        torch.cuda.clear_cache()\n",
    "\n",
    "# load model with masks\n",
    "def load_masked_model(existing_model, state_dict_path):\n",
    "\n",
    "    # first load like normal\n",
    "    load_unmasked_model(existing_model, state_dict_path)\n",
    "    \n",
    "    # then reapply the (previously removed) masks\n",
    "    mask_from_pruned(model=existing_model)\n",
    "\n",
    "    # prune.global_unstructured(\n",
    "    #     existing_model.parameters(), pruning_method=ThresholdPruning, threshold=0\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manage imports\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling,AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "def test_model(model_name, encodings, token_length, seq_len, stride, wandb, SPARSITY, is_finetuned=False, device=device):\n",
    "    loaded_model = OPTForCausalLM.from_pretrained(f'facebook/{model_name}',\n",
    "                                                  output_attentions=True,\n",
    "                                                  output_hidden_states=True).to(device=device) # type: ignore\n",
    "    loaded_model = torch.nn.DataParallel(loaded_model, device_ids=[0,1,2,3])\n",
    "    load_unmasked_model(loaded_model, f'pruned_models/{model_name}-{SPARSITY}.pt')\n",
    "    if is_finetuned:\n",
    "        load_unmasked_model(loaded_model, \n",
    "                            f'pruned_models/{model_name}-finetuned-{SPARSITY}.pt')\n",
    "    else:\n",
    "        if SPARSITY != 1:\n",
    "            load_unmasked_model(loaded_model, \n",
    "                            f'pruned_models/{model_name}-{SPARSITY}.pt')\n",
    "    loaded_model.eval()\n",
    "    _ = loaded_model(torch.randint(high=20, size=(1,10)))\n",
    "    \n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + token_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device=device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:,:-trg_len] = -100\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = loaded_model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "            \n",
    "        nlls.append(neg_log_likelihood)\n",
    "        \n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "            \n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "    wandb.log({\"perplexity\": ppl, 'density': SPARSITY})\n",
    "    \n",
    "    del loaded_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def finetune_model(model_name, tokenizer, SPARSITY, device=device, EPOCH_COUNT=10):\n",
    "    #encode tokens\n",
    "    def encode_tok(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "\n",
    "    #stream c4, training split\n",
    "    training_data = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train', streaming=True)\n",
    "    #IMPORTANT: process data while streaming -> remove unnecessary columns in batches\n",
    "    training_data = training_data.map(encode_tok, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    #set data to tensor mode\n",
    "    training_data = training_data.with_format(\"torch\")\n",
    "\n",
    "    #dataloader from dataloader (mlm=False when training without mask)\n",
    "    reformatted_data = DataLoader(training_data, collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False))\n",
    "    \n",
    "    loaded_model = OPTForCausalLM.from_pretrained(f'facebook/{model_name}',\n",
    "                                                  output_attentions=True,\n",
    "                                                  output_hidden_states=True).to(device=device) # type: ignore\n",
    "    loaded_model = torch.nn.DataParallel(loaded_model, device_ids=[0,1,2,3])# activate masks\n",
    "    \n",
    "    if SPARSITY != 1:\n",
    "        load_masked_model(loaded_model, f'pruned_models/{model_name}-{SPARSITY}.pt')\n",
    "    loaded_model.eval()\n",
    "    _ = loaded_model(torch.randint(high=20, size=(1,10)))\n",
    "    mask_from_pruned(model=loaded_model)\n",
    "    #training loop\n",
    "    loaded_model.train().to(device)\n",
    "    t_optim = torch.optim.AdamW(params=loaded_model.parameters(), lr=1e-5)\n",
    "    for epoch in tqdm(range(EPOCH_COUNT)):\n",
    "        training_data.set_epoch(epoch)\n",
    "        for i, batch in enumerate(reformatted_data):\n",
    "            if i == 5:\n",
    "                break\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = loaded_model(**batch)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            t_optim.step()\n",
    "            t_optim.zero_grad()\n",
    "    unmask_model(loaded_model)\n",
    "    torch.save(loaded_model.state_dict(), f'pruned_models/{model_name}-{SPARSITY}-finetuned.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input_prehooks import get_feature_storage_name\n",
    "import gc\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils import prune\n",
    "import calculate_mask\n",
    "import iterative_calculate_mask\n",
    "\n",
    "opt_blacklist = ['module.model.decoder.embed_tokens', 'module.model.decoder.embed_positions']\n",
    "\n",
    "#DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Using calibration data (inputs to each intermediate weight layer)\n",
    "# Iterate through named parameters, calculate inverse hessian and calculate mask\n",
    "\n",
    "def get_module_name(param_name):\n",
    "    if param_name[-5:] == \".bias\":\n",
    "        return param_name[:-5], \"bias\"\n",
    "    elif param_name[-7:] == \".weight\":\n",
    "        return param_name[:-7], \"weight\"\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def sparsegpt_prune(model, model_name, feature_hessians, \n",
    "EPSILON, SPARSENESS, B, Bs, module_blacklist=opt_blacklist, iterative=True):\n",
    "    module_dict = {}\n",
    "    for n, m in model.named_modules():\n",
    "        module_dict[n] = m\n",
    "    # print(module_dict.keys())\n",
    "    \n",
    "    param_names = []\n",
    "    param_dict = {}\n",
    "    for n, m in model.named_parameters():\n",
    "        param_names.append(n)\n",
    "        param_dict[n] = m\n",
    "    # print(parameter_list)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # for name in tqdm(param_names):\n",
    "        for param_name in tqdm(param_names, total=len(param_names)):\n",
    "            module_name, param_type = get_module_name(param_name)\n",
    "\n",
    "            # skip bias, embed, etc parameters\n",
    "            if module_name in module_blacklist or module_name is None \\\n",
    "                or param_type is None or param_type!=\"weight\":\n",
    "                continue\n",
    "\n",
    "            if len(param_dict[param_name].shape) < 2:\n",
    "                continue\n",
    "\n",
    "            param = param_dict[param_name]\n",
    "\n",
    "            #print(f\"Doing layer {name}\")\n",
    "            # get layer input from features, key is get_feature_storage_name(module_name)\n",
    "            # get_feature_storage_name(module_name) stores k_proj, v_proj, q_proj together\n",
    "            # since they are the same input\n",
    "            \n",
    "            layer_hessian = feature_hessians[get_feature_storage_name(module_name)].to(device=device)\n",
    "\n",
    "            # calculate inverse hessian\n",
    "            # check if input is flattened e.g. from 8,512,768 to 4096,768\n",
    "            inv_hess = calc_inverse_hessian(layer_hessian, epsilon=EPSILON)\n",
    "\n",
    "            # calculate mask\n",
    "            if iterative:\n",
    "                mask = iterative_calculate_mask.calculate_mask(W=param, H_inv=inv_hess, p=SPARSENESS, B=B, Bs=Bs)\n",
    "            else:\n",
    "                mask = calculate_mask.calculate_mask(W=param, H_inv=inv_hess, p=SPARSENESS, B=B, Bs=Bs)\n",
    "\n",
    "            # get module from lookup dictionary by module name\n",
    "            module = module_dict[module_name]\n",
    "            # apply mask\n",
    "            prune.custom_from_mask(module=module, name=param_type, mask=mask)\n",
    "            prune.remove(module=module, name=param_type)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()  \n",
    "\n",
    "    pruned_model_name = f'{model_name}-{SPARSENESS}'\n",
    "\n",
    "    torch.save(model.state_dict(), f'pruned_models/{pruned_model_name}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsify and Finetune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_size=128\n",
    "token_length=1024\n",
    "calibration_batch_size=1\n",
    "\n",
    "EPSILON = 1e-8\n",
    "B = 128 \n",
    "Bs = 128\n",
    "\n",
    "#hyperparam test, remove later\n",
    "EPOCH_COUNT = 10\n",
    "\n",
    "#set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "model_name = \"opt-1.3b\"\n",
    "#load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'facebook/{model_name}')\n",
    "#Load dataset\n",
    "dataset = load_dataset('c4', 'en', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[AAsking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  0%|          | 0/10 [00:01<?, ?it/s]\n",
      "  0%|          | 0/6 [00:27<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 39.41 GiB total capacity; 38.02 GiB already allocated; 32.50 MiB free; 38.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, SPARSITY \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(SPARSITIES, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(SPARSITIES))):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Load model with pre-trained head\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124;03m'''model = OPTForCausalLM.from_pretrained(f'facebook/{model_name}', output_attentions=True,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m                                           output_hidden_states=True).to(device=device) # type: ignore\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    model = torch.nn.DataParallel(model, device_ids=[0,1,2,3])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m                                   tokenizer=tokenizer,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m                                   EPOCH_COUNT=EPOCH_COUNT)'''\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mfinetune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSPARSITY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSPARSITY\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mfinetune_model\u001b[0;34m(model_name, tokenizer, SPARSITY, device, EPOCH_COUNT)\u001b[0m\n\u001b[1;32m     90\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     91\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 92\u001b[0m         \u001b[43mt_optim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m         t_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     94\u001b[0m unmask_model(loaded_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/optim/adamw.py:149\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    147\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 39.41 GiB total capacity; 38.02 GiB already allocated; 32.50 MiB free; 38.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#from testing_module import finetune_model\n",
    "#from trainingv2 import fine_tune\n",
    "SPARSITIES = [0.2,0.3,0.5,0.7,0.9,1]#0.1, 0.2,0.3,0.5,0.7,0.9,1\n",
    "#encode tokens\n",
    "\n",
    "    \n",
    "for i, SPARSITY in enumerate(tqdm(SPARSITIES, total=len(SPARSITIES))):\n",
    "    # Load model with pre-trained head\n",
    "    '''model = OPTForCausalLM.from_pretrained(f'facebook/{model_name}', output_attentions=True,\n",
    "                                           output_hidden_states=True).to(device=device) # type: ignore\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "    !nvidia-smi\n",
    "    \n",
    "    if i == 0:\n",
    "        feature_hessians = {}\n",
    "        #put_input_hooks(model=model, features=feature_hessians, storage_dir=storage_dir, offload_freq=10000, feature_storage_device='cpu')\n",
    "        put_input_hooks(model=model, features=feature_hessians, feature_storage_device='cpu')\n",
    "        split_model_calibration(model)\n",
    "    torch.cuda.empty_cache()\n",
    "    iterative_sparsegpt_prune_tune(model=model, model_size=model_name,\n",
    "                                   sparseness_sequence=[SPARSITY],\n",
    "                                   feature_hessians=feature_hessians,\n",
    "                                   EPSILON=EPSILON, B=B, Bs=Bs,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   EPOCH_COUNT=EPOCH_COUNT)'''\n",
    "    finetune_model(model_name=model_name, tokenizer=tokenizer, SPARSITY=SPARSITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maaquib111\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gs/gsfs0/home/asyed/pw/jobs/ICLR/wandb/run-20230224_184212-1zirk0a1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aaquib111/ICLR/runs/1zirk0a1\" target=\"_blank\">opt-350m Wikitext Test</a></strong> to <a href=\"https://wandb.ai/aaquib111/ICLR\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/gs/gsfs0/users/asyed/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "  0%|          | 0/562 [00:00<?, ?it/s]/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|█████████▉| 560/562 [00:48<00:00, 11.55it/s]\n",
      "100%|█████████▉| 560/562 [00:48<00:00, 11.53it/s]\n",
      "100%|█████████▉| 560/562 [00:48<00:00, 11.46it/s]\n",
      "100%|█████████▉| 560/562 [00:49<00:00, 11.40it/s]\n",
      "100%|█████████▉| 560/562 [00:49<00:00, 11.38it/s]\n",
      "100%|█████████▉| 560/562 [00:48<00:00, 11.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1zirk0a1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6baf2d817ae48879adc282fa7ebb242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>density</td><td>▁▂▄▅▇█</td></tr><tr><td>perplexity</td><td>▃█▅▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>density</td><td>1</td></tr><tr><td>perplexity</td><td>20.87292</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">opt-350m Wikitext Test</strong>: <a href=\"https://wandb.ai/aaquib111/ICLR/runs/1zirk0a1\" target=\"_blank\">https://wandb.ai/aaquib111/ICLR/runs/1zirk0a1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230224_184212-1zirk0a1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1zirk0a1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gs/gsfs0/home/asyed/pw/jobs/ICLR/wandb/run-20230224_184752-dmp7zrwy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aaquib111/ICLR/runs/dmp7zrwy\" target=\"_blank\">opt-350m Wikitext Test</a></strong> to <a href=\"https://wandb.ai/aaquib111/ICLR\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/562 [00:00<?, ?it/s]/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|█████████▉| 560/562 [00:48<00:00, 11.52it/s]\n",
      "100%|█████████▉| 560/562 [00:48<00:00, 11.51it/s]\n",
      "100%|█████████▉| 560/562 [00:48<00:00, 11.44it/s]\n",
      "100%|█████████▉| 560/562 [00:49<00:00, 11.42it/s]\n",
      "100%|█████████▉| 560/562 [00:49<00:00, 11.42it/s]\n",
      "100%|█████████▉| 560/562 [00:49<00:00, 11.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from testing_module import test_model\n",
    "\n",
    "model_name = \"opt-350m\"\n",
    "token_length=1024\n",
    "stride = 512\n",
    "wandb.init(project=\"ICLR\", \n",
    "           name = f'{model_name} Wikitext Test', \n",
    "           config={'token_length': token_length,\n",
    "                 'model_name': model_name,\n",
    "                 'stride': stride,\n",
    "                 'fine_tuned': 'not finetuned'})\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'facebook/{model_name}', \n",
    "                                          padding_side='left', \n",
    "                                          use_fast=False)\n",
    "# Load dataset\n",
    "test_set = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "encodings = tokenizer(\"\\n\\n\".join(test_set['text']), return_tensors='pt')\n",
    "\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "SPARSITIES = [0.2, 0.3, 0.5, 0.7, 0.9, 1]#, 0.4, 0.6, 0.8, 1\n",
    "\n",
    "for SPARSITY in SPARSITIES:\n",
    "    test_model(model_name, encodings, token_length, seq_len, stride, wandb, SPARSITY, is_finetuned=False)\n",
    "    \n",
    "### NOW DO FINETUNED\n",
    "wandb.init(project=\"ICLR\", \n",
    "           name = f'{model_name} Wikitext Test', \n",
    "           config={'token_length': token_length,\n",
    "                 'model_name': model_name,\n",
    "                 'stride': stride,\n",
    "                 'fine_tuned': 'finetuned'})\n",
    "for SPARSITY in SPARSITIES:\n",
    "    test_model(model_name, encodings, token_length, seq_len, stride, wandb, SPARSITY, is_finetuned=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteratively Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SparseGPT_pruning import sparsegpt_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsegpt_prune()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e86ec8e8174c6e268751d4c1c60ae4faf419c25d731f3f2c6b91607c46f997f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# from calculate_mask import calculate_mask\n",
    "# from inverse_hessian import inverse_hessian\n",
    "from input_prehooks import put_input_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000e+00, 4.8290e+04, 7.1300e+03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        [2.0000e+00, 4.8763e+04, 1.1000e+01,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        [2.0000e+00, 5.9700e+02, 1.4189e+04,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        ...,\n",
       "        [2.0000e+00, 3.8700e+02, 9.2980e+03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        [2.0000e+00, 1.0000e+02, 8.0200e+02,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        [2.0000e+00, 1.3300e+02, 4.0660e+03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "#Load dataset\n",
    "dataset = load_dataset('c4', 'en', streaming=True)\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "# Load model with pre-trained head\n",
    "model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True)\n",
    "# Load genrator\n",
    "generator = pipeline('text-generation', model=\"facebook/opt-125m\")\n",
    "# Create calibration data\n",
    "calibration_data = []\n",
    "for i, data in enumerate(iter(dataset['train'])):\n",
    "    if i > 7:\n",
    "        break\n",
    "    tokenized = tokenizer.encode(data['text'], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "    calibration_data.append(tokenized)\n",
    "#calibration_data = torch.transpose(torch.squeeze(torch.stack(calibration_data)),0,1).to(device=device)\n",
    "calibration_data = torch.squeeze(torch.stack(calibration_data)).to(device=device)\n",
    "calibration_data.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, put in forward hooks\n",
    "features = {}\n",
    "put_input_hooks(model=model, features=features)\n",
    "\n",
    "# Run calibration data through model at first to calculate features dictionary with\n",
    "# input tensors to each intermediate layer\n",
    "model(calibration_data)\n",
    "\n",
    "# function to get module name from parameter name\n",
    "def get_module_name(param_name):\n",
    "    if param_name[-5:] == \".bias\":\n",
    "        return param_name[:-5], \"bias\"\n",
    "    elif param_name[-7:] == \".weight\":\n",
    "        return param_name[:-7], \"weight\"\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input shape: torch.Size([8, 512])\n",
      "model.decoder\n",
      "model.decoder.embed_tokens\n",
      "input shape: torch.Size([8, 512])\n",
      "weight shape: torch.Size([50272, 768])\n",
      "model.decoder.embed_positions\n",
      "input shape: torch.Size([8, 512])\n",
      "weight shape: torch.Size([2050, 768])\n",
      "model.decoder.layers.0\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "model.decoder.layers.0.self_attn_layer_norm\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.0.self_attn\n",
      "model.decoder.layers.0.self_attn.q_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.0.self_attn.k_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.0.self_attn.v_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.0.self_attn.out_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.0.final_layer_norm\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.0.fc1\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([3072, 768])\n",
      "model.decoder.layers.0.activation_fn\n",
      "input shape: torch.Size([4096, 3072])\n",
      "model.decoder.layers.0.fc2\n",
      "input shape: torch.Size([4096, 3072])\n",
      "weight shape: torch.Size([768, 3072])\n",
      "model.decoder.layers.1\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "model.decoder.layers.1.self_attn_layer_norm\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.1.self_attn\n",
      "model.decoder.layers.1.self_attn.q_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.1.self_attn.k_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.1.self_attn.v_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.1.self_attn.out_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.1.final_layer_norm\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.1.fc1\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([3072, 768])\n",
      "model.decoder.layers.1.activation_fn\n",
      "input shape: torch.Size([4096, 3072])\n",
      "model.decoder.layers.1.fc2\n",
      "input shape: torch.Size([4096, 3072])\n",
      "weight shape: torch.Size([768, 3072])\n",
      "model.decoder.layers.2\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "model.decoder.layers.2.self_attn_layer_norm\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.2.self_attn\n",
      "model.decoder.layers.2.self_attn.q_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.2.self_attn.k_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.2.self_attn.v_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.2.self_attn.out_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.2.final_layer_norm\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.2.fc1\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([3072, 768])\n",
      "model.decoder.layers.2.activation_fn\n",
      "input shape: torch.Size([4096, 3072])\n",
      "model.decoder.layers.2.fc2\n",
      "input shape: torch.Size([4096, 3072])\n",
      "weight shape: torch.Size([768, 3072])\n",
      "model.decoder.layers.3\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "model.decoder.layers.3.self_attn_layer_norm\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.3.self_attn\n",
      "model.decoder.layers.3.self_attn.q_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.3.self_attn.k_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.3.self_attn.v_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.3.self_attn.out_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.3.final_layer_norm\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.3.fc1\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([3072, 768])\n",
      "model.decoder.layers.3.activation_fn\n",
      "input shape: torch.Size([4096, 3072])\n",
      "model.decoder.layers.3.fc2\n",
      "input shape: torch.Size([4096, 3072])\n",
      "weight shape: torch.Size([768, 3072])\n",
      "model.decoder.layers.4\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "model.decoder.layers.4.self_attn_layer_norm\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.4.self_attn\n",
      "model.decoder.layers.4.self_attn.q_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.4.self_attn.k_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.4.self_attn.v_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.4.self_attn.out_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.4.final_layer_norm\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.4.fc1\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([3072, 768])\n",
      "model.decoder.layers.4.activation_fn\n",
      "input shape: torch.Size([4096, 3072])\n",
      "model.decoder.layers.4.fc2\n",
      "input shape: torch.Size([4096, 3072])\n",
      "weight shape: torch.Size([768, 3072])\n",
      "model.decoder.layers.5\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "model.decoder.layers.5.self_attn_layer_norm\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.5.self_attn\n",
      "model.decoder.layers.5.self_attn.q_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.5.self_attn.k_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.5.self_attn.v_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.5.self_attn.out_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.5.final_layer_norm\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.5.fc1\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([3072, 768])\n",
      "model.decoder.layers.5.activation_fn\n",
      "input shape: torch.Size([4096, 3072])\n",
      "model.decoder.layers.5.fc2\n",
      "input shape: torch.Size([4096, 3072])\n",
      "weight shape: torch.Size([768, 3072])\n",
      "model.decoder.layers.6\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "model.decoder.layers.6.self_attn_layer_norm\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.6.self_attn\n",
      "model.decoder.layers.6.self_attn.q_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.6.self_attn.k_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.6.self_attn.v_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.6.self_attn.out_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.6.final_layer_norm\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.6.fc1\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([3072, 768])\n",
      "model.decoder.layers.6.activation_fn\n",
      "input shape: torch.Size([4096, 3072])\n",
      "model.decoder.layers.6.fc2\n",
      "input shape: torch.Size([4096, 3072])\n",
      "weight shape: torch.Size([768, 3072])\n",
      "model.decoder.layers.7\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "model.decoder.layers.7.self_attn_layer_norm\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.7.self_attn\n",
      "model.decoder.layers.7.self_attn.q_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.7.self_attn.k_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.7.self_attn.v_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.7.self_attn.out_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.7.final_layer_norm\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.7.fc1\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([3072, 768])\n",
      "model.decoder.layers.7.activation_fn\n",
      "input shape: torch.Size([4096, 3072])\n",
      "model.decoder.layers.7.fc2\n",
      "input shape: torch.Size([4096, 3072])\n",
      "weight shape: torch.Size([768, 3072])\n",
      "model.decoder.layers.8\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "model.decoder.layers.8.self_attn_layer_norm\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.8.self_attn\n",
      "model.decoder.layers.8.self_attn.q_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.8.self_attn.k_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.8.self_attn.v_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.8.self_attn.out_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.8.final_layer_norm\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.8.fc1\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([3072, 768])\n",
      "model.decoder.layers.8.activation_fn\n",
      "input shape: torch.Size([4096, 3072])\n",
      "model.decoder.layers.8.fc2\n",
      "input shape: torch.Size([4096, 3072])\n",
      "weight shape: torch.Size([768, 3072])\n",
      "model.decoder.layers.9\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "model.decoder.layers.9.self_attn_layer_norm\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.9.self_attn\n",
      "model.decoder.layers.9.self_attn.q_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.9.self_attn.k_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.9.self_attn.v_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.9.self_attn.out_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.9.final_layer_norm\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.9.fc1\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([3072, 768])\n",
      "model.decoder.layers.9.activation_fn\n",
      "input shape: torch.Size([4096, 3072])\n",
      "model.decoder.layers.9.fc2\n",
      "input shape: torch.Size([4096, 3072])\n",
      "weight shape: torch.Size([768, 3072])\n",
      "model.decoder.layers.10\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "model.decoder.layers.10.self_attn_layer_norm\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.10.self_attn\n",
      "model.decoder.layers.10.self_attn.q_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.10.self_attn.k_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.10.self_attn.v_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.10.self_attn.out_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.10.final_layer_norm\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.10.fc1\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([3072, 768])\n",
      "model.decoder.layers.10.activation_fn\n",
      "input shape: torch.Size([4096, 3072])\n",
      "model.decoder.layers.10.fc2\n",
      "input shape: torch.Size([4096, 3072])\n",
      "weight shape: torch.Size([768, 3072])\n",
      "model.decoder.layers.11\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "model.decoder.layers.11.self_attn_layer_norm\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.11.self_attn\n",
      "model.decoder.layers.11.self_attn.q_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.11.self_attn.k_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.11.self_attn.v_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.11.self_attn.out_proj\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768, 768])\n",
      "model.decoder.layers.11.final_layer_norm\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([768])\n",
      "model.decoder.layers.11.fc1\n",
      "input shape: torch.Size([4096, 768])\n",
      "weight shape: torch.Size([3072, 768])\n",
      "model.decoder.layers.11.activation_fn\n",
      "input shape: torch.Size([4096, 3072])\n",
      "model.decoder.layers.11.fc2\n",
      "input shape: torch.Size([4096, 3072])\n",
      "weight shape: torch.Size([768, 3072])\n",
      "model.decoder.final_layer_norm\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([768])\n",
      "lm_head\n",
      "input shape: torch.Size([8, 512, 768])\n",
      "weight shape: torch.Size([50272, 768])\n"
     ]
    }
   ],
   "source": [
    "# make a dictionary to access module by name\n",
    "# model_param_lookup_dict = {}\n",
    "# for param_name, param_iter in model.named_parameters():\n",
    "#     model_param_lookup_dict[param_name] = param_iter\n",
    "    \n",
    "model_lookup_dict = {}\n",
    "for module_name, module_iter in model.named_modules():\n",
    "    model_lookup_dict[module_name] = module_iter\n",
    "\n",
    "for k in features.keys():\n",
    "    # print(k)\n",
    "    try:\n",
    "        # print(f\"for {k}\")\n",
    "        # print(f\"{k} shape: {features[k][0].shape}\")\n",
    "        # print(f\"weight shape: {model_lookup_dict[k].weight.shape}\")\n",
    "        # if 512 in model_lookup_dict[k].weight.shape:\n",
    "        print(k)\n",
    "        print(f\"input shape: {features[k][0].shape}\")\n",
    "        print(f\"weight shape: {model_lookup_dict[k].weight.shape}\")\n",
    "    except:\n",
    "        continue\n",
    "    # except:\n",
    "        # continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.decoder.layers.0.self_attn.k_proj.weight\n",
      "layer input shape: torch.Size([8, 512, 768])\n",
      "input shape: torch.Size([8, 768, 512])\n",
      "shape of x @ x_t: torch.Size([768, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (768) must match the size of tensor b (8) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 148\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlayer input shape: \u001b[39m\u001b[39m{\u001b[39;00mlayer_input\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39m# print(f\"weight shape: {param.shape}\")\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[39m# calculate inverse hessian\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m inv_hess \u001b[39m=\u001b[39m inverse_hessian(torch\u001b[39m.\u001b[39;49mtranspose(layer_input, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m), epsilon\u001b[39m=\u001b[39;49mEPSILON)\n\u001b[1;32m    149\u001b[0m \u001b[39m# inv_hess = inverse_hessian(layer_input, epsilon=EPSILON)\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39m# print(f\"hessian shape: {inv_hess.shape}\")\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \n\u001b[1;32m    152\u001b[0m \u001b[39m# calculate mask\u001b[39;00m\n\u001b[1;32m    153\u001b[0m mask \u001b[39m=\u001b[39m calculate_mask(W\u001b[39m=\u001b[39mparam, H_inv\u001b[39m=\u001b[39minv_hess, p\u001b[39m=\u001b[39mSPARSENESS, B\u001b[39m=\u001b[39mB, Bs\u001b[39m=\u001b[39mBs)\n",
      "Cell \u001b[0;32mIn[132], line 93\u001b[0m, in \u001b[0;36minverse_hessian\u001b[0;34m(X, epsilon)\u001b[0m\n\u001b[1;32m     91\u001b[0m identity \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meye(X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m     92\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape of x @ x_t: \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39msum(X \u001b[39m@\u001b[39m X_T, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m H \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (torch\u001b[39m.\u001b[39;49msum(X \u001b[39m@\u001b[39;49m X_T, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m) \u001b[39m+\u001b[39;49m (epsilon \u001b[39m*\u001b[39;49m identity))\n\u001b[1;32m     94\u001b[0m \u001b[39m# print(torch.linalg.eig(H)[0])\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mH SHAPE: \u001b[39m\u001b[39m{\u001b[39;00mH\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (768) must match the size of tensor b (8) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "def calculate_mask(\n",
    "    W,\n",
    "    H_inv,\n",
    "    p,\n",
    "    B,\n",
    "    Bs,\n",
    "    ):\n",
    "\n",
    "    # Get the number of rows and columns in W\n",
    "\n",
    "    (d_row, d_col) = W.shape\n",
    "\n",
    "    # Initialize the pruning mask M and block quantization errors E to all zeros\n",
    "\n",
    "    M = torch.zeros(d_row, d_col, dtype=torch.bool)\n",
    "    E = torch.zeros(d_row, B, dtype=torch.float64)\n",
    "\n",
    "    # only need to calculate w_square and h_square once\n",
    "    # Loop over blocks of columns of W (as specified by B)\n",
    "\n",
    "    for i in range(0, d_col, B):\n",
    "\n",
    "        # Loop over columns within a block\n",
    "\n",
    "        for j in range(i, min(i + B, d_col)):\n",
    "\n",
    "            # If j is a multiple of Bs, prune a portion of the weights\n",
    "\n",
    "            if j % Bs == 0:\n",
    "\n",
    "                # Get the mask for the largest (1 - p)% of weights based on squared value and inverse hessian\n",
    "\n",
    "                # ASTERISK: prune_values is matrix of w^2/H^(-1)_cc\n",
    "\n",
    "                # Finding respective sections of hessian and weights matrix\n",
    "                w_square_section = torch.square(W[:, j:j + Bs])\n",
    "                h_square_section = torch.square(H_inv[j:j + Bs, j:j\n",
    "                        + Bs]).diag()  # 1 dimensional vector\n",
    "\n",
    "                # getting the prune values matrix from W and H^-1 sections\n",
    "                prune_values = w_square_section \\\n",
    "                    / h_square_section.unsqueeze(0)\n",
    "\n",
    "                #calulating cutoff for the weights\n",
    "                cutoff_value = torch.kthvalue(prune_values, int((1 - p)\n",
    "                        * d_row), dim=0)[0]\n",
    "\n",
    "                #getting the final mask\n",
    "                mask = prune_values > cutoff_value\n",
    "\n",
    "                #masking\n",
    "                M[:, j:j + Bs] = mask\n",
    "\n",
    "            # Calculate the pruning error for this column\n",
    "\n",
    "            E[:, j - i] = W[:, j] / H_inv[j, j]\n",
    "\n",
    "            # Freeze the weights that are not pruned by multiplying by the pruning mask\n",
    "            # Invert mask (~M equivalent to 1 - M < might be -(M + 1))\n",
    "\n",
    "            E[:, j - i] = ~M[:, j] * E[:, j - i]\n",
    "\n",
    "            # Update the weights in this block based on the pruning error and inverse hessian information\n",
    "\n",
    "            W[:, j:i + B] -= torch.ger(E[:, j - i], H_inv[j, j:i + B])\n",
    "\n",
    "        # Update all remaining weights\n",
    "\n",
    "        # print(f\"this weight shape: {W[:, i + B:].shape}\")\n",
    "        # print(f\"e shape: {E.shape}\")\n",
    "        # print(f\"Hessian shape: {H_inv[i:i + B, i + B:].shape}\")\n",
    "        W[:, i + B:] -= torch.matmul(E, H_inv[i:i + B, i + B:])\n",
    "\n",
    "    # return mask\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "def inverse_hessian(X, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Calculate the inverse of a positive-definite matrix using the Cholesky decomposition.\n",
    "    Args:\n",
    "    - X (torch.Tensor): dxn tensor\n",
    "    - epsilon (float): small constant to prevent Hessian from being singular\n",
    "    Returns:\n",
    "    - torch.Tensor: inverted matrix\n",
    "    \"\"\"\n",
    "    X = X.double()\n",
    "    print(f\"input shape: {X.shape}\")\n",
    "    X_T = torch.transpose(X, 1, 2)\n",
    "    identity = torch.eye(X.shape[1], dtype=torch.float64)\n",
    "    # print(f\"shape of x @ x_t: {torch.sum(X @ X_T, dim=0).shape}\")\n",
    "    H = 2 * (torch.sum(X @ X_T, dim=0) + (epsilon * identity))\n",
    "    # print(torch.linalg.eig(H)[0])\n",
    "    print(f\"H SHAPE: {H.shape}\")\n",
    "    # print(f\"num zeros in hessian: {torch.sum(H == 0)}\")\n",
    "    # print(f\"Determinant is {torch.linalg.det(H)}\")\n",
    "    # print(f\"Hessian Diagonal is {H.diag()}\")\n",
    "    H_inv = torch.inverse(H)\n",
    "    \n",
    "    # H_inv = torch.cholesky(H_inv).T\n",
    "    H_inv = torch.lu(H_inv)[0].T\n",
    "    \n",
    "    return H_inv\n",
    "\n",
    "\n",
    "# Re-load model with pre-trained head\n",
    "model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "# make a dictionary to access module by name\n",
    "model_lookup_dict = {}\n",
    "for module_name, module_iter in model.named_modules():\n",
    "    model_lookup_dict[module_name] = module_iter\n",
    "\n",
    "EPSILON = 1e-8\n",
    "SPARSENESS = .2\n",
    "B = 32\n",
    "Bs = 16\n",
    "\n",
    "layer_blacklist = ['model.decoder.embed_tokens.weight', 'model.decoder.embed_tokens.bias',\n",
    "'model.decoder.embed_positions.weight', 'model.decoder.final_layer_norm.weight',\n",
    "'model.decoder.final_layer_norm.bias']\n",
    "\n",
    "# Using calibration data (inputs to each intermediate weight layer)\n",
    "# Iterate through named parameters, calculate inverse hessian and calculate mask\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "\n",
    "        # skip the embed layer\n",
    "        if name in layer_blacklist:\n",
    "            continue\n",
    "        \n",
    "        # skip norms which have 1 dimension\n",
    "        if len(param.shape) < 2:\n",
    "            continue\n",
    "\n",
    "        module_name, param_type = get_module_name(name)\n",
    "\n",
    "        # apply to weight and bias layers\n",
    "        if param_type == \"weight\" or param_type == \"bias\":\n",
    "            # input to parameter\n",
    "            layer_input = features[module_name][0]\n",
    "            print(name)\n",
    "            print(f\"layer input shape: {layer_input.shape}\")\n",
    "            # print(f\"weight shape: {param.shape}\")\n",
    "            \n",
    "            # calculate inverse hessian\n",
    "            inv_hess = inverse_hessian(torch.transpose(layer_input, 1, 2), epsilon=EPSILON)\n",
    "            # inv_hess = inverse_hessian(layer_input, epsilon=EPSILON)\n",
    "            # print(f\"hessian shape: {inv_hess.shape}\")\n",
    "\n",
    "            # calculate mask\n",
    "            mask = calculate_mask(W=param, H_inv=inv_hess, p=SPARSENESS, B=B, Bs=Bs)\n",
    "            \n",
    "            # get module from lookup dictionary by module name\n",
    "            module = model_lookup_dict[module_name]\n",
    "            # apply mask\n",
    "            prune.custom_from_mask(module=module, name=param_type, mask=mask)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.decoder.embed_tokens.weight\n",
      "model.decoder.embed_positions.weight_orig\n",
      "model.decoder.final_layer_norm.weight\n",
      "model.decoder.final_layer_norm.bias\n",
      "model.decoder.layers.0.self_attn.k_proj.weight\n",
      "model.decoder.layers.0.self_attn.k_proj.bias\n",
      "model.decoder.layers.0.self_attn.v_proj.weight\n",
      "model.decoder.layers.0.self_attn.v_proj.bias\n",
      "model.decoder.layers.0.self_attn.q_proj.weight\n",
      "model.decoder.layers.0.self_attn.q_proj.bias\n",
      "model.decoder.layers.0.self_attn.out_proj.weight\n",
      "model.decoder.layers.0.self_attn.out_proj.bias\n",
      "model.decoder.layers.0.self_attn_layer_norm.weight\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias\n",
      "model.decoder.layers.0.fc1.weight\n",
      "model.decoder.layers.0.fc1.bias\n",
      "model.decoder.layers.0.fc2.weight\n",
      "model.decoder.layers.0.fc2.bias\n",
      "model.decoder.layers.0.final_layer_norm.weight\n",
      "model.decoder.layers.0.final_layer_norm.bias\n",
      "model.decoder.layers.1.self_attn.k_proj.weight\n",
      "model.decoder.layers.1.self_attn.k_proj.bias\n",
      "model.decoder.layers.1.self_attn.v_proj.weight\n",
      "model.decoder.layers.1.self_attn.v_proj.bias\n",
      "model.decoder.layers.1.self_attn.q_proj.weight\n",
      "model.decoder.layers.1.self_attn.q_proj.bias\n",
      "model.decoder.layers.1.self_attn.out_proj.weight\n",
      "model.decoder.layers.1.self_attn.out_proj.bias\n",
      "model.decoder.layers.1.self_attn_layer_norm.weight\n",
      "model.decoder.layers.1.self_attn_layer_norm.bias\n",
      "model.decoder.layers.1.fc1.weight\n",
      "model.decoder.layers.1.fc1.bias\n",
      "model.decoder.layers.1.fc2.weight\n",
      "model.decoder.layers.1.fc2.bias\n",
      "model.decoder.layers.1.final_layer_norm.weight\n",
      "model.decoder.layers.1.final_layer_norm.bias\n",
      "model.decoder.layers.2.self_attn.k_proj.weight\n",
      "model.decoder.layers.2.self_attn.k_proj.bias\n",
      "model.decoder.layers.2.self_attn.v_proj.weight\n",
      "model.decoder.layers.2.self_attn.v_proj.bias\n",
      "model.decoder.layers.2.self_attn.q_proj.weight\n",
      "model.decoder.layers.2.self_attn.q_proj.bias\n",
      "model.decoder.layers.2.self_attn.out_proj.weight\n",
      "model.decoder.layers.2.self_attn.out_proj.bias\n",
      "model.decoder.layers.2.self_attn_layer_norm.weight\n",
      "model.decoder.layers.2.self_attn_layer_norm.bias\n",
      "model.decoder.layers.2.fc1.weight\n",
      "model.decoder.layers.2.fc1.bias\n",
      "model.decoder.layers.2.fc2.weight\n",
      "model.decoder.layers.2.fc2.bias\n",
      "model.decoder.layers.2.final_layer_norm.weight\n",
      "model.decoder.layers.2.final_layer_norm.bias\n",
      "model.decoder.layers.3.self_attn.k_proj.weight\n",
      "model.decoder.layers.3.self_attn.k_proj.bias\n",
      "model.decoder.layers.3.self_attn.v_proj.weight\n",
      "model.decoder.layers.3.self_attn.v_proj.bias\n",
      "model.decoder.layers.3.self_attn.q_proj.weight\n",
      "model.decoder.layers.3.self_attn.q_proj.bias\n",
      "model.decoder.layers.3.self_attn.out_proj.weight\n",
      "model.decoder.layers.3.self_attn.out_proj.bias\n",
      "model.decoder.layers.3.self_attn_layer_norm.weight\n",
      "model.decoder.layers.3.self_attn_layer_norm.bias\n",
      "model.decoder.layers.3.fc1.weight\n",
      "model.decoder.layers.3.fc1.bias\n",
      "model.decoder.layers.3.fc2.weight\n",
      "model.decoder.layers.3.fc2.bias\n",
      "model.decoder.layers.3.final_layer_norm.weight\n",
      "model.decoder.layers.3.final_layer_norm.bias\n",
      "model.decoder.layers.4.self_attn.k_proj.weight\n",
      "model.decoder.layers.4.self_attn.k_proj.bias\n",
      "model.decoder.layers.4.self_attn.v_proj.weight\n",
      "model.decoder.layers.4.self_attn.v_proj.bias\n",
      "model.decoder.layers.4.self_attn.q_proj.weight\n",
      "model.decoder.layers.4.self_attn.q_proj.bias\n",
      "model.decoder.layers.4.self_attn.out_proj.weight\n",
      "model.decoder.layers.4.self_attn.out_proj.bias\n",
      "model.decoder.layers.4.self_attn_layer_norm.weight\n",
      "model.decoder.layers.4.self_attn_layer_norm.bias\n",
      "model.decoder.layers.4.fc1.weight\n",
      "model.decoder.layers.4.fc1.bias\n",
      "model.decoder.layers.4.fc2.weight\n",
      "model.decoder.layers.4.fc2.bias\n",
      "model.decoder.layers.4.final_layer_norm.weight\n",
      "model.decoder.layers.4.final_layer_norm.bias\n",
      "model.decoder.layers.5.self_attn.k_proj.weight\n",
      "model.decoder.layers.5.self_attn.k_proj.bias\n",
      "model.decoder.layers.5.self_attn.v_proj.weight\n",
      "model.decoder.layers.5.self_attn.v_proj.bias\n",
      "model.decoder.layers.5.self_attn.q_proj.weight\n",
      "model.decoder.layers.5.self_attn.q_proj.bias\n",
      "model.decoder.layers.5.self_attn.out_proj.weight\n",
      "model.decoder.layers.5.self_attn.out_proj.bias\n",
      "model.decoder.layers.5.self_attn_layer_norm.weight\n",
      "model.decoder.layers.5.self_attn_layer_norm.bias\n",
      "model.decoder.layers.5.fc1.weight\n",
      "model.decoder.layers.5.fc1.bias\n",
      "model.decoder.layers.5.fc2.weight\n",
      "model.decoder.layers.5.fc2.bias\n",
      "model.decoder.layers.5.final_layer_norm.weight\n",
      "model.decoder.layers.5.final_layer_norm.bias\n",
      "model.decoder.layers.6.self_attn.k_proj.weight\n",
      "model.decoder.layers.6.self_attn.k_proj.bias\n",
      "model.decoder.layers.6.self_attn.v_proj.weight\n",
      "model.decoder.layers.6.self_attn.v_proj.bias\n",
      "model.decoder.layers.6.self_attn.q_proj.weight\n",
      "model.decoder.layers.6.self_attn.q_proj.bias\n",
      "model.decoder.layers.6.self_attn.out_proj.weight\n",
      "model.decoder.layers.6.self_attn.out_proj.bias\n",
      "model.decoder.layers.6.self_attn_layer_norm.weight\n",
      "model.decoder.layers.6.self_attn_layer_norm.bias\n",
      "model.decoder.layers.6.fc1.weight\n",
      "model.decoder.layers.6.fc1.bias\n",
      "model.decoder.layers.6.fc2.weight\n",
      "model.decoder.layers.6.fc2.bias\n",
      "model.decoder.layers.6.final_layer_norm.weight\n",
      "model.decoder.layers.6.final_layer_norm.bias\n",
      "model.decoder.layers.7.self_attn.k_proj.weight\n",
      "model.decoder.layers.7.self_attn.k_proj.bias\n",
      "model.decoder.layers.7.self_attn.v_proj.weight\n",
      "model.decoder.layers.7.self_attn.v_proj.bias\n",
      "model.decoder.layers.7.self_attn.q_proj.weight\n",
      "model.decoder.layers.7.self_attn.q_proj.bias\n",
      "model.decoder.layers.7.self_attn.out_proj.weight\n",
      "model.decoder.layers.7.self_attn.out_proj.bias\n",
      "model.decoder.layers.7.self_attn_layer_norm.weight\n",
      "model.decoder.layers.7.self_attn_layer_norm.bias\n",
      "model.decoder.layers.7.fc1.weight\n",
      "model.decoder.layers.7.fc1.bias\n",
      "model.decoder.layers.7.fc2.weight\n",
      "model.decoder.layers.7.fc2.bias\n",
      "model.decoder.layers.7.final_layer_norm.weight\n",
      "model.decoder.layers.7.final_layer_norm.bias\n",
      "model.decoder.layers.8.self_attn.k_proj.weight\n",
      "model.decoder.layers.8.self_attn.k_proj.bias\n",
      "model.decoder.layers.8.self_attn.v_proj.weight\n",
      "model.decoder.layers.8.self_attn.v_proj.bias\n",
      "model.decoder.layers.8.self_attn.q_proj.weight\n",
      "model.decoder.layers.8.self_attn.q_proj.bias\n",
      "model.decoder.layers.8.self_attn.out_proj.weight\n",
      "model.decoder.layers.8.self_attn.out_proj.bias\n",
      "model.decoder.layers.8.self_attn_layer_norm.weight\n",
      "model.decoder.layers.8.self_attn_layer_norm.bias\n",
      "model.decoder.layers.8.fc1.weight\n",
      "model.decoder.layers.8.fc1.bias\n",
      "model.decoder.layers.8.fc2.weight\n",
      "model.decoder.layers.8.fc2.bias\n",
      "model.decoder.layers.8.final_layer_norm.weight\n",
      "model.decoder.layers.8.final_layer_norm.bias\n",
      "model.decoder.layers.9.self_attn.k_proj.weight\n",
      "model.decoder.layers.9.self_attn.k_proj.bias\n",
      "model.decoder.layers.9.self_attn.v_proj.weight\n",
      "model.decoder.layers.9.self_attn.v_proj.bias\n",
      "model.decoder.layers.9.self_attn.q_proj.weight\n",
      "model.decoder.layers.9.self_attn.q_proj.bias\n",
      "model.decoder.layers.9.self_attn.out_proj.weight\n",
      "model.decoder.layers.9.self_attn.out_proj.bias\n",
      "model.decoder.layers.9.self_attn_layer_norm.weight\n",
      "model.decoder.layers.9.self_attn_layer_norm.bias\n",
      "model.decoder.layers.9.fc1.weight\n",
      "model.decoder.layers.9.fc1.bias\n",
      "model.decoder.layers.9.fc2.weight\n",
      "model.decoder.layers.9.fc2.bias\n",
      "model.decoder.layers.9.final_layer_norm.weight\n",
      "model.decoder.layers.9.final_layer_norm.bias\n",
      "model.decoder.layers.10.self_attn.k_proj.weight\n",
      "model.decoder.layers.10.self_attn.k_proj.bias\n",
      "model.decoder.layers.10.self_attn.v_proj.weight\n",
      "model.decoder.layers.10.self_attn.v_proj.bias\n",
      "model.decoder.layers.10.self_attn.q_proj.weight\n",
      "model.decoder.layers.10.self_attn.q_proj.bias\n",
      "model.decoder.layers.10.self_attn.out_proj.weight\n",
      "model.decoder.layers.10.self_attn.out_proj.bias\n",
      "model.decoder.layers.10.self_attn_layer_norm.weight\n",
      "model.decoder.layers.10.self_attn_layer_norm.bias\n",
      "model.decoder.layers.10.fc1.weight\n",
      "model.decoder.layers.10.fc1.bias\n",
      "model.decoder.layers.10.fc2.weight\n",
      "model.decoder.layers.10.fc2.bias\n",
      "model.decoder.layers.10.final_layer_norm.weight\n",
      "model.decoder.layers.10.final_layer_norm.bias\n",
      "model.decoder.layers.11.self_attn.k_proj.weight\n",
      "model.decoder.layers.11.self_attn.k_proj.bias\n",
      "model.decoder.layers.11.self_attn.v_proj.weight\n",
      "model.decoder.layers.11.self_attn.v_proj.bias\n",
      "model.decoder.layers.11.self_attn.q_proj.weight\n",
      "model.decoder.layers.11.self_attn.q_proj.bias\n",
      "model.decoder.layers.11.self_attn.out_proj.weight\n",
      "model.decoder.layers.11.self_attn.out_proj.bias\n",
      "model.decoder.layers.11.self_attn_layer_norm.weight\n",
      "model.decoder.layers.11.self_attn_layer_norm.bias\n",
      "model.decoder.layers.11.fc1.weight\n",
      "model.decoder.layers.11.fc1.bias\n",
      "model.decoder.layers.11.fc2.weight\n",
      "model.decoder.layers.11.fc2.bias\n",
      "model.decoder.layers.11.final_layer_norm.weight\n",
      "model.decoder.layers.11.final_layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "for n, m in model.named_parameters():\n",
    "    print(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('pyt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01ca8e50707366aa71ffa41d1a1f13415d8b38eae741916c87480058f12910d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

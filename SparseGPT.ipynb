{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf348a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torch\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = load_dataset('c4', 'en', streaming=True)\n",
    "model_text = \"facebook/opt-125m\"\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_text)\n",
    "\n",
    "# Load model with pre-trained head\n",
    "model = OPTForCausalLM.from_pretrained(model_text, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "generator = pipeline('text-generation', model=model_text)\n",
    "\n",
    "calibration_data = []\n",
    "for i, data in enumerate(iter(dataset['train'])):\n",
    "    if i > 128:\n",
    "        break\n",
    "    tokenized = tokenizer.encode(data['text'], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "    calibration_data.append(tokenized)\n",
    "calibration_data = torch.squeeze(torch.stack(calibration_data)).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49669afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def inverse_hessian(X, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Calculate the inverse of a positive-definite matrix using the Cholesky decomposition.\n",
    "    Args:\n",
    "    - X (torch.Tensor): dxn tensor\n",
    "    - epsilon (float): small constant to prevent Hessian from being singular\n",
    "    Returns:\n",
    "    - torch.Tensor: inverted matrix\n",
    "    \"\"\"\n",
    "    X = X.float()\n",
    "    X_T = torch.transpose(X, 0, 1)\n",
    "    identity = torch.eye(X.shape[0], dtype=torch.float32)\n",
    "    H_inv = torch.inverse(2 * (X @ X_T + epsilon * identity))\n",
    "    #H_inv = torch.cholesky(H_inv).T\n",
    "    H_inv = torch.lu(H_inv)[0].T\n",
    "    \n",
    "    return H_inv\n",
    "\n",
    "# W is weights matrix for one layer\n",
    "# H_inv is inverse hessian for one layer\n",
    "# p is proportion of weights to 0\n",
    "# B is lazy block size, low B helps to reduce memory use\n",
    "# Bs is inverse of how often to make masks (e.g. when Bs is 4, make new masks with 20% sparseness every 4 columns)\n",
    "def calculate_mask(W, H_inv, p, B, Bs):\n",
    "    # Get the number of rows and columns in W\n",
    "    d_row, d_col = W.shape\n",
    "    \n",
    "    # Initialize the pruning mask M and block quantization errors E to all zeros\n",
    "    M = torch.zeros(d_row, d_col, dtype=torch.bool)\n",
    "    E = torch.zeros(d_row, B)\n",
    "\n",
    "    # only need to calculate w_square and h_square once\n",
    "    # w_square = torch.square(W)\n",
    "    # h_square = torch.square(H_inv)\n",
    "\n",
    "    # Loop over blocks of columns of W\n",
    "    for i in range(0, d_col, B):\n",
    "        # Loop over columns within a block\n",
    "        for j in range(i, min(i + B, d_col)):\n",
    "            # If j is a multiple of Bs, prune a portion of the weights\n",
    "            if j % Bs == 0:\n",
    "                # Get the mask for the largest (1 - p)% of weights based on squared value and inverse hessian\n",
    "\n",
    "                # prune_values is matrix of w^2/H^(-1)_cc\n",
    "                \n",
    "                w_square_section = torch.square(W[:, j:j+Bs])\n",
    "                h_square_section = torch.square(H_inv[j:j+Bs, j:j+Bs]).diag() # 1 dimensional vector\n",
    "\n",
    "                # print(\"weights squared and h_inv:\")\n",
    "                # print(w_square_section)\n",
    "                # print(h_square_section)\n",
    "\n",
    "                prune_values = w_square_section / h_square_section.unsqueeze(0)\n",
    "                # print(\"prune values: \")\n",
    "                # print(prune_values)\n",
    "\n",
    "                cutoff_value = torch.kthvalue(prune_values, int((1 - p) * d_row), dim=0)[0]\n",
    "                # print(\"cutoff value: \")\n",
    "                # print(cutoff_value)\n",
    "    \n",
    "                # print(\"mask: \")\n",
    "                mask = prune_values > cutoff_value\n",
    "            \n",
    "                M[:, j:j+Bs] = mask\n",
    "\n",
    "            # Calculate the pruning error for this column\n",
    "            E[:, j-i] = W[:, j] / H_inv[j, j]\n",
    "            # Freeze the weights that are not pruned by multiplying by the pruning mask\n",
    "            # Invert mask (~M equivalent to 1 - M)\n",
    "            E[:, j-i] = ~M[:, j] * E[:, j-i]\n",
    "            # Update the weights in this block based on the pruning error and inverse hessian information\n",
    "            W[:, j:i+B] -= torch.ger(E[:, j-i], H_inv[j, j:i+B])\n",
    "        # Update all remaining weights\n",
    "        W[:, i+B:] -= torch.matmul(E, H_inv[i:i+B, i+B:])\n",
    "    \n",
    "    # return mask\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faee467a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = torch.randn(512, 512, dtype=torch.float32)\n",
    "lmbda = 0.1\n",
    "#print(torch.transpose(calibration_data,0,1).shape)\n",
    "H_inv = inverse_hessian(torch.transpose(calibration_data,0,1), lmbda)\n",
    "#H_inv = inverse_hessian(X, lmbda)\n",
    "print(H_inv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e80748e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.9859e+00, -2.4619e-03, -3.8556e-03,  ..., -5.4346e-04,\n",
       "         -3.8320e-05,  1.7932e-04],\n",
       "        [-4.2152e-05,  9.1403e-05,  1.9986e-01,  ...,  1.1777e-01,\n",
       "         -1.4799e-02,  9.2090e-04],\n",
       "        [ 3.2381e-05, -2.9018e-04, -3.2976e-04,  ...,  1.6778e-02,\n",
       "          1.0439e-02, -5.0478e-02],\n",
       "        ...,\n",
       "        [ 1.1874e-03,  5.0497e-03,  4.4598e-03,  ..., -1.8853e-09,\n",
       "         -1.9854e-02,  2.5553e-02],\n",
       "        [-1.8534e-03, -1.0318e-03, -1.3461e-03,  ..., -3.2509e-09,\n",
       "         -1.5735e-09, -9.7070e-01],\n",
       "        [ 1.1275e-04, -4.1557e-04,  4.2876e-04,  ..., -6.0911e-10,\n",
       "         -1.9336e-09, -2.3139e-09]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f75d50",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a04e81",
   "metadata": {},
   "source": [
    "# Prune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5af3ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([129, 129])\n",
      "model.decoder.layers.0.self_attn.k_proj.weight\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zeros(): argument 'size' must be tuple of SymInts, but found element of type float at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.decoder.layers\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name)\n\u001b[1;32m----> 6\u001b[0m     param \u001b[38;5;241m=\u001b[39m \u001b[43msparse_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_hess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[113], line 48\u001b[0m, in \u001b[0;36msparse_gpt\u001b[1;34m(weights, H_inv, B, Bs, p)\u001b[0m\n\u001b[0;32m     45\u001b[0m M \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(drow, dcol)\u001b[38;5;241m.\u001b[39mbyte()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Initialize E as a 1d tensor with shape (B,) and fill with zeros\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m E \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Loop over the blocks of columns\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, dcol, B):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Loop over the elements within each block\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: zeros(): argument 'size' must be tuple of SymInts, but found element of type float at pos 2"
     ]
    }
   ],
   "source": [
    "inv_hess = inverse_hessian(calibration_data, 0.2)\n",
    "print(inv_hess.shape)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    if 'model.decoder.layers' in name:\n",
    "        print(name)\n",
    "        param = sparse_gpt(param, inv_hess, 0.5, 32, 32)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac281cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, my dog is cute and I love her. I'm a little nervous about her because she's a little bit shy and I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "input2 = tokenizer(\"What the fuck did you just fucking say about me, you little bitch?\", return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "output = model.generate(input1.input_ids, max_length=100, num_return_sequences=1, temperature=0.5, top_p=0.95)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "07a6f8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3598d1fcb68810e1ab8d19050593974172dcc02c5f12874feb8b1a070749563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

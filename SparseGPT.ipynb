{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from calculate_mask import calculate_mask\n",
    "from inverse_hessian import inverse_hessian\n",
    "from input_prehooks import put_input_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000e+00, 4.8290e+04, 7.1300e+03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        [2.0000e+00, 4.8763e+04, 1.1000e+01,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        [2.0000e+00, 5.9700e+02, 1.4189e+04,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        [2.0000e+00, 6.1790e+03, 1.7100e+02,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        [2.0000e+00, 1.3300e+02, 4.4650e+03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "#Load dataset\n",
    "dataset = load_dataset('c4', 'en', streaming=True)\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load model with pre-trained head\n",
    "\n",
    "model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "# Load generator\n",
    "generator = pipeline('text-generation', model=model_name)\n",
    "# Create calibration data\n",
    "\n",
    "calibration_size=4\n",
    "token_length=512\n",
    "\n",
    "calibration_data_array = []\n",
    "for i, data in enumerate(iter(dataset['train'])): # type: ignore\n",
    "    if i > calibration_size:\n",
    "        break\n",
    "    tokenized = tokenizer.encode(data['text'], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=token_length)\n",
    "    calibration_data_array.append(tokenized)\n",
    "calibration_data = torch.squeeze(torch.stack(calibration_data_array)).to(device=device)\n",
    "calibration_data.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, put in forward hooks\n",
    "features = {}\n",
    "put_input_hooks(model=model, features=features)\n",
    "\n",
    "# Run calibration data through model at first to calculate features dictionary with\n",
    "# input tensors to each intermediate layer\n",
    "model(calibration_data)\n",
    "\n",
    "# function to get module name from parameter name\n",
    "def get_module_name(param_name):\n",
    "    if param_name[-5:] == \".bias\":\n",
    "        return param_name[:-5], \"bias\"\n",
    "    elif param_name[-7:] == \".weight\":\n",
    "        return param_name[:-7], \"weight\"\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load model with pre-trained head\n",
    "# model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "# make a dictionary to access module by name\n",
    "module_lookup_dict = {}\n",
    "for module_name, module_iter in model.named_modules():\n",
    "    module_lookup_dict[module_name] = module_iter\n",
    "EPSILON = 0.0001\n",
    "SPARSENESS = .9\n",
    "B = 128\n",
    "Bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:28<00:00,  6.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "layer_blacklist = ['model.decoder.embed_tokens.weight', 'model.decoder.embed_tokens.bias',\n",
    "'model.decoder.embed_positions.weight']\n",
    "\n",
    "# Using calibration data (inputs to each intermediate weight layer)\n",
    "# Iterate through named parameters, calculate inverse hessian and calculate mask\n",
    "\n",
    "# without this\n",
    "param_lookup_dict = {}\n",
    "param_names = []\n",
    "for name, param in model.named_parameters():\n",
    "    param_names.append(name)\n",
    "    param_lookup_dict[name] = param\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name in tqdm(param_names):\n",
    "        param = param_lookup_dict[name]\n",
    "\n",
    "        # skip the embed layer\n",
    "        if name in layer_blacklist:\n",
    "            continue\n",
    "        \n",
    "        # skip norms which have 1 dimension\n",
    "        if len(param.shape) < 2:\n",
    "            continue\n",
    "\n",
    "        module_name, param_type = get_module_name(name)\n",
    "\n",
    "        # apply to weight and bias layers\n",
    "        if param_type == \"weight\" or param_type == \"bias\":\n",
    "            # input to parameter\n",
    "            layer_input = features[module_name][0]\n",
    "\n",
    "            # calculate inverse hessian\n",
    "            # check if input is flattened e.g. from 8,512,768 to 4096,768\n",
    "            if len(layer_input.shape) == 2:\n",
    "                inv_hess = inverse_hessian(torch.transpose(layer_input, 0, 1), epsilon=EPSILON, \n",
    "                flattened=True)\n",
    "\n",
    "            else:\n",
    "                inv_hess = inverse_hessian(torch.transpose(layer_input, 1, 2), epsilon=EPSILON,\n",
    "                flattened=False)\n",
    "\n",
    "            # calculate mask\n",
    "            mask = calculate_mask(W=param, H_inv=inv_hess, p=SPARSENESS, B=B, Bs=Bs)\n",
    "            \n",
    "            # get module from lookup dictionary by module name\n",
    "            module = module_lookup_dict[module_name]\n",
    "            # apply mask\n",
    "            prune.custom_from_mask(module=module, name=param_type, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PRUNED MODEL\n",
    "pruned_model_name = 'opt-125m'\n",
    "# torch.save(model,'pruned_models/' + pruned_model_name)\n",
    "# model.save_pretrained(save_directory = 'pruned_models/' + pruned_model_name)\n",
    "\n",
    "torch.save(model.state_dict(), 'pruned_models/opt-125m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     42\u001b[0m         prune\u001b[39m.\u001b[39mcustom_from_mask(module\u001b[39m=\u001b[39mmodule, name\u001b[39m=\u001b[39mparam_type, mask\u001b[39m=\u001b[39mmask)\n\u001b[1;32m---> 44\u001b[0m apply_identity_prune(model\u001b[39m=\u001b[39;49mloaded_model)\n\u001b[0;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m n, m \u001b[39min\u001b[39;00m loaded_model\u001b[39m.\u001b[39mnamed_parameters():\n\u001b[0;32m     46\u001b[0m     \u001b[39mprint\u001b[39m(n)\n",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m, in \u001b[0;36mapply_identity_prune\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     28\u001b[0m     param_lookup_dict[name] \u001b[39m=\u001b[39m param\n\u001b[0;32m     30\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m tqdm(param_names):\n\u001b[0;32m     32\u001b[0m         param \u001b[39m=\u001b[39m param_lookup_dict[name]\n\u001b[0;32m     34\u001b[0m         \u001b[39m# skip the embed layer\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# LOAD PRUNED MODEL\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')\n",
    "\n",
    "loaded_model = OPTForCausalLM.from_pretrained('facebook/opt-125m', output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "def apply_identity_prune(model):\n",
    "    layer_blacklist = ['loaded_model.decoder.embed_tokens.weight', 'loaded_model.decoder.embed_tokens.bias',\n",
    "    'loaded_model.decoder.embed_positions.weight']\n",
    "\n",
    "    # Using calibration data (inputs to each intermediate weight layer)\n",
    "    # Iterate through named parameters, calculate inverse hessian and calculate mask\n",
    "\n",
    "    # without this\n",
    "    param_lookup_dict = {}\n",
    "    param_names = []\n",
    "    for name, param in model.named_parameters():\n",
    "        param_names.append(name)\n",
    "        param_lookup_dict[name] = param\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name in tqdm(param_names):\n",
    "            param = param_lookup_dict[name]\n",
    "\n",
    "            # skip the embed layer\n",
    "            # if name in layer_blacklist:\n",
    "            #     continue\n",
    "\n",
    "            if 'embed' in name:\n",
    "                continue\n",
    "            \n",
    "            # skip norms which have 1 dimension\n",
    "            if len(param.shape) < 2:\n",
    "                continue\n",
    "        \n",
    "        prune.custom_from_mask(module=module, name=param_type, mask=mask)\n",
    "\n",
    "apply_identity_prune(model=loaded_model)\n",
    "for n, m in loaded_model.named_parameters():\n",
    "    print(n)\n",
    "\n",
    "# loaded_model.load_state_dict(torch.load('pruned_models/opt-125m'))\n",
    "\n",
    "# pretrained_model = 'pruned_models/opt-125m'\n",
    "\n",
    "# model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "# loaded_model = OPTForCausalLM.from_pretrained(pretrained_model).to(device=device) # type: ignore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello, my dog is cute and I love her. I'm a little nervous about her because she's a little bit shy and I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device=device)\n",
    "input2 = tokenizer(\"What the fuck did you just fucking say about me, you little bitch?\", return_tensors=\"pt\", padding=\"max_length\", truncation=True).to(device=device)\n",
    "output = loaded_model.generate(input1.input_ids, max_length=100, num_return_sequences=1, temperature=0.9, top_p=0.95)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Proportion of weights that are 0:\n",
    "\n",
    "def get_prop_zeros(model):\n",
    "    return torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) / (torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) + torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight != 0))\n",
    "\n",
    "print(get_prop_zeros(loaded_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, my dog is cute and I love her. I'm a little nervous about her because she's a little bit shy and I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REGULAR OUTPUT\n",
    "model2 = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "input1 = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "input2 = tokenizer(\"What the fuck did you just fucking say about me, you little bitch?\", return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "output = model2.generate(input1.input_ids, max_length=100, num_return_sequences=1, temperature=0.9, top_p=0.95)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtesting_module\u001b[39;00m \u001b[39mimport\u001b[39;00m calculate_perp\n\u001b[0;32m      3\u001b[0m \u001b[39m# def calculate_perp(model, input_data, device):\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m#     input_data = torch.squeeze(torch.stack(input_data)).to(device=device)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m#     input_data.double()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[39m# lmao out of memory\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[39mprint\u001b[39m(calculate_perp(model, calibration_data_array, \u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\phill\\Documents\\Deep Learning\\Sparse-GPT-Finetuning\\testing_module.py:13\u001b[0m, in \u001b[0;36mcalculate_perp\u001b[1;34m(model, input_data, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m input_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(torch\u001b[39m.\u001b[39mstack(input_data))\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     12\u001b[0m input_data\u001b[39m.\u001b[39mdouble()\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[39m=\u001b[39m model(input_data)[\u001b[39m0\u001b[39m] \n\u001b[0;32m     14\u001b[0m log_probs \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mlog_softmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m neg_log_likelihood \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mlog_probs\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\phill\\anaconda3\\envs\\pyt-exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[1;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[1;32mc:\\Users\\phill\\anaconda3\\envs\\pyt-exp\\lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:929\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    926\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m    928\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 929\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m    930\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    931\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    932\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    933\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    934\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    935\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    936\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    937\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    938\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    939\u001b[0m )\n\u001b[0;32m    941\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    943\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\phill\\anaconda3\\envs\\pyt-exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[1;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[1;32mc:\\Users\\phill\\anaconda3\\envs\\pyt-exp\\lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:628\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    625\u001b[0m past_key_values_length \u001b[39m=\u001b[39m past_key_values[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 628\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_tokens(input_ids)\n\u001b[0;32m    630\u001b[0m \u001b[39m# embed positions\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\phill\\anaconda3\\envs\\pyt-exp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[1;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[1;32mc:\\Users\\phill\\anaconda3\\envs\\pyt-exp\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\phill\\anaconda3\\envs\\pyt-exp\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "from testing_module import calculate_perp\n",
    "\n",
    "# def calculate_perp(model, input_data, device):\n",
    "#     input_data = torch.squeeze(torch.stack(input_data)).to(device=device)\n",
    "#     input_data.double()\n",
    "#     outputs = model(input_data)[0] \n",
    "#     log_probs = outputs[0, -1, :].log_softmax(-1)\n",
    "#     neg_log_likelihood = -log_probs.mean()\n",
    "#     perplexity = torch.exp(neg_log_likelihood)      \n",
    "#     return perplexity.item()\n",
    "\n",
    "# print(calculate_perp(model, input_data, device))\n",
    "\n",
    "# lmao out of memory\n",
    "print(calculate_perp(model, calibration_data_array, 'cpu'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e86ec8e8174c6e268751d4c1c60ae4faf419c25d731f3f2c6b91607c46f997f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e524b42c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5cfe088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils.mask_utils import calculate_mask\n",
    "from utils.hessian_utils import calc_inverse_hessian\n",
    "from utils.prehook_utils import put_input_hooks\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9d694",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ebc486",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALIBRATION_SIZE=128\n",
    "TOKEN_LENGTH=1024\n",
    "CALIBRATION_BATCH_SIZE=1\n",
    "\n",
    "EPSILON = 1e-8\n",
    "B = 128 \n",
    "Bs = 128\n",
    "\n",
    "#hyperparam test, remove later\n",
    "EPOCH_COUNT = 10\n",
    "\n",
    "#set device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "MODEL_NAME = \"opt-6.7b\"\n",
    "\n",
    "#load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'facebook/{MODEL_NAME}')\n",
    "#Load dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d03ec",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c1056",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3216a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on batches of calibration data, then concatenate inputs\n",
    "def split_model_calibration(model):\n",
    "    batch_sentences = []\n",
    "    for i, data in tqdm(enumerate(iter(dataset['train'])), total=CALIBRATION_SIZE):\n",
    "        if i < CALIBRATION_SIZE + 1:\n",
    "            if len(batch_sentences) >= CALIBRATION_BATCH_SIZE:\n",
    "                with torch.no_grad():\n",
    "                    encoded_input = tokenizer(batch_sentences, return_tensors=\"pt\",\n",
    "                                              padding=\"max_length\", max_length=TOKEN_LENGTH,\n",
    "                                              truncation=True).to(device=DEVICE)\n",
    "                    model(**encoded_input, labels=encoded_input.input_ids)\n",
    "                    del encoded_input\n",
    "                    torch.cuda.empty_cache()\n",
    "                    batch_sentences = []\n",
    "            batch_sentences.append(data['text'])\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf95af6",
   "metadata": {},
   "source": [
    "# SparseGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8919231",
   "metadata": {},
   "source": [
    "## Prune models (SparseGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fef33d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c538b5dc5947138841ea64d7caee16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 25 13:12:49 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    60W / 250W |  26355MiB / 40960MiB |     43%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCI...  On   | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    36W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCI...  On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    34W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PCI...  On   | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    33W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1806      C   ...l8/apps/conda3/bin/python    26353MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/128 [00:00<00:30,  4.17it/s]\u001b[A/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "  2%|▏         | 2/128 [00:16<20:44,  9.88s/it]\u001b[A\n",
      "  2%|▏         | 3/128 [00:39<32:38, 15.67s/it]\u001b[A\n",
      "  3%|▎         | 4/128 [01:02<38:03, 18.41s/it]\u001b[A\n",
      "  4%|▍         | 5/128 [01:24<40:51, 19.93s/it]\u001b[A\n",
      "  5%|▍         | 6/128 [01:47<42:25, 20.86s/it]\u001b[A\n",
      "  5%|▌         | 7/128 [02:10<43:15, 21.45s/it]\u001b[A\n",
      "  6%|▋         | 8/128 [02:32<43:39, 21.83s/it]\u001b[A\n",
      "  7%|▋         | 9/128 [02:55<43:50, 22.10s/it]\u001b[A\n",
      "  8%|▊         | 10/128 [03:18<43:48, 22.27s/it]\u001b[A\n",
      "  9%|▊         | 11/128 [03:40<43:40, 22.40s/it]\u001b[A\n",
      "  9%|▉         | 12/128 [04:03<43:28, 22.49s/it]\u001b[A\n",
      " 10%|█         | 13/128 [04:26<43:11, 22.54s/it]\u001b[A\n",
      " 11%|█         | 14/128 [04:48<42:53, 22.58s/it]\u001b[A\n",
      " 12%|█▏        | 15/128 [05:11<42:34, 22.61s/it]\u001b[A\n",
      " 12%|█▎        | 16/128 [05:34<42:13, 22.62s/it]\u001b[A\n",
      " 13%|█▎        | 17/128 [05:56<41:53, 22.65s/it]\u001b[A\n",
      " 14%|█▍        | 18/128 [06:19<41:32, 22.66s/it]\u001b[A\n",
      " 15%|█▍        | 19/128 [06:42<41:10, 22.66s/it]\u001b[A\n",
      " 16%|█▌        | 20/128 [07:04<40:48, 22.67s/it]\u001b[A\n",
      " 16%|█▋        | 21/128 [07:27<40:25, 22.67s/it]\u001b[A\n",
      " 17%|█▋        | 22/128 [07:50<40:03, 22.67s/it]\u001b[A\n",
      " 18%|█▊        | 23/128 [08:12<39:41, 22.68s/it]\u001b[A\n",
      " 19%|█▉        | 24/128 [08:35<39:19, 22.68s/it]\u001b[A\n",
      " 20%|█▉        | 25/128 [08:58<38:56, 22.68s/it]\u001b[A\n",
      " 20%|██        | 26/128 [09:20<38:34, 22.69s/it]\u001b[A\n",
      " 21%|██        | 27/128 [09:43<38:11, 22.69s/it]\u001b[A\n",
      " 22%|██▏       | 28/128 [10:06<37:49, 22.69s/it]\u001b[A\n",
      " 23%|██▎       | 29/128 [10:28<37:26, 22.69s/it]\u001b[A\n",
      " 23%|██▎       | 30/128 [10:51<37:03, 22.69s/it]\u001b[A\n",
      " 24%|██▍       | 31/128 [11:14<36:41, 22.69s/it]\u001b[A\n",
      " 25%|██▌       | 32/128 [11:37<36:17, 22.69s/it]\u001b[A\n",
      " 26%|██▌       | 33/128 [11:59<35:55, 22.69s/it]\u001b[A\n",
      " 27%|██▋       | 34/128 [12:22<35:32, 22.68s/it]\u001b[A\n",
      " 27%|██▋       | 35/128 [12:45<35:09, 22.68s/it]\u001b[A\n",
      " 28%|██▊       | 36/128 [13:07<34:47, 22.69s/it]\u001b[A\n",
      " 29%|██▉       | 37/128 [13:30<34:24, 22.69s/it]\u001b[A\n",
      " 30%|██▉       | 38/128 [13:53<34:02, 22.69s/it]\u001b[A\n",
      " 30%|███       | 39/128 [14:15<33:39, 22.69s/it]\u001b[A\n",
      " 31%|███▏      | 40/128 [14:38<33:22, 22.76s/it]\u001b[A\n",
      " 32%|███▏      | 41/128 [15:01<33:11, 22.89s/it]\u001b[A\n",
      " 33%|███▎      | 42/128 [15:24<32:43, 22.83s/it]\u001b[A\n",
      " 34%|███▎      | 43/128 [15:47<32:16, 22.79s/it]\u001b[A\n",
      " 34%|███▍      | 44/128 [16:10<31:51, 22.76s/it]\u001b[A\n",
      " 35%|███▌      | 45/128 [16:32<31:26, 22.73s/it]\u001b[A\n",
      " 36%|███▌      | 46/128 [16:55<31:03, 22.73s/it]\u001b[A\n",
      " 37%|███▋      | 47/128 [17:18<30:39, 22.71s/it]\u001b[A\n",
      " 38%|███▊      | 48/128 [17:40<30:16, 22.71s/it]\u001b[A\n",
      " 38%|███▊      | 49/128 [18:03<29:53, 22.71s/it]\u001b[A\n",
      " 39%|███▉      | 50/128 [18:26<29:30, 22.70s/it]\u001b[A\n",
      " 40%|███▉      | 51/128 [18:48<29:07, 22.69s/it]\u001b[A\n",
      " 41%|████      | 52/128 [19:11<28:44, 22.70s/it]\u001b[A\n",
      " 41%|████▏     | 53/128 [19:34<28:21, 22.69s/it]\u001b[A\n",
      " 42%|████▏     | 54/128 [19:56<27:59, 22.69s/it]\u001b[A\n",
      " 43%|████▎     | 55/128 [20:19<27:36, 22.69s/it]\u001b[A\n",
      " 44%|████▍     | 56/128 [20:42<27:13, 22.69s/it]\u001b[A\n",
      " 45%|████▍     | 57/128 [21:05<26:51, 22.69s/it]\u001b[A\n",
      " 45%|████▌     | 58/128 [21:27<26:28, 22.69s/it]\u001b[A\n",
      " 46%|████▌     | 59/128 [21:50<26:05, 22.68s/it]\u001b[A\n",
      " 47%|████▋     | 60/128 [22:13<25:42, 22.69s/it]\u001b[A\n",
      " 48%|████▊     | 61/128 [22:35<25:19, 22.68s/it]\u001b[A\n",
      " 48%|████▊     | 62/128 [22:58<24:57, 22.68s/it]\u001b[A\n",
      " 49%|████▉     | 63/128 [23:21<24:47, 22.89s/it]\u001b[A\n",
      " 50%|█████     | 64/128 [23:44<24:20, 22.83s/it]\u001b[A\n",
      " 51%|█████     | 65/128 [24:07<23:55, 22.79s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "from utils.prune_utils import sparsegpt_prune\n",
    "\n",
    "SPARSITIES = [0.2,0.3,0.5,0.7,0.9,1]#0.1, 0.2,0.3,0.5,0.7,0.9,1\n",
    "    \n",
    "for i, SPARSITY in enumerate(tqdm(SPARSITIES, total=len(SPARSITIES))):\n",
    "    model = OPTForCausalLM.from_pretrained(f'facebook/{MODEL_NAME}', \n",
    "                                       output_attentions=True, \n",
    "                                       output_hidden_states=True).to(device=DEVICE)\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "    !nvidia-smi\n",
    "    # Calculate feature hessians only for the first iteration\n",
    "    if i == 0:\n",
    "        feature_hessians = {}\n",
    "        #put_input_hooks(model=model, features=feature_hessians, storage_dir=storage_dir, offload_freq=10000, feature_storage_device='cpu')\n",
    "        put_input_hooks(model=model, features=feature_hessians, feature_storage_device='cpu')\n",
    "        split_model_calibration(model)\n",
    "    \n",
    "    # Prune using the sparseGPT method, saves as pruned_models/{model_name}-{SPARSENESS}.pt WITHOUT mask\n",
    "    sparsegpt_prune(model, MODEL_NAME, feature_hessians, EPSILON, SPARSITY, B, Bs)\n",
    "    !nvidia-smi\n",
    "    del model\n",
    "del feature_hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c94384d",
   "metadata": {},
   "source": [
    "## Finetune models (SparseGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00620490",
   "metadata": {},
   "source": [
    "from finetune_utils import finetune_model\n",
    "\n",
    "SPARSITIES = [0.2,0.3,0.5,0.7,0.9,1]#0.1, 0.2,0.3,0.5,0.7,0.9,1\n",
    "    \n",
    "for i, SPARSITY in enumerate(tqdm(SPARSITIES, total=len(SPARSITIES))):\n",
    "    # Finetune, saves as pruned_models/{model_name}-{SPARSENESS}-finetuned.pt WITHOUT mask\n",
    "    finetune_model(MODEL_NAME, tokenizer, SPARSITY, EPOCH_COUNT=EPOCH_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a32b3c",
   "metadata": {},
   "source": [
    "## Test models (SparseGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0edef0",
   "metadata": {},
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c0e2db",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from testing_module import test_model\n",
    "\n",
    "TOKEN_LENGTH=1024\n",
    "STRIDE = 512\n",
    "wandb.init(project=\"ICLR\", \n",
    "           name = f'{MODEL_NAME} Wikitext Test', \n",
    "           config={'token_length': TOKEN_LENGTH,\n",
    "                 'model_name': MODEL_NAME,\n",
    "                 'stride': STRIDE,\n",
    "                 'fine_tuned': 'not finetuned'})\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'facebook/{MODEL_NAME}', \n",
    "                                          padding_side='left', \n",
    "                                          use_fast=False)\n",
    "# Load dataset\n",
    "test_set = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "encodings = tokenizer(\"\\n\\n\".join(test_set['text']), return_tensors='pt')\n",
    "\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "SPARSITIES = [0.2, 0.3, 0.5, 0.7, 0.9, 1]#, 0.4, 0.6, 0.8, 1\n",
    "\n",
    "for SPARSITY in SPARSITIES:\n",
    "    test_model(model_name, encodings, token_length, seq_len, stride, wandb, SPARSITY, is_finetuned=False)\n",
    "    \n",
    "### NOW DO FINETUNED\n",
    "wandb.init(project=\"ICLR\", \n",
    "           name = f'{model_name} Wikitext Test', \n",
    "           config={'token_length': token_length,\n",
    "                 'model_name': model_name,\n",
    "                 'stride': stride,\n",
    "                 'fine_tuned': 'finetuned'})\n",
    "for SPARSITY in SPARSITIES:\n",
    "    test_model(model_name, encodings, token_length, seq_len, stride, wandb, SPARSITY, is_finetuned=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5a72a",
   "metadata": {},
   "source": [
    "# Cerebras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2447310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

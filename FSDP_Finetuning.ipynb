{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a50c37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import argparse\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import  DataCollatorForLanguageModeling, OPTForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "from utils.save_utils import load_masked_model, load_masked_model_single\n",
    "\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from tqdm import tqdm\n",
    "\n",
    "########################################################################\n",
    "# This is a fully working simple example to use Accelerate\n",
    "#\n",
    "# This example trains a Bert base model on GLUE MRPC\n",
    "# in any of the following settings (with the same script):\n",
    "#   - single CPU or single GPU\n",
    "#   - multi GPUS (using PyTorch distributed mode)\n",
    "#   - (multi) TPUs\n",
    "#   - fp16 (mixed-precision) or fp32 (normal precision)\n",
    "#   - FSDP\n",
    "#\n",
    "# This example also demonstrates the checkpointing and sharding capabilities\n",
    "#\n",
    "# To run it in each of these various modes, follow the instructions\n",
    "# in the readme for examples:\n",
    "# https://github.com/huggingface/accelerate/tree/main/examples\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "\n",
    "MAX_GPU_BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "# New Code #\n",
    "# Converting Bytes to Megabytes\n",
    "def b2mb(x):\n",
    "    return int(x / 2**20)\n",
    "\n",
    "\n",
    "# New Code #\n",
    "# This context manager is used to track the peak memory usage of the process\n",
    "class TorchTracemalloc:\n",
    "    def __enter__(self):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_max_memory_allocated()  # reset the peak gauge to zero\n",
    "        self.begin = torch.cuda.memory_allocated()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        self.end = torch.cuda.memory_allocated()\n",
    "        self.peak = torch.cuda.max_memory_allocated()\n",
    "        self.used = b2mb(self.end - self.begin)\n",
    "        self.peaked = b2mb(self.peak - self.begin)\n",
    "        # print(f\"delta used/peak {self.used:4d}/{self.peaked:4d}\")\n",
    "\n",
    "\n",
    "# For testing only\n",
    "if os.environ.get(\"TESTING_MOCKED_DATALOADERS\", None) == \"1\":\n",
    "    from accelerate.test_utils.training import mocked_dataloaders\n",
    "\n",
    "    get_dataloaders = mocked_dataloaders  # noqa: F811\n",
    "\n",
    "\n",
    "def training_function(config, args):\n",
    "    # For testing only\n",
    "    if os.environ.get(\"TESTING_MOCKED_DATALOADERS\", None) == \"1\":\n",
    "        config[\"num_epochs\"] = 2\n",
    "    # Initialize accelerator\n",
    "    if args.with_tracking:\n",
    "        accelerator = Accelerator(\n",
    "            cpu=args.cpu, mixed_precision=args.mixed_precision, log_with=\"wandb\", logging_dir=args.logging_dir\n",
    "        )\n",
    "    else:\n",
    "        accelerator = Accelerator()\n",
    "    accelerator.print(accelerator.distributed_type)\n",
    "\n",
    "    if hasattr(args.checkpointing_steps, \"isdigit\"):\n",
    "        if args.checkpointing_steps == \"epoch\":\n",
    "            checkpointing_steps = args.checkpointing_steps\n",
    "        elif args.checkpointing_steps.isdigit():\n",
    "            checkpointing_steps = int(args.checkpointing_steps)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Argument `checkpointing_steps` must be either a number or `epoch`. `{args.checkpointing_steps}` passed.\"\n",
    "            )\n",
    "    else:\n",
    "        checkpointing_steps = None\n",
    "    # Sample hyper-parameters for learning rate, batch size, seed and a few other HPs\n",
    "    lr = config[\"lr\"]\n",
    "    num_epochs = int(config[\"num_epochs\"])\n",
    "    seed = int(config[\"seed\"])\n",
    "    batch_size = int(config[\"batch_size\"])\n",
    "\n",
    "    # We need to initialize the trackers we use, and also store our configuration\n",
    "    if args.with_tracking:\n",
    "        experiment_config = vars(args)\n",
    "        accelerator.init_trackers(\"fsdp_glue_no_trainer\", experiment_config)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'facebook/{config[\"model_name\"]}', padding_side='left', model_max_length=512)\n",
    "    #datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "    datasets = load_dataset('c4', 'en', streaming=True)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # max_length=None => use the model max length (it's actually the default)\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "\n",
    "    # Apply the method we just defined to all the examples in all the splits of the dataset\n",
    "    # starting with the main process first:\n",
    "    with accelerator.main_process_first():\n",
    "        tokenized_datasets = datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\", \"timestamp\", \"url\"],\n",
    "        )\n",
    "\n",
    "    # We also rename the 'label' column to 'labels' which is the expected name for labels by the models of the\n",
    "    # transformers library\n",
    "    #tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "    # If the batch size is too big we use gradient accumulation\n",
    "    gradient_accumulation_steps = 1\n",
    "    if batch_size > MAX_GPU_BATCH_SIZE and accelerator.distributed_type != DistributedType.TPU:\n",
    "        gradient_accumulation_steps = batch_size // MAX_GPU_BATCH_SIZE\n",
    "        batch_size = MAX_GPU_BATCH_SIZE\n",
    "\n",
    "    def collate_fn(examples):\n",
    "        # On TPU it's best to pad everything to the same length or training will be very slow.\n",
    "        if accelerator.distributed_type == DistributedType.TPU:\n",
    "            return tokenizer.pad(examples, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "        return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "    # Instantiate dataloaders.\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"train\"], collate_fn=collate_fn, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Instantiate the model (we build the model here so that the seed also control new weights initialization)\n",
    "    #model = AutoModelForSequenceClassification.from_pretrained(args.model_name_or_path, return_dict=True)\n",
    "    model = OPTForCausalLM.from_pretrained(f'facebook/{config[\"model_name\"]}',\n",
    "                                                      output_attentions=True,\n",
    "                                                      output_hidden_states=True)\n",
    "\n",
    "    load_masked_model_single(model, f'pruned_models/{config[\"model_name\"]}-{config[\"sparsity\"]}.pt')\n",
    "\n",
    "    # New Code #\n",
    "    # For FSDP feature, it is highly recommended and efficient to prepare the model before creating optimizer\n",
    "    model = accelerator.prepare(model)\n",
    "    #accelerator.print(model)\n",
    "\n",
    "    # Instantiate optimizer\n",
    "    # New Code #\n",
    "    # For FSDP feature, at present it doesn't support multiple parameter groups,\n",
    "    # so we need to create a single parameter group for the whole model\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr, weight_decay=2e-4)\n",
    "\n",
    "    # Instantiate scheduler\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=2,\n",
    "        num_training_steps=(config[\"train_steps\"] * num_epochs) // gradient_accumulation_steps,\n",
    "    )\n",
    "\n",
    "    # New Code #\n",
    "    # For FSDP feature, prepare everything except the model as we have already prepared the model\n",
    "    # before creating the optimizer\n",
    "    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n",
    "    # prepare method.\n",
    "    optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    overall_step = 0\n",
    "\n",
    "    # Potentially load in the weights and states from a previous save\n",
    "    if args.resume_from_checkpoint:\n",
    "        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n",
    "            accelerator.print(f\"Resumed from checkpoint: {args.resume_from_checkpoint}\")\n",
    "            accelerator.load_state(args.resume_from_checkpoint)\n",
    "            path = os.path.basename(args.resume_from_checkpoint)\n",
    "        else:\n",
    "            # Get the most recent checkpoint\n",
    "            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n",
    "            dirs.sort(key=os.path.getctime)\n",
    "            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n",
    "        # Extract `epoch_{i}` or `step_{i}`\n",
    "        training_difference = os.path.splitext(path)[0]\n",
    "\n",
    "        if \"epoch\" in training_difference:\n",
    "            num_epochs -= int(training_difference.replace(\"epoch_\", \"\"))\n",
    "            resume_step = None\n",
    "        else:\n",
    "            resume_step = int(training_difference.replace(\"step_\", \"\"))\n",
    "            num_epochs -= resume_step // len(train_dataloader)\n",
    "            # If resuming by step, we also need to know exactly how far into the DataLoader we went\n",
    "            resume_step = (num_epochs * len(train_dataloader)) - resume_step\n",
    "\n",
    "    # Now we train the model\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # New Code #\n",
    "        # context manager to track the peak memory usage during the training epoch\n",
    "        with TorchTracemalloc() as tracemalloc:\n",
    "            model.train()\n",
    "            if args.with_tracking:\n",
    "                total_loss = 0\n",
    "            for step, batch in enumerate(tqdm(train_dataloader, total=config['max_step'])):\n",
    "                if step == config['max_step']:\n",
    "                    break\n",
    "                # We need to skip steps until we reach the resumed step\n",
    "                if args.resume_from_checkpoint and epoch == 0:\n",
    "                    if resume_step is not None and step < resume_step:\n",
    "                        pass\n",
    "                # We could avoid this line since we set the accelerator with `device_placement=True`.\n",
    "                batch.to(accelerator.device)\n",
    "                outputs = model(**batch, labels=batch['input_ids'])\n",
    "                loss = outputs.loss\n",
    "                #print(f'Loss: {loss}')\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "                # We keep track of the loss at each epoch\n",
    "                if args.with_tracking:\n",
    "                    total_loss += loss.detach().float()\n",
    "                accelerator.backward(loss)\n",
    "                if step % gradient_accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    # accelerator.print(lr_scheduler.get_lr())\n",
    "\n",
    "                overall_step += 1\n",
    "\n",
    "                if isinstance(checkpointing_steps, int):\n",
    "                    output_dir = f\"step_{overall_step}\"\n",
    "                    if overall_step % checkpointing_steps == 0:\n",
    "                        if args.output_dir is not None:\n",
    "                            output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                        accelerator.save_state(output_dir)\n",
    "        # New Code #\n",
    "        # Printing the GPU memory usage details such as allocated memory, peak memory, and total memory usage\n",
    "        accelerator.print(\"Memory before entering the train : {}\".format(b2mb(tracemalloc.begin)))\n",
    "        accelerator.print(\"Memory consumed at the end of the train (end-begin): {}\".format(tracemalloc.used))\n",
    "        accelerator.print(\"Peak Memory consumed during the train (max-begin): {}\".format(tracemalloc.peaked))\n",
    "        accelerator.print(\n",
    "            \"Total Peak Memory consumed during the train (max): {}\".format(\n",
    "                tracemalloc.peaked + b2mb(tracemalloc.begin)\n",
    "            )\n",
    "        )\n",
    "        # Logging the peak memory usage of the GPU to the tracker\n",
    "        if args.with_tracking:\n",
    "            accelerator.log(\n",
    "                {\n",
    "                    \"train_total_peak_memory\": tracemalloc.peaked + b2mb(tracemalloc.begin),\n",
    "                },\n",
    "                step=epoch,\n",
    "        )\n",
    "    torch.save(model, f'pruned_models/{config[\"model_name\"]}-{config[\"sparsity\"]}-finetuned.pt')\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--mixed_precision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        choices=[\"no\", \"fp16\", \"bf16\"],\n",
    "        help=\"Whether to use mixed precision. Choose\"\n",
    "        \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n",
    "        \"and an Nvidia Ampere GPU.\",\n",
    "    )\n",
    "    parser.add_argument(\"--cpu\", action=\"store_true\", help=\"If passed, will train on the CPU.\")\n",
    "    parser.add_argument(\n",
    "        \"--checkpointing_steps\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_from_checkpoint\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"If the training should continue from a checkpoint folder.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--with_tracking\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to load in all available experiment trackers from the environment and use them for logging.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\".\",\n",
    "        help=\"Optional save directory where all checkpoint folders will be stored. Default is the current working directory.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging_dir\",\n",
    "        type=str,\n",
    "        default=\"logs\",\n",
    "        help=\"Location on where to store experiment tracking logs`\",\n",
    "    )\n",
    "    #args = parser.parse_args()\n",
    "    args = parser.parse_args(['--mixed_precision', 'fp16'])\n",
    "    training_function(config, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbddb2d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistributedType.NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/cuda/memory.py:282: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/100 [00:04<08:05,  4.90s/it]\u001b[A\n",
      "  2%|▏         | 2/100 [00:05<03:31,  2.16s/it]\u001b[A\n",
      "  3%|▎         | 3/100 [00:05<02:08,  1.32s/it]\u001b[A\n",
      "  4%|▍         | 4/100 [00:05<01:29,  1.07it/s]\u001b[A\n",
      "  5%|▌         | 5/100 [00:06<01:08,  1.39it/s]\u001b[A\n",
      "  6%|▌         | 6/100 [00:06<00:55,  1.70it/s]\u001b[A\n",
      "  7%|▋         | 7/100 [00:06<00:46,  1.98it/s]\u001b[A\n",
      "  8%|▊         | 8/100 [00:07<00:41,  2.22it/s]\u001b[A\n",
      "  9%|▉         | 9/100 [00:07<00:37,  2.41it/s]\u001b[A\n",
      " 10%|█         | 10/100 [00:07<00:35,  2.56it/s]\u001b[A\n",
      " 11%|█         | 11/100 [00:08<00:33,  2.68it/s]\u001b[A\n",
      " 12%|█▏        | 12/100 [00:08<00:31,  2.77it/s]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:08<00:30,  2.83it/s]\u001b[A\n",
      " 14%|█▍        | 14/100 [00:09<00:29,  2.87it/s]\u001b[A\n",
      " 15%|█▌        | 15/100 [00:09<00:29,  2.91it/s]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:09<00:28,  2.93it/s]\u001b[A\n",
      " 17%|█▋        | 17/100 [00:10<00:33,  2.51it/s]\u001b[A\n",
      " 18%|█▊        | 18/100 [00:10<00:28,  2.87it/s]\u001b[A\n",
      " 19%|█▉        | 19/100 [00:10<00:27,  2.90it/s]\u001b[A\n",
      " 20%|██        | 20/100 [00:11<00:27,  2.92it/s]\u001b[A\n",
      " 21%|██        | 21/100 [00:11<00:26,  2.94it/s]\u001b[A\n",
      " 22%|██▏       | 22/100 [00:11<00:26,  2.95it/s]\u001b[A\n",
      " 23%|██▎       | 23/100 [00:12<00:26,  2.96it/s]\u001b[A\n",
      " 24%|██▍       | 24/100 [00:12<00:25,  2.96it/s]\u001b[A\n",
      " 25%|██▌       | 25/100 [00:12<00:25,  2.96it/s]\u001b[A\n",
      " 26%|██▌       | 26/100 [00:13<00:24,  2.97it/s]\u001b[A\n",
      " 27%|██▋       | 27/100 [00:13<00:24,  2.97it/s]\u001b[A\n",
      " 28%|██▊       | 28/100 [00:13<00:24,  2.97it/s]\u001b[A\n",
      " 29%|██▉       | 29/100 [00:14<00:23,  2.97it/s]\u001b[A\n",
      " 30%|███       | 30/100 [00:14<00:23,  2.97it/s]\u001b[A\n",
      " 31%|███       | 31/100 [00:14<00:23,  2.97it/s]\u001b[A\n",
      " 32%|███▏      | 32/100 [00:15<00:22,  2.97it/s]\u001b[A\n",
      " 33%|███▎      | 33/100 [00:15<00:26,  2.53it/s]\u001b[A\n",
      " 34%|███▍      | 34/100 [00:16<00:22,  2.89it/s]\u001b[A\n",
      " 35%|███▌      | 35/100 [00:16<00:22,  2.91it/s]\u001b[A\n",
      " 36%|███▌      | 36/100 [00:16<00:21,  2.93it/s]\u001b[A\n",
      " 37%|███▋      | 37/100 [00:17<00:21,  2.94it/s]\u001b[A\n",
      " 38%|███▊      | 38/100 [00:17<00:21,  2.95it/s]\u001b[A\n",
      " 39%|███▉      | 39/100 [00:17<00:20,  2.96it/s]\u001b[A\n",
      " 40%|████      | 40/100 [00:18<00:20,  2.96it/s]\u001b[A\n",
      " 41%|████      | 41/100 [00:18<00:19,  2.96it/s]\u001b[A\n",
      " 42%|████▏     | 42/100 [00:18<00:19,  2.97it/s]\u001b[A\n",
      " 43%|████▎     | 43/100 [00:19<00:19,  2.97it/s]\u001b[A\n",
      " 44%|████▍     | 44/100 [00:19<00:18,  2.97it/s]\u001b[A\n",
      " 45%|████▌     | 45/100 [00:19<00:18,  2.97it/s]\u001b[A\n",
      " 46%|████▌     | 46/100 [00:20<00:18,  2.97it/s]\u001b[A\n",
      " 47%|████▋     | 47/100 [00:20<00:17,  2.97it/s]\u001b[A\n",
      " 48%|████▊     | 48/100 [00:20<00:17,  2.97it/s]\u001b[A\n",
      " 49%|████▉     | 49/100 [00:21<00:20,  2.52it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:21<00:17,  2.88it/s]\u001b[A\n",
      " 51%|█████     | 51/100 [00:21<00:16,  2.91it/s]\u001b[A\n",
      " 52%|█████▏    | 52/100 [00:22<00:16,  2.92it/s]\u001b[A\n",
      " 53%|█████▎    | 53/100 [00:22<00:16,  2.93it/s]\u001b[A\n",
      " 54%|█████▍    | 54/100 [00:22<00:15,  2.94it/s]\u001b[A\n",
      " 55%|█████▌    | 55/100 [00:23<00:15,  2.95it/s]\u001b[A\n",
      " 56%|█████▌    | 56/100 [00:23<00:14,  2.95it/s]\u001b[A\n",
      " 57%|█████▋    | 57/100 [00:23<00:14,  2.95it/s]\u001b[A\n",
      " 58%|█████▊    | 58/100 [00:24<00:14,  2.96it/s]\u001b[A\n",
      " 59%|█████▉    | 59/100 [00:24<00:13,  2.96it/s]\u001b[A\n",
      " 60%|██████    | 60/100 [00:24<00:13,  2.96it/s]\u001b[A\n",
      " 61%|██████    | 61/100 [00:25<00:13,  2.96it/s]\u001b[A\n",
      " 62%|██████▏   | 62/100 [00:25<00:12,  2.96it/s]\u001b[A\n",
      " 63%|██████▎   | 63/100 [00:25<00:12,  2.96it/s]\u001b[A\n",
      " 64%|██████▍   | 64/100 [00:26<00:12,  2.96it/s]\u001b[A\n",
      " 65%|██████▌   | 65/100 [00:26<00:13,  2.52it/s]\u001b[A\n",
      " 66%|██████▌   | 66/100 [00:27<00:11,  2.87it/s]\u001b[A\n",
      " 67%|██████▋   | 67/100 [00:27<00:11,  2.90it/s]\u001b[A\n",
      " 68%|██████▊   | 68/100 [00:27<00:10,  2.92it/s]\u001b[A\n",
      " 69%|██████▉   | 69/100 [00:28<00:10,  2.93it/s]\u001b[A\n",
      " 70%|███████   | 70/100 [00:28<00:10,  2.94it/s]\u001b[A\n",
      " 71%|███████   | 71/100 [00:28<00:09,  2.94it/s]\u001b[A\n",
      " 72%|███████▏  | 72/100 [00:29<00:09,  2.95it/s]\u001b[A\n",
      " 73%|███████▎  | 73/100 [00:29<00:09,  2.95it/s]\u001b[A\n",
      " 74%|███████▍  | 74/100 [00:29<00:08,  2.95it/s]\u001b[A\n",
      " 75%|███████▌  | 75/100 [00:30<00:08,  2.95it/s]\u001b[A\n",
      " 76%|███████▌  | 76/100 [00:30<00:08,  2.96it/s]\u001b[A\n",
      " 77%|███████▋  | 77/100 [00:30<00:07,  2.96it/s]\u001b[A\n",
      " 78%|███████▊  | 78/100 [00:31<00:07,  2.95it/s]\u001b[A\n",
      " 79%|███████▉  | 79/100 [00:31<00:07,  2.95it/s]\u001b[A\n",
      " 80%|████████  | 80/100 [00:31<00:06,  2.95it/s]\u001b[A\n",
      " 81%|████████  | 81/100 [00:32<00:07,  2.51it/s]\u001b[A\n",
      " 82%|████████▏ | 82/100 [00:32<00:06,  2.87it/s]\u001b[A\n",
      " 83%|████████▎ | 83/100 [00:32<00:05,  2.89it/s]\u001b[A\n",
      " 84%|████████▍ | 84/100 [00:33<00:05,  2.91it/s]\u001b[A\n",
      " 85%|████████▌ | 85/100 [00:33<00:05,  2.92it/s]\u001b[A\n",
      " 86%|████████▌ | 86/100 [00:33<00:04,  2.93it/s]\u001b[A\n",
      " 87%|████████▋ | 87/100 [00:34<00:04,  2.94it/s]\u001b[A\n",
      " 88%|████████▊ | 88/100 [00:34<00:04,  2.94it/s]\u001b[A\n",
      " 89%|████████▉ | 89/100 [00:34<00:03,  2.94it/s]\u001b[A\n",
      " 90%|█████████ | 90/100 [00:35<00:03,  2.94it/s]\u001b[A\n",
      " 91%|█████████ | 91/100 [00:35<00:03,  2.94it/s]\u001b[A\n",
      " 92%|█████████▏| 92/100 [00:35<00:02,  2.94it/s]\u001b[A\n",
      " 93%|█████████▎| 93/100 [00:36<00:02,  2.94it/s]\u001b[A\n",
      " 94%|█████████▍| 94/100 [00:36<00:02,  2.94it/s]\u001b[A\n",
      " 95%|█████████▌| 95/100 [00:36<00:01,  2.94it/s]\u001b[A\n",
      " 96%|█████████▌| 96/100 [00:37<00:01,  2.94it/s]\u001b[A\n",
      " 97%|█████████▋| 97/100 [00:37<00:01,  2.51it/s]\u001b[A\n",
      " 98%|█████████▊| 98/100 [00:38<00:00,  2.86it/s]\u001b[A\n",
      " 99%|█████████▉| 99/100 [00:38<00:00,  2.89it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:38<00:00,  2.57it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:39<01:18, 39.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before entering the train : 6273\n",
      "Memory consumed at the end of the train (end-begin): 21232\n",
      "Peak Memory consumed during the train (max-begin): 29189\n",
      "Total Peak Memory consumed during the train (max): 35462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/100 [00:02<04:51,  2.94s/it]\u001b[A\n",
      "  2%|▏         | 2/100 [00:03<02:12,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 3/100 [00:03<01:26,  1.13it/s]\u001b[A\n",
      "  4%|▍         | 4/100 [00:03<01:04,  1.49it/s]\u001b[A\n",
      "  5%|▌         | 5/100 [00:04<00:52,  1.82it/s]\u001b[A\n",
      "  6%|▌         | 6/100 [00:04<00:45,  2.09it/s]\u001b[A\n",
      "  7%|▋         | 7/100 [00:04<00:40,  2.31it/s]\u001b[A\n",
      "  8%|▊         | 8/100 [00:05<00:37,  2.48it/s]\u001b[A\n",
      "  9%|▉         | 9/100 [00:05<00:34,  2.61it/s]\u001b[A\n",
      " 10%|█         | 10/100 [00:05<00:33,  2.70it/s]\u001b[A\n",
      " 11%|█         | 11/100 [00:06<00:32,  2.77it/s]\u001b[A\n",
      " 12%|█▏        | 12/100 [00:06<00:31,  2.82it/s]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:06<00:30,  2.86it/s]\u001b[A\n",
      " 14%|█▍        | 14/100 [00:07<00:29,  2.89it/s]\u001b[A\n",
      " 15%|█▌        | 15/100 [00:07<00:29,  2.90it/s]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:07<00:28,  2.91it/s]\u001b[A\n",
      " 17%|█▋        | 17/100 [00:08<00:33,  2.49it/s]\u001b[A\n",
      " 18%|█▊        | 18/100 [00:08<00:28,  2.85it/s]\u001b[A\n",
      " 19%|█▉        | 19/100 [00:09<00:28,  2.87it/s]\u001b[A\n",
      " 20%|██        | 20/100 [00:09<00:27,  2.89it/s]\u001b[A\n",
      " 21%|██        | 21/100 [00:09<00:27,  2.91it/s]\u001b[A\n",
      " 22%|██▏       | 22/100 [00:10<00:26,  2.92it/s]\u001b[A\n",
      " 23%|██▎       | 23/100 [00:10<00:26,  2.92it/s]\u001b[A\n",
      " 24%|██▍       | 24/100 [00:10<00:25,  2.93it/s]\u001b[A\n",
      " 25%|██▌       | 25/100 [00:11<00:25,  2.93it/s]\u001b[A\n",
      " 26%|██▌       | 26/100 [00:11<00:25,  2.93it/s]\u001b[A\n",
      " 27%|██▋       | 27/100 [00:11<00:24,  2.93it/s]\u001b[A\n",
      " 28%|██▊       | 28/100 [00:12<00:24,  2.93it/s]\u001b[A\n",
      " 29%|██▉       | 29/100 [00:12<00:24,  2.93it/s]\u001b[A\n",
      " 30%|███       | 30/100 [00:12<00:23,  2.94it/s]\u001b[A\n",
      " 31%|███       | 31/100 [00:13<00:23,  2.94it/s]\u001b[A\n",
      " 32%|███▏      | 32/100 [00:13<00:23,  2.94it/s]\u001b[A\n",
      " 33%|███▎      | 33/100 [00:14<00:26,  2.50it/s]\u001b[A\n",
      " 34%|███▍      | 34/100 [00:14<00:23,  2.86it/s]\u001b[A\n",
      " 35%|███▌      | 35/100 [00:14<00:22,  2.88it/s]\u001b[A\n",
      " 36%|███▌      | 36/100 [00:14<00:22,  2.90it/s]\u001b[A\n",
      " 37%|███▋      | 37/100 [00:15<00:21,  2.91it/s]\u001b[A\n",
      " 38%|███▊      | 38/100 [00:15<00:21,  2.92it/s]\u001b[A\n",
      " 39%|███▉      | 39/100 [00:15<00:20,  2.92it/s]\u001b[A\n",
      " 40%|████      | 40/100 [00:16<00:20,  2.93it/s]\u001b[A\n",
      " 41%|████      | 41/100 [00:16<00:20,  2.93it/s]\u001b[A\n",
      " 42%|████▏     | 42/100 [00:16<00:19,  2.93it/s]\u001b[A\n",
      " 43%|████▎     | 43/100 [00:17<00:19,  2.93it/s]\u001b[A\n",
      " 44%|████▍     | 44/100 [00:17<00:19,  2.93it/s]\u001b[A\n",
      " 45%|████▌     | 45/100 [00:17<00:18,  2.93it/s]\u001b[A\n",
      " 46%|████▌     | 46/100 [00:18<00:18,  2.93it/s]\u001b[A\n",
      " 47%|████▋     | 47/100 [00:18<00:18,  2.94it/s]\u001b[A\n",
      " 48%|████▊     | 48/100 [00:19<00:17,  2.94it/s]\u001b[A\n",
      " 49%|████▉     | 49/100 [00:19<00:20,  2.50it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:19<00:17,  2.85it/s]\u001b[A\n",
      " 51%|█████     | 51/100 [00:20<00:17,  2.87it/s]\u001b[A\n",
      " 52%|█████▏    | 52/100 [00:20<00:16,  2.89it/s]\u001b[A\n",
      " 53%|█████▎    | 53/100 [00:20<00:16,  2.90it/s]\u001b[A\n",
      " 54%|█████▍    | 54/100 [00:21<00:15,  2.91it/s]\u001b[A\n",
      " 55%|█████▌    | 55/100 [00:21<00:15,  2.92it/s]\u001b[A\n",
      " 56%|█████▌    | 56/100 [00:21<00:15,  2.92it/s]\u001b[A\n",
      " 57%|█████▋    | 57/100 [00:22<00:14,  2.92it/s]\u001b[A\n",
      " 58%|█████▊    | 58/100 [00:22<00:14,  2.92it/s]\u001b[A\n",
      " 59%|█████▉    | 59/100 [00:22<00:14,  2.92it/s]\u001b[A\n",
      " 60%|██████    | 60/100 [00:23<00:13,  2.92it/s]\u001b[A\n",
      " 61%|██████    | 61/100 [00:23<00:13,  2.92it/s]\u001b[A\n",
      " 62%|██████▏   | 62/100 [00:23<00:12,  2.93it/s]\u001b[A\n",
      " 63%|██████▎   | 63/100 [00:24<00:12,  2.93it/s]\u001b[A\n",
      " 64%|██████▍   | 64/100 [00:24<00:12,  2.92it/s]\u001b[A\n",
      " 65%|██████▌   | 65/100 [00:25<00:14,  2.49it/s]\u001b[A\n",
      " 66%|██████▌   | 66/100 [00:25<00:11,  2.84it/s]\u001b[A\n",
      " 67%|██████▋   | 67/100 [00:25<00:11,  2.87it/s]\u001b[A\n",
      " 68%|██████▊   | 68/100 [00:26<00:11,  2.89it/s]\u001b[A\n",
      " 69%|██████▉   | 69/100 [00:26<00:10,  2.90it/s]\u001b[A\n",
      " 70%|███████   | 70/100 [00:26<00:10,  2.91it/s]\u001b[A\n",
      " 71%|███████   | 71/100 [00:27<00:09,  2.91it/s]\u001b[A\n",
      " 72%|███████▏  | 72/100 [00:27<00:09,  2.92it/s]\u001b[A\n",
      " 73%|███████▎  | 73/100 [00:27<00:09,  2.92it/s]\u001b[A\n",
      " 74%|███████▍  | 74/100 [00:28<00:08,  2.92it/s]\u001b[A\n",
      " 75%|███████▌  | 75/100 [00:28<00:08,  2.92it/s]\u001b[A\n",
      " 76%|███████▌  | 76/100 [00:28<00:08,  2.93it/s]\u001b[A\n",
      " 77%|███████▋  | 77/100 [00:29<00:07,  2.93it/s]\u001b[A\n",
      " 78%|███████▊  | 78/100 [00:29<00:07,  2.92it/s]\u001b[A\n",
      " 79%|███████▉  | 79/100 [00:29<00:07,  2.92it/s]\u001b[A\n",
      " 80%|████████  | 80/100 [00:30<00:06,  2.92it/s]\u001b[A\n",
      " 81%|████████  | 81/100 [00:30<00:07,  2.49it/s]\u001b[A\n",
      " 82%|████████▏ | 82/100 [00:30<00:06,  2.84it/s]\u001b[A\n",
      " 83%|████████▎ | 83/100 [00:31<00:05,  2.87it/s]\u001b[A\n",
      " 84%|████████▍ | 84/100 [00:31<00:05,  2.88it/s]\u001b[A\n",
      " 85%|████████▌ | 85/100 [00:31<00:05,  2.89it/s]\u001b[A\n",
      " 86%|████████▌ | 86/100 [00:32<00:04,  2.90it/s]\u001b[A\n",
      " 87%|████████▋ | 87/100 [00:32<00:04,  2.91it/s]\u001b[A\n",
      " 88%|████████▊ | 88/100 [00:32<00:04,  2.92it/s]\u001b[A\n",
      " 89%|████████▉ | 89/100 [00:33<00:03,  2.92it/s]\u001b[A\n",
      " 90%|█████████ | 90/100 [00:33<00:03,  2.92it/s]\u001b[A\n",
      " 91%|█████████ | 91/100 [00:33<00:03,  2.92it/s]\u001b[A\n",
      " 92%|█████████▏| 92/100 [00:34<00:02,  2.92it/s]\u001b[A\n",
      " 93%|█████████▎| 93/100 [00:34<00:02,  2.92it/s]\u001b[A\n",
      " 94%|█████████▍| 94/100 [00:35<00:02,  2.92it/s]\u001b[A\n",
      " 95%|█████████▌| 95/100 [00:35<00:01,  2.92it/s]\u001b[A\n",
      " 96%|█████████▌| 96/100 [00:35<00:01,  2.92it/s]\u001b[A\n",
      " 97%|█████████▋| 97/100 [00:36<00:01,  2.49it/s]\u001b[A\n",
      " 98%|█████████▊| 98/100 [00:36<00:00,  2.84it/s]\u001b[A\n",
      " 99%|█████████▉| 99/100 [00:36<00:00,  2.87it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:37<00:00,  2.68it/s]\u001b[A\n",
      " 67%|██████▋   | 2/3 [01:16<00:38, 38.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before entering the train : 27506\n",
      "Memory consumed at the end of the train (end-begin): 0\n",
      "Peak Memory consumed during the train (max-begin): 7957\n",
      "Total Peak Memory consumed during the train (max): 35463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/100 [00:02<04:53,  2.97s/it]\u001b[A\n",
      "  2%|▏         | 2/100 [00:03<02:13,  1.36s/it]\u001b[A\n",
      "  3%|▎         | 3/100 [00:03<01:26,  1.12it/s]\u001b[A\n",
      "  4%|▍         | 4/100 [00:03<01:04,  1.48it/s]\u001b[A\n",
      "  5%|▌         | 5/100 [00:04<00:52,  1.80it/s]\u001b[A\n",
      "  6%|▌         | 6/100 [00:04<00:45,  2.07it/s]\u001b[A\n",
      "  7%|▋         | 7/100 [00:04<00:40,  2.29it/s]\u001b[A\n",
      "  8%|▊         | 8/100 [00:05<00:37,  2.46it/s]\u001b[A\n",
      "  9%|▉         | 9/100 [00:05<00:35,  2.59it/s]\u001b[A\n",
      " 10%|█         | 10/100 [00:05<00:33,  2.69it/s]\u001b[A\n",
      " 11%|█         | 11/100 [00:06<00:32,  2.76it/s]\u001b[A\n",
      " 12%|█▏        | 12/100 [00:06<00:31,  2.81it/s]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:06<00:30,  2.84it/s]\u001b[A\n",
      " 14%|█▍        | 14/100 [00:07<00:29,  2.87it/s]\u001b[A\n",
      " 15%|█▌        | 15/100 [00:07<00:29,  2.89it/s]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:07<00:28,  2.90it/s]\u001b[A\n",
      " 17%|█▋        | 17/100 [00:08<00:33,  2.48it/s]\u001b[A\n",
      " 18%|█▊        | 18/100 [00:08<00:28,  2.83it/s]\u001b[A\n",
      " 19%|█▉        | 19/100 [00:09<00:28,  2.86it/s]\u001b[A\n",
      " 20%|██        | 20/100 [00:09<00:27,  2.88it/s]\u001b[A\n",
      " 21%|██        | 21/100 [00:09<00:27,  2.90it/s]\u001b[A\n",
      " 22%|██▏       | 22/100 [00:10<00:26,  2.91it/s]\u001b[A\n",
      " 23%|██▎       | 23/100 [00:10<00:26,  2.91it/s]\u001b[A\n",
      " 24%|██▍       | 24/100 [00:10<00:26,  2.92it/s]\u001b[A\n",
      " 25%|██▌       | 25/100 [00:11<00:25,  2.92it/s]\u001b[A\n",
      " 26%|██▌       | 26/100 [00:11<00:25,  2.92it/s]\u001b[A\n",
      " 27%|██▋       | 27/100 [00:11<00:25,  2.92it/s]\u001b[A\n",
      " 28%|██▊       | 28/100 [00:12<00:24,  2.92it/s]\u001b[A\n",
      " 29%|██▉       | 29/100 [00:12<00:24,  2.92it/s]\u001b[A\n",
      " 30%|███       | 30/100 [00:12<00:23,  2.92it/s]\u001b[A\n",
      " 31%|███       | 31/100 [00:13<00:23,  2.93it/s]\u001b[A\n",
      " 32%|███▏      | 32/100 [00:13<00:23,  2.92it/s]\u001b[A\n",
      " 33%|███▎      | 33/100 [00:14<00:26,  2.49it/s]\u001b[A\n",
      " 34%|███▍      | 34/100 [00:14<00:23,  2.85it/s]\u001b[A\n",
      " 35%|███▌      | 35/100 [00:14<00:22,  2.87it/s]\u001b[A\n",
      " 36%|███▌      | 36/100 [00:14<00:22,  2.89it/s]\u001b[A\n",
      " 37%|███▋      | 37/100 [00:15<00:21,  2.90it/s]\u001b[A\n",
      " 38%|███▊      | 38/100 [00:15<00:21,  2.91it/s]\u001b[A\n",
      " 39%|███▉      | 39/100 [00:16<00:20,  2.91it/s]\u001b[A\n",
      " 40%|████      | 40/100 [00:16<00:20,  2.91it/s]\u001b[A\n",
      " 41%|████      | 41/100 [00:16<00:20,  2.92it/s]\u001b[A\n",
      " 42%|████▏     | 42/100 [00:17<00:19,  2.92it/s]\u001b[A\n",
      " 43%|████▎     | 43/100 [00:17<00:19,  2.93it/s]\u001b[A\n",
      " 44%|████▍     | 44/100 [00:17<00:19,  2.92it/s]\u001b[A\n",
      " 45%|████▌     | 45/100 [00:18<00:18,  2.93it/s]\u001b[A\n",
      " 46%|████▌     | 46/100 [00:18<00:18,  2.93it/s]\u001b[A\n",
      " 47%|████▋     | 47/100 [00:18<00:18,  2.93it/s]\u001b[A\n",
      " 48%|████▊     | 48/100 [00:19<00:17,  2.93it/s]\u001b[A\n",
      " 49%|████▉     | 49/100 [00:19<00:20,  2.49it/s]\u001b[A\n",
      " 50%|█████     | 50/100 [00:19<00:17,  2.85it/s]\u001b[A\n",
      " 51%|█████     | 51/100 [00:20<00:17,  2.87it/s]\u001b[A\n",
      " 52%|█████▏    | 52/100 [00:20<00:16,  2.88it/s]\u001b[A\n",
      " 53%|█████▎    | 53/100 [00:20<00:16,  2.89it/s]\u001b[A\n",
      " 54%|█████▍    | 54/100 [00:21<00:15,  2.90it/s]\u001b[A\n",
      " 55%|█████▌    | 55/100 [00:21<00:15,  2.91it/s]\u001b[A\n",
      " 56%|█████▌    | 56/100 [00:21<00:15,  2.92it/s]\u001b[A\n",
      " 57%|█████▋    | 57/100 [00:22<00:14,  2.92it/s]\u001b[A\n",
      " 58%|█████▊    | 58/100 [00:22<00:14,  2.92it/s]\u001b[A\n",
      " 59%|█████▉    | 59/100 [00:22<00:14,  2.92it/s]\u001b[A\n",
      " 60%|██████    | 60/100 [00:23<00:13,  2.92it/s]\u001b[A\n",
      " 61%|██████    | 61/100 [00:23<00:13,  2.92it/s]\u001b[A\n",
      " 62%|██████▏   | 62/100 [00:23<00:13,  2.92it/s]\u001b[A\n",
      " 63%|██████▎   | 63/100 [00:24<00:12,  2.92it/s]\u001b[A\n",
      " 64%|██████▍   | 64/100 [00:24<00:12,  2.92it/s]\u001b[A\n",
      " 65%|██████▌   | 65/100 [00:25<00:14,  2.49it/s]\u001b[A\n",
      " 66%|██████▌   | 66/100 [00:25<00:11,  2.84it/s]\u001b[A\n",
      " 67%|██████▋   | 67/100 [00:25<00:11,  2.87it/s]\u001b[A\n",
      " 68%|██████▊   | 68/100 [00:26<00:11,  2.88it/s]\u001b[A\n",
      " 69%|██████▉   | 69/100 [00:26<00:10,  2.89it/s]\u001b[A\n",
      " 70%|███████   | 70/100 [00:26<00:10,  2.90it/s]\u001b[A\n",
      " 71%|███████   | 71/100 [00:27<00:09,  2.90it/s]\u001b[A\n",
      " 72%|███████▏  | 72/100 [00:27<00:09,  2.91it/s]\u001b[A\n",
      " 73%|███████▎  | 73/100 [00:27<00:09,  2.92it/s]\u001b[A\n",
      " 74%|███████▍  | 74/100 [00:28<00:08,  2.92it/s]\u001b[A\n",
      " 75%|███████▌  | 75/100 [00:28<00:08,  2.92it/s]\u001b[A\n",
      " 76%|███████▌  | 76/100 [00:28<00:08,  2.93it/s]\u001b[A\n",
      " 77%|███████▋  | 77/100 [00:29<00:07,  2.93it/s]\u001b[A\n",
      " 78%|███████▊  | 78/100 [00:29<00:07,  2.92it/s]\u001b[A\n",
      " 79%|███████▉  | 79/100 [00:29<00:07,  2.92it/s]\u001b[A\n",
      " 80%|████████  | 80/100 [00:30<00:06,  2.92it/s]\u001b[A\n",
      " 81%|████████  | 81/100 [00:30<00:07,  2.49it/s]\u001b[A\n",
      " 82%|████████▏ | 82/100 [00:31<00:06,  2.84it/s]\u001b[A\n",
      " 83%|████████▎ | 83/100 [00:31<00:05,  2.87it/s]\u001b[A\n",
      " 84%|████████▍ | 84/100 [00:31<00:05,  2.88it/s]\u001b[A\n",
      " 85%|████████▌ | 85/100 [00:32<00:05,  2.89it/s]\u001b[A\n",
      " 86%|████████▌ | 86/100 [00:32<00:04,  2.90it/s]\u001b[A\n",
      " 87%|████████▋ | 87/100 [00:32<00:04,  2.91it/s]\u001b[A\n",
      " 88%|████████▊ | 88/100 [00:33<00:04,  2.91it/s]\u001b[A\n",
      " 89%|████████▉ | 89/100 [00:33<00:03,  2.91it/s]\u001b[A\n",
      " 90%|█████████ | 90/100 [00:33<00:03,  2.91it/s]\u001b[A\n",
      " 91%|█████████ | 91/100 [00:34<00:03,  2.91it/s]\u001b[A\n",
      " 92%|█████████▏| 92/100 [00:34<00:02,  2.92it/s]\u001b[A\n",
      " 93%|█████████▎| 93/100 [00:34<00:02,  2.92it/s]\u001b[A\n",
      " 94%|█████████▍| 94/100 [00:35<00:02,  2.92it/s]\u001b[A\n",
      " 95%|█████████▌| 95/100 [00:35<00:01,  2.92it/s]\u001b[A\n",
      " 96%|█████████▌| 96/100 [00:35<00:01,  2.92it/s]\u001b[A\n",
      " 97%|█████████▋| 97/100 [00:36<00:01,  2.49it/s]\u001b[A\n",
      " 98%|█████████▊| 98/100 [00:36<00:00,  2.85it/s]\u001b[A\n",
      " 99%|█████████▉| 99/100 [00:36<00:00,  2.87it/s]\u001b[A\n",
      "100%|██████████| 100/100 [00:37<00:00,  2.67it/s]\u001b[A\n",
      "100%|██████████| 3/3 [01:54<00:00, 38.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory before entering the train : 27506\n",
      "Memory consumed at the end of the train (end-begin): 0\n",
      "Peak Memory consumed during the train (max-begin): 7957\n",
      "Total Peak Memory consumed during the train (max): 35463\n"
     ]
    }
   ],
   "source": [
    "config = {\"lr\": 2e-5, \"num_epochs\": 3,\n",
    "              \"seed\": 1, \"batch_size\": 16,\n",
    "              'model_name': 'opt-1.3b',\n",
    "              'sparsity': 1,\"train_steps\": 5,\n",
    "              'max_step': 100}\n",
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c17996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

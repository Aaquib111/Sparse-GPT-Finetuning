{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e524b42c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5cfe088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils.mask_utils import calculate_mask\n",
    "from utils.hessian_utils import calc_inverse_hessian\n",
    "from utils.prehook_utils import put_input_hooks\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9d694",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ebc486",
   "metadata": {},
   "outputs": [],
   "source": [
    "CALIBRATION_SIZE=128\n",
    "TOKEN_LENGTH=1024\n",
    "CALIBRATION_BATCH_SIZE=1\n",
    "\n",
    "EPSILON = 1e-8\n",
    "B = 128 \n",
    "Bs = 128\n",
    "\n",
    "#hyperparam test, remove later\n",
    "EPOCH_COUNT = 10\n",
    "\n",
    "#set device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "MODEL_NAME = \"opt-125m\"\n",
    "\n",
    "#load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'facebook/{MODEL_NAME}')\n",
    "#Load dataset\n",
    "dataset = load_dataset('c4', 'en', streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d03ec",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c1056",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3216a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model on batches of calibration data, then concatenate inputs\n",
    "def split_model_calibration(model):\n",
    "    batch_sentences = []\n",
    "    for i, data in tqdm(enumerate(iter(dataset['train'])), total=CALIBRATION_SIZE):\n",
    "        if i < CALIBRATION_SIZE + 1:\n",
    "            if len(batch_sentences) >= CALIBRATION_BATCH_SIZE:\n",
    "                with torch.no_grad():\n",
    "                    encoded_input = tokenizer(batch_sentences, return_tensors=\"pt\",\n",
    "                                              padding=\"max_length\", max_length=TOKEN_LENGTH,\n",
    "                                              truncation=True).to(device=DEVICE)\n",
    "                    model(**encoded_input, labels=encoded_input.input_ids)\n",
    "                    del encoded_input\n",
    "                    torch.cuda.empty_cache()\n",
    "                    batch_sentences = []\n",
    "            batch_sentences.append(data['text'])\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf95af6",
   "metadata": {},
   "source": [
    "# SparseGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8919231",
   "metadata": {},
   "source": [
    "## Prune models (SparseGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fef33d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c538b5dc5947138841ea64d7caee16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 25 13:12:49 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    60W / 250W |  26355MiB / 40960MiB |     43%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCI...  On   | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    36W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCI...  On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    34W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PCI...  On   | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    33W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1806      C   ...l8/apps/conda3/bin/python    26353MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/128 [00:00<00:30,  4.17it/s]\u001b[A/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "  2%|▏         | 2/128 [00:16<20:44,  9.88s/it]\u001b[A\n",
      "  2%|▏         | 3/128 [00:39<32:38, 15.67s/it]\u001b[A\n",
      "  3%|▎         | 4/128 [01:02<38:03, 18.41s/it]\u001b[A\n",
      "  4%|▍         | 5/128 [01:24<40:51, 19.93s/it]\u001b[A\n",
      "  5%|▍         | 6/128 [01:47<42:25, 20.86s/it]\u001b[A\n",
      "  5%|▌         | 7/128 [02:10<43:15, 21.45s/it]\u001b[A\n",
      "  6%|▋         | 8/128 [02:32<43:39, 21.83s/it]\u001b[A\n",
      "  7%|▋         | 9/128 [02:55<43:50, 22.10s/it]\u001b[A\n",
      "  8%|▊         | 10/128 [03:18<43:48, 22.27s/it]\u001b[A\n",
      "  9%|▊         | 11/128 [03:40<43:40, 22.40s/it]\u001b[A\n",
      "  9%|▉         | 12/128 [04:03<43:28, 22.49s/it]\u001b[A\n",
      " 10%|█         | 13/128 [04:26<43:11, 22.54s/it]\u001b[A\n",
      " 11%|█         | 14/128 [04:48<42:53, 22.58s/it]\u001b[A\n",
      " 12%|█▏        | 15/128 [05:11<42:34, 22.61s/it]\u001b[A\n",
      " 12%|█▎        | 16/128 [05:34<42:13, 22.62s/it]\u001b[A\n",
      " 13%|█▎        | 17/128 [05:56<41:53, 22.65s/it]\u001b[A\n",
      " 14%|█▍        | 18/128 [06:19<41:32, 22.66s/it]\u001b[A\n",
      " 15%|█▍        | 19/128 [06:42<41:10, 22.66s/it]\u001b[A\n",
      " 16%|█▌        | 20/128 [07:04<40:48, 22.67s/it]\u001b[A\n",
      " 16%|█▋        | 21/128 [07:27<40:25, 22.67s/it]\u001b[A\n",
      " 17%|█▋        | 22/128 [07:50<40:03, 22.67s/it]\u001b[A\n",
      " 18%|█▊        | 23/128 [08:12<39:41, 22.68s/it]\u001b[A\n",
      " 19%|█▉        | 24/128 [08:35<39:19, 22.68s/it]\u001b[A\n",
      " 20%|█▉        | 25/128 [08:58<38:56, 22.68s/it]\u001b[A\n",
      " 20%|██        | 26/128 [09:20<38:34, 22.69s/it]\u001b[A\n",
      " 21%|██        | 27/128 [09:43<38:11, 22.69s/it]\u001b[A\n",
      " 22%|██▏       | 28/128 [10:06<37:49, 22.69s/it]\u001b[A\n",
      " 23%|██▎       | 29/128 [10:28<37:26, 22.69s/it]\u001b[A\n",
      " 23%|██▎       | 30/128 [10:51<37:03, 22.69s/it]\u001b[A\n",
      " 24%|██▍       | 31/128 [11:14<36:41, 22.69s/it]\u001b[A\n",
      " 25%|██▌       | 32/128 [11:37<36:17, 22.69s/it]\u001b[A\n",
      " 26%|██▌       | 33/128 [11:59<35:55, 22.69s/it]\u001b[A\n",
      " 27%|██▋       | 34/128 [12:22<35:32, 22.68s/it]\u001b[A\n",
      " 27%|██▋       | 35/128 [12:45<35:09, 22.68s/it]\u001b[A\n",
      " 28%|██▊       | 36/128 [13:07<34:47, 22.69s/it]\u001b[A\n",
      " 29%|██▉       | 37/128 [13:30<34:24, 22.69s/it]\u001b[A\n",
      " 30%|██▉       | 38/128 [13:53<34:02, 22.69s/it]\u001b[A\n",
      " 30%|███       | 39/128 [14:15<33:39, 22.69s/it]\u001b[A\n",
      " 31%|███▏      | 40/128 [14:38<33:22, 22.76s/it]\u001b[A\n",
      " 32%|███▏      | 41/128 [15:01<33:11, 22.89s/it]\u001b[A\n",
      " 33%|███▎      | 42/128 [15:24<32:43, 22.83s/it]\u001b[A\n",
      " 34%|███▎      | 43/128 [15:47<32:16, 22.79s/it]\u001b[A\n",
      " 34%|███▍      | 44/128 [16:10<31:51, 22.76s/it]\u001b[A\n",
      " 35%|███▌      | 45/128 [16:32<31:26, 22.73s/it]\u001b[A\n",
      " 36%|███▌      | 46/128 [16:55<31:03, 22.73s/it]\u001b[A\n",
      " 37%|███▋      | 47/128 [17:18<30:39, 22.71s/it]\u001b[A\n",
      " 38%|███▊      | 48/128 [17:40<30:16, 22.71s/it]\u001b[A\n",
      " 38%|███▊      | 49/128 [18:03<29:53, 22.71s/it]\u001b[A\n",
      " 39%|███▉      | 50/128 [18:26<29:30, 22.70s/it]\u001b[A\n",
      " 40%|███▉      | 51/128 [18:48<29:07, 22.69s/it]\u001b[A\n",
      " 41%|████      | 52/128 [19:11<28:44, 22.70s/it]\u001b[A\n",
      " 41%|████▏     | 53/128 [19:34<28:21, 22.69s/it]\u001b[A\n",
      " 42%|████▏     | 54/128 [19:56<27:59, 22.69s/it]\u001b[A\n",
      " 43%|████▎     | 55/128 [20:19<27:36, 22.69s/it]\u001b[A\n",
      " 44%|████▍     | 56/128 [20:42<27:13, 22.69s/it]\u001b[A\n",
      " 45%|████▍     | 57/128 [21:05<26:51, 22.69s/it]\u001b[A\n",
      " 45%|████▌     | 58/128 [21:27<26:28, 22.69s/it]\u001b[A\n",
      " 46%|████▌     | 59/128 [21:50<26:05, 22.68s/it]\u001b[A\n",
      " 47%|████▋     | 60/128 [22:13<25:42, 22.69s/it]\u001b[A\n",
      " 48%|████▊     | 61/128 [22:35<25:19, 22.68s/it]\u001b[A\n",
      " 48%|████▊     | 62/128 [22:58<24:57, 22.68s/it]\u001b[A\n",
      " 49%|████▉     | 63/128 [23:21<24:47, 22.89s/it]\u001b[A\n",
      " 50%|█████     | 64/128 [23:44<24:20, 22.83s/it]\u001b[A\n",
      " 51%|█████     | 65/128 [24:07<23:55, 22.79s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "from utils.prune_utils import sparsegpt_prune\n",
    "\n",
    "SPARSITIES = [0.2,0.3,0.5,0.7,0.9,1]#0.1, 0.2,0.3,0.5,0.7,0.9,1\n",
    "    \n",
    "for i, SPARSITY in enumerate(tqdm(SPARSITIES, total=len(SPARSITIES))):\n",
    "    model = OPTForCausalLM.from_pretrained(f'facebook/{MODEL_NAME}', \n",
    "                                       output_attentions=True, \n",
    "                                       output_hidden_states=True).to(device=DEVICE)\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "    !nvidia-smi\n",
    "    # Calculate feature hessians only for the first iteration\n",
    "    if i == 0:\n",
    "        feature_hessians = {}\n",
    "        #put_input_hooks(model=model, features=feature_hessians, storage_dir=storage_dir, offload_freq=10000, feature_storage_device='cpu')\n",
    "        all_hooks = put_input_hooks(model=model, features=feature_hessians, feature_storage_device='cpu')\n",
    "        split_model_calibration(model)\n",
    "        for hook in all_hooks:\n",
    "            hook.remove()\n",
    "    \n",
    "    # Prune using the sparseGPT method, saves as pruned_models/{model_name}-{SPARSENESS}.pt WITHOUT mask\n",
    "    sparsegpt_prune(model, MODEL_NAME, feature_hessians, EPSILON, SPARSITY, B, Bs)\n",
    "    !nvidia-smi\n",
    "    del model\n",
    "del feature_hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c94384d",
   "metadata": {},
   "source": [
    "## Finetune models (SparseGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d74a9f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.16.0-py3-none-any.whl (199 kB)\n",
      "\u001b[K     |████████████████████████████████| 199 kB 28.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /gs/gsfs0/hpc01/rhel8/apps/conda3/lib/python3.9/site-packages (from accelerate) (1.20.3)\n",
      "Requirement already satisfied: torch>=1.4.0 in /gs/gsfs0/home/asyed/.local/lib/python3.9/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: psutil in /gs/gsfs0/hpc01/rhel8/apps/conda3/lib/python3.9/site-packages (from accelerate) (5.9.1)\n",
      "Requirement already satisfied: pyyaml in /gs/gsfs0/hpc01/rhel8/apps/conda3/lib/python3.9/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /gs/gsfs0/hpc01/rhel8/apps/conda3/lib/python3.9/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /gs/gsfs0/hpc01/rhel8/apps/conda3/lib/python3.9/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions in /gs/gsfs0/hpc01/rhel8/apps/conda3/lib/python3.9/site-packages (from torch>=1.4.0->accelerate) (4.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /gs/gsfs0/home/asyed/.local/lib/python3.9/site-packages (from torch>=1.4.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /gs/gsfs0/home/asyed/.local/lib/python3.9/site-packages (from torch>=1.4.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /gs/gsfs0/home/asyed/.local/lib/python3.9/site-packages (from torch>=1.4.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /gs/gsfs0/home/asyed/.local/lib/python3.9/site-packages (from torch>=1.4.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: wheel in /gs/gsfs0/hpc01/rhel8/apps/conda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /gs/gsfs0/home/asyed/.local/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (56.0.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.16.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/gs/gsfs0/hpc01/rhel8/apps/conda3/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a30c4a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[AAsking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  0%|          | 0/10 [00:03<?, ?it/s]\n",
      "  0%|          | 0/6 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_28356/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">505065541.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 4&gt;</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_28356/505065541.py'</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/gs/gsfs0/home/asyed/pw/jobs/ICLR/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">finetune_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">53</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">finetune_model</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">50 │   │   │   </span>outputs = loaded_model(**batch)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">51 │   │   │   </span>loss = outputs[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">52 │   │   │   </span>loss.backward()                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>53 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>t_optim.step()                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">54 │   │   │   </span>t_optim.zero_grad()                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55 │   </span>unmask_model(loaded_model)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">56 │   </span>torch.save(loaded_model.state_dict(), <span style=\"color: #808000; text-decoration-color: #808000\">f'pruned_models/{</span>model_name<span style=\"color: #808000; text-decoration-color: #808000\">}-{</span>SPARSITY<span style=\"color: #808000; text-decoration-color: #808000\">}-fi</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">optimizer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">140</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">137 │   │   │   │   </span>obj, *_ = args                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">138 │   │   │   │   </span>profile_name = <span style=\"color: #808000; text-decoration-color: #808000\">\"Optimizer.step#{}.step\"</span>.format(obj.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__class__</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">139 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.autograd.profiler.record_function(profile_name):         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>140 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>out = func(*args, **kwargs)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">141 │   │   │   │   │   </span>obj._optimizer_step_code()                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">142 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> out                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">143 </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_mode.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">27</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 24 │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(func)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 25 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>(*args, **kwargs):                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 26 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.clone():                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 27 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 28 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> cast(F, decorate_context)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 29 │   </span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 30 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_wrap_generator</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, func):                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">adamw.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">149</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">146 │   │   │   │   │   # Exponential moving average of gradient values</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">147 │   │   │   │   │   </span>state[<span style=\"color: #808000; text-decoration-color: #808000\">'exp_avg'</span>] = torch.zeros_like(p, memory_format=torch.pres <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">148 │   │   │   │   │   # Exponential moving average of squared gradient values</span>         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>149 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>state[<span style=\"color: #808000; text-decoration-color: #808000\">'exp_avg_sq'</span>] = torch.zeros_like(p, memory_format=torch.p <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">150 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> amsgrad:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">151 │   │   │   │   │   │   # Maintains max of all exp. moving avg. of sq. grad. values</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">152 │   │   │   │   │   │   </span>state[<span style=\"color: #808000; text-decoration-color: #808000\">'max_exp_avg_sq'</span>] = torch.zeros_like(p, memory_format <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39.41</span> GiB total \n",
       "capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37.89</span> GiB already allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.50</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">38.10</span> GiB reserved in total by \n",
       "PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid \n",
       "fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_28356/\u001b[0m\u001b[1;33m505065541.py\u001b[0m:\u001b[94m6\u001b[0m in \u001b[92m<cell line: 4>\u001b[0m                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_28356/505065541.py'\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/gs/gsfs0/home/asyed/pw/jobs/ICLR/utils/\u001b[0m\u001b[1;33mfinetune_utils.py\u001b[0m:\u001b[94m53\u001b[0m in \u001b[92mfinetune_model\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m50 \u001b[0m\u001b[2m│   │   │   \u001b[0moutputs = loaded_model(**batch)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m51 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = outputs[\u001b[94m0\u001b[0m]                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m52 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss.backward()                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m53 \u001b[2m│   │   │   \u001b[0mt_optim.step()                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m54 \u001b[0m\u001b[2m│   │   │   \u001b[0mt_optim.zero_grad()                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m55 \u001b[0m\u001b[2m│   \u001b[0munmask_model(loaded_model)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m56 \u001b[0m\u001b[2m│   \u001b[0mtorch.save(loaded_model.state_dict(), \u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33mpruned_models/\u001b[0m\u001b[33m{\u001b[0mmodel_name\u001b[33m}\u001b[0m\u001b[33m-\u001b[0m\u001b[33m{\u001b[0mSPARSITY\u001b[33m}\u001b[0m\u001b[33m-fi\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/optim/\u001b[0m\u001b[1;33moptimizer.py\u001b[0m:\u001b[94m140\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mwrapper\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mobj, *_ = args                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m138 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mprofile_name = \u001b[33m\"\u001b[0m\u001b[33mOptimizer.step#\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m.step\u001b[0m\u001b[33m\"\u001b[0m.format(obj.\u001b[91m__class__\u001b[0m.\u001b[91m__name\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.autograd.profiler.record_function(profile_name):         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m140 \u001b[2m│   │   │   │   │   \u001b[0mout = func(*args, **kwargs)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m141 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mobj._optimizer_step_code()                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m142 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m out                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m143 \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/autograd/\u001b[0m\u001b[1;33mgrad_mode.py\u001b[0m:\u001b[94m27\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mdecorate_context\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 24 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 25 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 26 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.clone():                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 27 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 28 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m cast(F, decorate_context)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 29 \u001b[0m\u001b[2m│   \u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 30 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_wrap_generator\u001b[0m(\u001b[96mself\u001b[0m, func):                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/optim/\u001b[0m\u001b[1;33madamw.py\u001b[0m:\u001b[94m149\u001b[0m in \u001b[92mstep\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m146 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# Exponential moving average of gradient values\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m147 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mstate[\u001b[33m'\u001b[0m\u001b[33mexp_avg\u001b[0m\u001b[33m'\u001b[0m] = torch.zeros_like(p, memory_format=torch.pres \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m148 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# Exponential moving average of squared gradient values\u001b[0m         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m149 \u001b[2m│   │   │   │   │   \u001b[0mstate[\u001b[33m'\u001b[0m\u001b[33mexp_avg_sq\u001b[0m\u001b[33m'\u001b[0m] = torch.zeros_like(p, memory_format=torch.p \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m150 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m amsgrad:                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[2m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mstate[\u001b[33m'\u001b[0m\u001b[33mmax_exp_avg_sq\u001b[0m\u001b[33m'\u001b[0m] = torch.zeros_like(p, memory_format \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m16.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m39.41\u001b[0m GiB total \n",
       "capacity; \u001b[1;36m37.89\u001b[0m GiB already allocated; \u001b[1;36m14.50\u001b[0m MiB free; \u001b[1;36m38.10\u001b[0m GiB reserved in total by \n",
       "PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory try setting max_split_size_mb to avoid \n",
       "fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.finetune_utils import finetune_model\n",
    "SPARSITIES = [0.2,0.3,0.5,0.7,0.9,1]#0.1, 0.2,0.3,0.5,0.7,0.9,1\n",
    "    \n",
    "for i, SPARSITY in enumerate(tqdm(SPARSITIES, total=len(SPARSITIES))):\n",
    "    # Finetune, saves as pruned_models/{model_name}-{SPARSENESS}-finetuned.pt WITHOUT mask\n",
    "    finetune_model(MODEL_NAME, tokenizer, SPARSITY, EPOCH_COUNT=EPOCH_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f6594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 26 03:34:38 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    59W / 250W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCI...  On   | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    39W / 250W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCI...  On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    34W / 250W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PCI...  On   | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   26C    P0    33W / 250W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a32b3c",
   "metadata": {},
   "source": [
    "## Test models (SparseGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6465a996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maaquib111\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edeec343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gs/gsfs0/home/asyed/pw/jobs/ICLR/wandb/run-20230225_181742-28wl6j59</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aaquib111/ICLR/runs/28wl6j59\" target=\"_blank\">opt-1.3b Wikitext Test</a></strong> to <a href=\"https://wandb.ai/aaquib111/ICLR\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/gs/gsfs0/users/asyed/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "  0%|          | 0/562 [00:00<?, ?it/s]/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|█████████▉| 560/562 [02:12<00:00,  4.21it/s]\n",
      "  3%|▎         | 18/562 [00:04<02:11,  4.13it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from testing_module import test_model\n",
    "\n",
    "TOKEN_LENGTH=1024\n",
    "STRIDE = 512\n",
    "wandb.init(project=\"ICLR\", \n",
    "           name = f'{MODEL_NAME} Wikitext Test', \n",
    "           config={'token_length': TOKEN_LENGTH,\n",
    "                 'model_name': MODEL_NAME,\n",
    "                 'stride': STRIDE,\n",
    "                 'fine_tuned': 'not finetuned'})\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'facebook/{MODEL_NAME}', \n",
    "                                          padding_side='left', \n",
    "                                          use_fast=False)\n",
    "# Load dataset\n",
    "test_set = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "encodings = tokenizer(\"\\n\\n\".join(test_set['text']), return_tensors='pt')\n",
    "\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "SPARSITIES = [0.2, 0.3, 0.5, 0.7, 0.9, 1]#, 0.4, 0.6, 0.8, 1\n",
    "\n",
    "for SPARSITY in SPARSITIES:\n",
    "    test_model(MODEL_NAME, encodings, TOKEN_LENGTH, seq_len, STRIDE, wandb, SPARSITY, is_finetuned=False)\n",
    "    \n",
    "### NOW DO FINETUNED\n",
    "wandb.init(project=\"ICLR\", \n",
    "           name = f'{MODEL_NAME} Wikitext Test', \n",
    "           config={'token_length': token_length,\n",
    "                 'model_name': MODEL_NAME,\n",
    "                 'stride': stride,\n",
    "                 'fine_tuned': 'finetuned'})\n",
    "for SPARSITY in SPARSITIES:\n",
    "    test_model(MODEL_NAME, encodings, TOKEN_LENGTH, seq_len, STRIDE, wandb, SPARSITY, is_finetuned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d72e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3lgu8ffn) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f59bd581324110bf7c1219b1fbfae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">opt-1.3b Wikitext Test</strong>: <a href=\"https://wandb.ai/aaquib111/ICLR/runs/3lgu8ffn\" target=\"_blank\">https://wandb.ai/aaquib111/ICLR/runs/3lgu8ffn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230226_230058-3lgu8ffn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3lgu8ffn). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f972d555b104b30b5d04b527ac8e64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668259616320333, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/gs/gsfs0/home/asyed/pw/jobs/ICLR/wandb/run-20230226_230618-3ux7cs2u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aaquib111/ICLR/runs/3ux7cs2u\" target=\"_blank\">opt-125m Wikitext Test</a></strong> to <a href=\"https://wandb.ai/aaquib111/ICLR\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/gs/gsfs0/users/asyed/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "  0%|          | 0/562 [00:00<?, ?it/s]/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|█████████▉| 560/562 [00:19<00:00, 28.68it/s]\n",
      "100%|█████████▉| 560/562 [00:19<00:00, 28.92it/s]\n",
      "100%|█████████▉| 560/562 [00:19<00:00, 28.70it/s]\n",
      "100%|█████████▉| 560/562 [00:19<00:00, 28.73it/s]\n",
      "100%|█████████▉| 560/562 [00:19<00:00, 28.89it/s]\n",
      "100%|█████████▉| 560/562 [00:19<00:00, 28.82it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils.test_utils import test_model\n",
    "\n",
    "TOKEN_LENGTH=1024\n",
    "STRIDE = 512\n",
    "wandb.init(project=\"ICLR\", \n",
    "           name = f'{MODEL_NAME} Wikitext Test', \n",
    "           config={'token_length': TOKEN_LENGTH,\n",
    "                 'model_name': MODEL_NAME,\n",
    "                 'stride': STRIDE,\n",
    "                 'fine_tuned': 'iterative'})\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'facebook/{MODEL_NAME}', \n",
    "                                          padding_side='left', \n",
    "                                          use_fast=False)\n",
    "# Load dataset\n",
    "test_set = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "encodings = tokenizer(\"\\n\\n\".join(test_set['text']), return_tensors='pt')\n",
    "\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "SPARSITIES = [0.2, 0.3, 0.5, 0.7, 0.9, 1]#, 0.4, 0.6, 0.8, 1\n",
    "\n",
    "for SPARSITY in SPARSITIES:\n",
    "    test_model(MODEL_NAME, encodings, TOKEN_LENGTH, seq_len, STRIDE, wandb, SPARSITY, model_type='iterative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5a72a",
   "metadata": {},
   "source": [
    "# Cerebras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2447310",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['opt-125m', 'opt-350m', 'opt-1.3b', 'opt-2.7b']:\n",
    "    for sparsity in [0.2,0.3,0.5,0.7,0.9,1]:\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

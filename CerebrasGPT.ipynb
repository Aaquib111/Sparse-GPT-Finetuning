{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from calculate_mask import calculate_mask\n",
    "from inverse_hessian import inverse_hessian\n",
    "from input_prehooks import put_input_hooks\n",
    "from testing_module import calculate_perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "#Load dataset\n",
    "dataset = load_dataset('c4', 'en', streaming=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "# Load model with pre-trained head\n",
    "model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "# Load generator\n",
    "generator = pipeline('text-generation', model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune \n",
    "def get_module_name(param_name):\n",
    "    if param_name[-5:] == \".bias\":\n",
    "        return param_name[:-5], \"bias\"\n",
    "    elif param_name[-7:] == \".weight\":\n",
    "        return param_name[:-7], \"weight\"\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "layer_blacklist = ['model.decoder.embed_tokens.weight', 'model.decoder.embed_tokens.bias',\n",
    "'model.decoder.embed_positions.weight']\n",
    "\n",
    "# Using calibration data (inputs to each intermediate weight layer)\n",
    "# Iterate through named parameters, calculate inverse hessian and calculate mask\n",
    "\n",
    "# mask the lowest magnitude weights of the whole model\n",
    "def mask_cerebras(model, amount=.2):\n",
    "    module_dict = {}\n",
    "    for n, m in model.named_modules():\n",
    "        module_dict[n] = m\n",
    "    # print(module_dict.keys())\n",
    "    \n",
    "    param_lookup_dict = {}\n",
    "    param_names = []\n",
    "    for name, param in model.named_parameters():\n",
    "        param_names.append(name)\n",
    "        param_lookup_dict[name] = param\n",
    "\n",
    "    parameter_list = []\n",
    "    for n, m in model.named_parameters():\n",
    "        parameter_list.append(n)\n",
    "    # print(parameter_list)\n",
    "\n",
    "    for n in parameter_list:\n",
    "        module_name, param_type = get_module_name(n)\n",
    "        if module_name is None or param_type is None or param_type==\"bias\":\n",
    "            continue\n",
    "\n",
    "        # skip the embed layer\n",
    "        if module_name in layer_blacklist:\n",
    "            continue\n",
    "        \n",
    "        param = param_lookup_dict[n]\n",
    "        # skip norms which have 1 dimension\n",
    "        if len(param.shape) < 2:\n",
    "            continue\n",
    "        \n",
    "        # perform the masking\n",
    "        module = module_dict[module_name]\n",
    "        torch.nn.utils.prune.l1_unstructured(module=module, name=param_type, amount=amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.2000)\n"
     ]
    }
   ],
   "source": [
    "# Load model with pre-trained head\n",
    "import torch\n",
    "def get_prop_zeros(model):\n",
    "    return torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) / (torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) + torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight != 0))\n",
    "\n",
    "model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "print(get_prop_zeros(model))\n",
    "\n",
    "mask_cerebras(model, amount=.2)\n",
    "print(get_prop_zeros(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_cerebras(model, amount=.4)\n",
    "print(get_prop_zeros(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model with pre-trained head\n",
    "model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE PRUNED MODEL\n",
    "pruned_model_name = f'opt-125m-{SPARSENESS}'\n",
    "# torch.save(model,'pruned_models/' + pruned_model_name)\n",
    "# model.save_pretrained(save_directory = 'pruned_models/' + pruned_model_name)\n",
    "\n",
    "torch.save(model.state_dict(), f'pruned_models/{pruned_model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD SAVED MODEL\n",
    "\n",
    "from save_pruned_model import load_into_model\n",
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "loaded_model = OPTForCausalLM.from_pretrained('facebook/opt-125m', output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "load_into_model(loaded_model, 'pruned_models/opt-125m.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/opt-125m\"\n",
    "\n",
    "def get_prop_zeros(model):\n",
    "    return torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) / (torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) + torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight != 0))\n",
    "\n",
    "print(get_prop_zeros(loaded_model))\n",
    "print(get_prop_zeros(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# REGULAR OUTPUT\n",
    "dense_model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True).to(device=device)\n",
    "encoded_test_input = tokenizer('What did you just say to me? I will have you know', return_tensors=\"pt\",\n",
    "                                                                                    padding=\"max_length\", \n",
    "                                                                                    max_length=token_length, \n",
    "                                                                                    truncation=True)\n",
    "#print(encoded_test_input)\n",
    "print('DENSE MODEL:')\n",
    "with torch.no_grad():\n",
    "    generated_ids = dense_model.generate(**encoded_test_input, max_new_tokens=30, num_beams=5, do_sample=True)\n",
    "dense_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(f'\\tOutput: {dense_output}')\n",
    "\n",
    "print('SPARSE MODEL: ')\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**encoded_test_input, max_new_tokens=30, num_beams=5, do_sample=True)\n",
    "sparse_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(f'\\tOutput: {sparse_output}')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\", output_attentions=True, output_hidden_states=True).to(device=device)\n",
    "encoded_test_input = tokenizer('What did you just say to me? I will have you know', return_tensors=\"pt\",\n",
    "                                                                                    padding=\"max_length\", \n",
    "                                                                                    max_length=token_length, \n",
    "                                                                                    truncation=True)\n",
    "print(torch.exp(dense_model(**encoded_test_input, labels = encoded_test_input.input_ids).loss))\n",
    "print(torch.exp(model(**encoded_test_input, labels = encoded_test_input.input_ids).loss))\n",
    "print(torch.exp(loaded_model(**encoded_test_input, labels = encoded_test_input.input_ids).loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_dataset('wikitext', 'wikitext-2-v1', split='test[:10%]')\n",
    "tokenized_test = tokenizer(test_set['text'])\n",
    "\n",
    "flattened_input_ids = [item for sublist in tokenized_test.input_ids for item in sublist]\n",
    "flattened_input_ids = flattened_input_ids[:(len(flattened_input_ids) - (len(flattened_input_ids) % token_length))]\n",
    "flattened_input_ids = torch.Tensor(flattened_input_ids).reshape(-1, token_length).int()\n",
    "\n",
    "flattened_masks = [item for sublist in tokenized_test.attention_mask for item in sublist]\n",
    "flattened_masks = flattened_masks[:(len(flattened_masks) - (len(flattened_masks) % token_length))]\n",
    "flattened_masks = torch.Tensor(flattened_masks).reshape(-1, token_length).int()\n",
    "\n",
    "test_dict = {'input_ids': flattened_input_ids, 'attention_mask': flattened_masks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dense_model.eval()\n",
    "output = model(**test_dict, labels=test_dict['input_ids'])\n",
    "output2 = dense_model(**test_dict, labels=test_dict['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.exp(output.loss))\n",
    "print(torch.exp(output2.loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('pyt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01ca8e50707366aa71ffa41d1a1f13415d8b38eae741916c87480058f12910d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from utils.prune_utils import sparsegpt_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_size = \"opt-350m\"\n",
    "\n",
    "model_name = f\"facebook/{model_size}\"\n",
    "\n",
    "#Load dataset\n",
    "dataset = load_dataset('wikitext', \"wikitext-2-raw-v1\", streaming=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calibrate model (get inputs to each layer with calibration data)\n",
    "\n",
    "calibration_size=128\n",
    "token_length=2048\n",
    "calibration_batch_size=2\n",
    "\n",
    "EPSILON = 0.01\n",
    "B = 128\n",
    "Bs = 128\n",
    "\n",
    "# run model on batches of calibration data, then concatenate inputs\n",
    "def split_model_calibration(model):\n",
    "    batch_sentences = []\n",
    "    for i, data in tqdm(enumerate(iter(dataset['train'])), total=calibration_size):\n",
    "        if i < calibration_size + 1:\n",
    "            if len(batch_sentences) >= calibration_batch_size:\n",
    "                with torch.no_grad():\n",
    "                    encoded_input = tokenizer(batch_sentences, return_tensors=\"pt\",\n",
    "                                              padding=\"max_length\", max_length=token_length,\n",
    "                                              truncation=True).to(device=device)\n",
    "                    model(**encoded_input, labels=encoded_input.input_ids)\n",
    "                    batch_sentences = []\n",
    "            batch_sentences.append(data['text'])\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/128 [00:00<00:29,  4.32it/s]/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "129it [03:37,  1.69s/it]                         \n",
      "100%|██████████| 388/388 [00:43<00:00,  8.94it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 10/10 [01:28<00:00,  8.88s/it]\n",
      "129it [03:38,  1.69s/it]                         \n",
      "100%|██████████| 388/388 [00:44<00:00,  8.63it/s]\n",
      "100%|██████████| 10/10 [01:14<00:00,  7.43s/it]\n",
      "129it [03:36,  1.68s/it]                         \n",
      "100%|██████████| 388/388 [00:45<00:00,  8.61it/s]\n",
      "100%|██████████| 10/10 [01:14<00:00,  7.45s/it]\n",
      "129it [03:48,  1.77s/it]                         \n",
      "100%|██████████| 388/388 [00:46<00:00,  8.38it/s]\n",
      "100%|██████████| 10/10 [01:14<00:00,  7.45s/it]\n",
      "129it [03:23,  1.58s/it]                         \n",
      "100%|██████████| 388/388 [00:42<00:00,  9.18it/s]\n",
      "100%|██████████| 10/10 [01:10<00:00,  7.00s/it]\n"
     ]
    }
   ],
   "source": [
    "from iterative_prune_finetune import iterative_sparsegpt_prune_tune\n",
    "from utils.prehook_utils import put_input_hooks,remove_all_hooks\n",
    "from utils.prune_utils import sparsegpt_prune\n",
    "from utils.finetune_utils import finetune_model_inplace\n",
    "from utils.save_utils import unmask_model\n",
    "\n",
    "SPARSITIES = [0.9, 0.7, 0.5, 0.3, 0.2]#0.1, 0.2,0.3,0.5,0.7,0.9,1\n",
    "for SPARSENESS in SPARSITIES:\n",
    "    model = OPTForCausalLM.from_pretrained(f'facebook/{model_size}', \n",
    "                                       output_attentions=True, \n",
    "                                       output_hidden_states=True).to(device=device)\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "\n",
    "    feature_hessians = {}\n",
    "    #put_input_hooks(model=model, features=feature_hessians, storage_dir=storage_dir, offload_freq=10000, feature_storage_device='cpu')\n",
    "    all_hooks = put_input_hooks(model=model, features=feature_hessians, feature_storage_device='cpu')\n",
    "    split_model_calibration(model)\n",
    "    for hook in all_hooks:\n",
    "        hook.remove()\n",
    "    sparsegpt_prune(model=model, model_name=model_size, \n",
    "                    feature_hessians=feature_hessians, \n",
    "                    EPSILON=EPSILON, SPARSENESS=SPARSENESS, B=B, Bs=Bs, save_model=False)\n",
    "    torch.cuda.empty_cache()\n",
    "    finetune_model_inplace(model=model, tokenizer=tokenizer, \n",
    "                           SPARSITY=SPARSENESS, device=device, EPOCH_COUNT=10)\n",
    "    #unmask_model(model=model)\n",
    "    pruned_model_name = f'{model_size}-finetuned-{SPARSENESS}'\n",
    "    torch.save(model.state_dict(), f'pruned_models/{pruned_model_name}-iterative.pt')\n",
    "# Prune using the sparseGPT method, saves as pruned_models/{model_name}-{SPARSENESS}.pt WITHOUT mask\n",
    "#iterative_sparsegpt_prune_tune(model, model_size, SPARSITIES, feature_hessians, EPSILON, B, Bs, tokenizer, EPOCH_COUNT=10)\n",
    "\n",
    "del model\n",
    "del feature_hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "01ca8e50707366aa71ffa41d1a1f13415d8b38eae741916c87480058f12910d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

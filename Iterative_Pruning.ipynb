{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phillipguo/opt/anaconda3/envs/pyt/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "from calculate_mask import calculate_mask\n",
    "from inverse_hessian import calc_inverse_hessian\n",
    "from input_prehooks import put_input_hooks\n",
    "from testing_module import calculate_perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_size = \"opt-125m\"\n",
    "\n",
    "model_name = f\"facebook/{model_size}\"\n",
    "# model_name = \"facebook/opt-1.3b\"\n",
    "\n",
    "#Load dataset\n",
    "dataset = load_dataset('wikitext', \"wikitext-2-raw-v1\", streaming=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calibrate model (get inputs to each layer with calibration data)\n",
    "\n",
    "calibration_size=4\n",
    "token_length=512\n",
    "calibration_batch_size=2\n",
    "\n",
    "EPSILON = 1e-8\n",
    "B = 4\n",
    "Bs = 2\n",
    "\n",
    "# run model on batches of calibration data, then concatenate inputs\n",
    "def split_model_calibration(model):\n",
    "    batch_sentences = []\n",
    "    for i, data in tqdm(enumerate(iter(dataset['train'])), total=calibration_size):\n",
    "        if i < calibration_size + 1:\n",
    "            if len(batch_sentences) >= calibration_batch_size:\n",
    "                with torch.no_grad():\n",
    "                    encoded_input = tokenizer(batch_sentences, return_tensors=\"pt\",\n",
    "                                              padding=\"max_length\", max_length=token_length,\n",
    "                                              truncation=True).to(device=device)\n",
    "                    model(**encoded_input, labels=encoded_input.input_ids)\n",
    "                    batch_sentences = []\n",
    "            batch_sentences.append(data['text'])\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:04,  1.22it/s]                       \n",
      "  0%|          | 0/5 [00:00<?, ?it/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 3\n",
      "batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "{'input_ids': tensor([[    2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,  5457,   468, 44068,  6374, 41674,  6395,  5457,  1437, 50118,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1],\n",
      "        [    2,  2211,   267, 38183,   117,   468, 44068,  6374,   155,  4832,\n",
      "          1890, 36902, 41674,    36,  2898,  4832, 47416, 23133, 18164, 42393,\n",
      "         21402, 20024, 48018, 50033, 49080, 49587, 49432, 48947, 49017,   246,\n",
      "          2156,  6474,   479,   468, 44068,  6374,     9,     5, 36954,   155,\n",
      "          4839,  2156, 10266,  4997,     7,    25,   468, 44068,  6374, 41674,\n",
      "          6395,   751,  1429,  2156,    16,    10, 15714,   774,   787,    12,\n",
      "          1039,   816,   569,   177,  2226,    30, 43561,     8,  2454,     4,\n",
      "         36753,    13,     5, 15592, 39435,   479, 30939,    11,   644,  1466,\n",
      "            11,  1429,  2156,    24,    16,     5,   371,   177,    11,     5,\n",
      "           468, 44068,  6374,   651,   479, 23564,   154,     5,   276, 24904,\n",
      "             9, 15714,     8,   588,   787,    12,  1039,    86, 23841,    25,\n",
      "            63, 20193,  2156,     5,   527,  1237, 12980,     7,     5,    78,\n",
      "           177,     8,  3905,     5,    22,  8603, 13802,    22,  2156,    10,\n",
      "         14914,   831,  1933,  2754,     5,  1226,     9,  7155,   493,   148,\n",
      "             5,  4665,  5122, 12560,  1771,    54,  3008,  3556,   909,  1414,\n",
      "             8,    32, 30259,   136,     5, 16659,  1933,    22,  2912,   424,\n",
      "           415,   219, 22546,    22,   479,  1437, 50118],\n",
      "        [    2,    20,   177,   880,   709,    11,  1824,  2156,  3406,    81,\n",
      "            10,   739,  4745,     9,     5,   173,   626,    15,   468, 44068,\n",
      "          6374, 41674,  3082,   479,   616,    24, 12544,     5,  2526,  1575,\n",
      "             9,     5,   651,  2156,    24,    67, 12796,  1533, 11431,  2156,\n",
      "           215,    25,   442,     5,   177,    55, 36341,    13,   651, 19298,\n",
      "           479, 35177,  6004,   248,  5236,   102,  8768,   267,  1438,     8,\n",
      "         17964, 15225, 23552, 17040, 36066,   258,  1835,    31,   986, 11410,\n",
      "          2156,   552,    19,   468, 44068,  6374, 41674,  3082,   736, 29072,\n",
      "          3592, 10548,  6498,   479,    83,   739,   165,     9,  6737,  7521,\n",
      "             5,  8543,   479,    20,   177,   128,    29,  1273,  4782,    21,\n",
      "         26115,    30,   392,   128,   282,   479,  1437, 50118,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[    2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [    2,  5457,   468, 44068,  6374, 41674,  6395,  5457,  1437, 50118,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [    2,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [    2,  2211,   267, 38183,   117,   468, 44068,  6374,   155,  4832,\n",
      "          1890, 36902, 41674,    36,  2898,  4832, 47416, 23133, 18164, 42393,\n",
      "         21402, 20024, 48018, 50033, 49080, 49587, 49432, 48947, 49017,   246,\n",
      "          2156,  6474,   479,   468, 44068,  6374,     9,     5, 36954,   155,\n",
      "          4839,  2156, 10266,  4997,     7,    25,   468, 44068,  6374, 41674,\n",
      "          6395,   751,  1429,  2156,    16,    10, 15714,   774,   787,    12,\n",
      "          1039,   816,   569,   177,  2226,    30, 43561,     8,  2454,     4,\n",
      "         36753,    13,     5, 15592, 39435,   479, 30939,    11,   644,  1466,\n",
      "            11,  1429,  2156,    24,    16,     5,   371,   177,    11,     5,\n",
      "           468, 44068,  6374,   651,   479, 23564,   154,     5,   276, 24904,\n",
      "             9, 15714,     8,   588,   787,    12,  1039,    86, 23841,    25,\n",
      "            63, 20193,  2156,     5,   527,  1237, 12980,     7,     5,    78,\n",
      "           177,     8,  3905,     5,    22,  8603, 13802,    22,  2156,    10,\n",
      "         14914,   831,  1933,  2754,     5,  1226,     9,  7155,   493,   148,\n",
      "             5,  4665,  5122, 12560,  1771,    54,  3008,  3556,   909,  1414,\n",
      "             8,    32, 30259,   136,     5, 16659,  1933,    22,  2912,   424,\n",
      "           415,   219, 22546,    22,   479,  1437, 50118],\n",
      "        [    2,    20,   177,   880,   709,    11,  1824,  2156,  3406,    81,\n",
      "            10,   739,  4745,     9,     5,   173,   626,    15,   468, 44068,\n",
      "          6374, 41674,  3082,   479,   616,    24, 12544,     5,  2526,  1575,\n",
      "             9,     5,   651,  2156,    24,    67, 12796,  1533, 11431,  2156,\n",
      "           215,    25,   442,     5,   177,    55, 36341,    13,   651, 19298,\n",
      "           479, 35177,  6004,   248,  5236,   102,  8768,   267,  1438,     8,\n",
      "         17964, 15225, 23552, 17040, 36066,   258,  1835,    31,   986, 11410,\n",
      "          2156,   552,    19,   468, 44068,  6374, 41674,  3082,   736, 29072,\n",
      "          3592, 10548,  6498,   479,    83,   739,   165,     9,  6737,  7521,\n",
      "             5,  8543,   479,    20,   177,   128,    29,  1273,  4782,    21,\n",
      "         26115,    30,   392,   128,   282,   479,  1437, 50118,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:04<00:16,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.224172115325928\n",
      "Batch size: 3\n",
      "batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "{'input_ids': tensor([[    2,    85,  1145,  ...,     1,     1,     1],\n",
      "        [    2,     1,     1,  ...,     1,     1,     1],\n",
      "        [    2,  5457,  5457,  ...,     1,     1,     1],\n",
      "        [    2,     1,     1,  ...,     1,     1,     1],\n",
      "        [    2,   287,    19,  ...,   479,  1437, 50118]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[    2,    85,  1145,  ...,  -100,  -100,  -100],\n",
      "        [    2,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [    2,  5457,  5457,  ...,  -100,  -100,  -100],\n",
      "        [    2,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [    2,   287,    19,  ...,   479,  1437, 50118]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:09<00:14,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 3\n",
      "batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "{'input_ids': tensor([[    2,    20,   177,  ...,   479,  1437, 50118],\n",
      "        [    2,  6354,  5090,  ...,     1,     1,     1],\n",
      "        [    2,     1,     1,  ...,     1,     1,     1],\n",
      "        [    2,  5457,  5457,  ...,     1,     1,     1],\n",
      "        [    2,     1,     1,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0]]), 'labels': tensor([[    2,    20,   177,  ...,   479,  1437, 50118],\n",
      "        [    2,  6354,  5090,  ...,  -100,  -100,  -100],\n",
      "        [    2,  -100,  -100,  ...,  -100,  -100,  -100],\n",
      "        [    2,  5457,  5457,  ...,  -100,  -100,  -100],\n",
      "        [    2,  -100,  -100,  ...,  -100,  -100,  -100]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:10<00:15,  5.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39miterative_prune_finetune\u001b[39;00m \u001b[39mimport\u001b[39;00m iterative_sparsegpt_prune_tune, iterative_cerebras_prune_tune\n\u001b[1;32m     37\u001b[0m \u001b[39m# iterative_sparsegpt_prune_tune(model=model, model_size=model_size, sparseness_sequence=sparseness_sequence,\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m#  feature_hessians=feature_hessians, EPSILON=EPSILON, B=B, Bs=Bs, training_data=training_data, tokenizer=tokenizer)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m iterative_cerebras_prune_tune(model\u001b[39m=\u001b[39;49mmodel, model_size\u001b[39m=\u001b[39;49mmodel_size, sparseness_sequence\u001b[39m=\u001b[39;49msparseness_sequence, \n\u001b[1;32m     41\u001b[0m training_data\u001b[39m=\u001b[39;49mtraining_data, tokenizer\u001b[39m=\u001b[39;49mtokenizer, EPOCH_COUNT\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     42\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# for sparseness_index in range(len(sparseness_sequence)):\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39mfor sparseness in sparseness_sequence:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[39m    '''\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Deep Learning/Sparse-GPT-Finetuning/iterative_prune_finetune.py:48\u001b[0m, in \u001b[0;36miterative_cerebras_prune_tune\u001b[0;34m(model, model_size, sparseness_sequence, training_data, tokenizer, EPOCH_COUNT)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m# activate masks\u001b[39;00m\n\u001b[1;32m     46\u001b[0m mask_from_pruned(model\u001b[39m=\u001b[39mmodel)\n\u001b[0;32m---> 48\u001b[0m fine_tune(model\u001b[39m=\u001b[39;49mmodel, training_data\u001b[39m=\u001b[39;49mtraining_data, EPOCH_COUNT\u001b[39m=\u001b[39;49mEPOCH_COUNT, tokenizer\u001b[39m=\u001b[39;49mtokenizer)\n\u001b[1;32m     50\u001b[0m \u001b[39m# deactivate masks\u001b[39;00m\n\u001b[1;32m     51\u001b[0m unmask_model(model\u001b[39m=\u001b[39mmodel)\n",
      "File \u001b[0;32m~/Documents/Deep Learning/Sparse-GPT-Finetuning/trainingv2.py:31\u001b[0m, in \u001b[0;36mfine_tune\u001b[0;34m(model, training_data, EPOCH_COUNT, tokenizer)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch keys: \u001b[39m\u001b[39m{\u001b[39;00mbatch\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[39mprint\u001b[39m(batch)\n\u001b[0;32m---> 31\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m     32\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     33\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyt/lib/python3.10/site-packages/torch/nn/modules/module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyt/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:929\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    926\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    928\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 929\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m    930\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    931\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    932\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    933\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    934\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    935\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    936\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    937\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    938\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    939\u001b[0m )\n\u001b[1;32m    941\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    943\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyt/lib/python3.10/site-packages/torch/nn/modules/module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyt/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:693\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    685\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    690\u001b[0m     )\n\u001b[1;32m    691\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    694\u001b[0m         hidden_states,\n\u001b[1;32m    695\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    696\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    697\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    698\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    699\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    702\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    704\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyt/lib/python3.10/site-packages/torch/nn/modules/module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyt/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:350\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions, use_cache, past_key_value)\u001b[0m\n\u001b[1;32m    347\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(hidden_states)\n\u001b[1;32m    348\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_fn(hidden_states)\n\u001b[0;32m--> 350\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(hidden_states)\n\u001b[1;32m    351\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    353\u001b[0m hidden_states \u001b[39m=\u001b[39m (residual \u001b[39m+\u001b[39m hidden_states)\u001b[39m.\u001b[39mview(hidden_states_shape)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyt/lib/python3.10/site-packages/torch/nn/modules/module.py:1201\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks:\n\u001b[1;32m   1200\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_pre_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1201\u001b[0m         result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1202\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Deep Learning/Sparse-GPT-Finetuning/input_prehooks.py:77\u001b[0m, in \u001b[0;36mput_input_hooks.<locals>.get_features.<locals>.pre_hook\u001b[0;34m(model, input)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39m# make new entry if not existing\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     features[storage_name] \u001b[39m=\u001b[39m input_hessian\n\u001b[0;32m---> 77\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mempty_cache()\n\u001b[1;32m     78\u001b[0m \u001b[39m'''if features[storage_name].shape[0] % offload_freq == 0:\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m    if os.path.exists(f'{storage_dir}/{name}'):\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m        existing_tensor = torch.load(f'{storage_dir}/{name}')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39m        torch.save(features[storage_name],f'{storage_dir}/{name}')\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39m    del features[storage_name]'''\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pyt/lib/python3.10/site-packages/torch/cuda/memory.py:113\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInvalid fraction value: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    108\u001b[0m                          \u001b[39m'\u001b[39m\u001b[39mAllowed range: 0~1\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(fraction))\n\u001b[1;32m    110\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_cuda_setMemoryFraction(fraction, device)\n\u001b[0;32m--> 113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mempty_cache\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39m    allocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    `nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m        more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m is_initialized():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SparseGPT fine tune loop\n",
    "from SparseGPT_pruning import sparsegpt_prune\n",
    "from trainingv2 import fine_tune\n",
    "from save_pruned_model import mask_from_pruned, unmask_model\n",
    "\n",
    "# first, calibrate model\n",
    "model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True,\n",
    "                                        output_hidden_states=True).to(device=device) # type: ignore\n",
    "feature_hessians = {}\n",
    "#put_input_hooks(model=model, features=feature_hessians, storage_dir=storage_dir, offload_freq=10000, feature_storage_device='cpu')\n",
    "put_input_hooks(model=model, features=feature_hessians, feature_storage_device='cpu')\n",
    "split_model_calibration(model)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# set up data\n",
    "def encode_tok(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "#stream c4, training split\n",
    "training_data = load_dataset('wikitext', \"wikitext-2-raw-v1\", split='train', streaming=True)\n",
    "#load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')\n",
    "#IMPORTANT: process data while streaming -> remove unnecessary columns in batches\n",
    "training_data = training_data.map(encode_tok, batched=True, remove_columns=[\"text\"])\n",
    "#set data to tensor mode\n",
    "training_data = training_data.with_format(\"torch\")\n",
    "\n",
    "def get_prop_zeros(model):\n",
    "    return torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) / (torch.numel(model.get_decoder().layers[0].self_attn.k_proj.weight))\n",
    "\n",
    "# now, fine tune loop\n",
    "# sparseness is defined as proportion of nonzeros (opposite of intuitive)\n",
    "# sparseness_sequence = [.9, .8, .7, .6, .5, .4, .3, .2]\n",
    "sparseness_sequence = [.9, .5, .4]\n",
    "\n",
    "from iterative_prune_finetune import iterative_sparsegpt_prune_tune, iterative_cerebras_prune_tune\n",
    "# iterative_sparsegpt_prune_tune(model=model, model_size=model_size, sparseness_sequence=sparseness_sequence,\n",
    "#  feature_hessians=feature_hessians, EPSILON=EPSILON, B=B, Bs=Bs, training_data=training_data, tokenizer=tokenizer)\n",
    "\n",
    "iterative_cerebras_prune_tune(model=model, model_size=model_size, sparseness_sequence=sparseness_sequence, \n",
    "training_data=training_data, tokenizer=tokenizer, EPOCH_COUNT=2)\n",
    "'''\n",
    "# for sparseness_index in range(len(sparseness_sequence)):\n",
    "for sparseness in sparseness_sequence:\n",
    "\n",
    "    # if sparseness_index == 0:\n",
    "    #     sparseness_dif = sparseness_sequence[sparseness_index]\n",
    "    # else:\n",
    "    #     sparseness_dif = sparseness_sequence[sparseness_index] / sparseness_sequence[sparseness_index -1]\n",
    "    \n",
    "    sparsegpt_prune(model=model, feature_hessians=feature_hessians, SPARSENESS=sparseness, EPSILON=EPSILON, B=B, Bs=Bs)\n",
    "    print(f\"After pruning, Model has {get_prop_zeros(model)}\")\n",
    "\n",
    "    # activate masks\n",
    "    mask_from_pruned(model=model)\n",
    "\n",
    "    fine_tune(model=model, training_data=training_data, EPOCH_COUNT=1, tokenizer=tokenizer)\n",
    "    \n",
    "    print(f\"After fine-tuning, Model has {get_prop_zeros(model)}\")\n",
    "\n",
    "    # deactivate masks\n",
    "    unmask_model(model=model)\n",
    "\n",
    "    pruned_model_name = f'{model_size}-test-{sparseness}'\n",
    "    torch.save(model.state_dict(), f'pruned_models/{pruned_model_name}.pt')\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True,\n",
    "                                        output_hidden_states=True).to(device=device) # type: ignore\n",
    "\n",
    "# set up data\n",
    "def encode_tok(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "#stream c4, training split\n",
    "training_data = load_dataset('wikitext', \"wikitext-2-raw-v1\", split='train', streaming=True)\n",
    "#load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')\n",
    "#IMPORTANT: process data while streaming -> remove unnecessary columns in batches\n",
    "training_data = training_data.map(encode_tok, batched=True, remove_columns=[\"text\"])\n",
    "#set data to tensor mode\n",
    "training_data = training_data.with_format(\"torch\")\n",
    "\n",
    "sparsegpt_prune(model=model, feature_hessians=feature_hessians, SPARSENESS=.8, EPSILON=EPSILON, B=B, Bs=Bs)\n",
    "print(f\"After pruning, Model has {get_prop_zeros(model)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_from_pruned(model)\n",
    "\n",
    "for n, m in model.named_buffers():\n",
    "    # print(torch.sum(m == 0))\n",
    "    print(m)\n",
    "\n",
    "print(f\"After pruning, Model has {get_prop_zeros(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune(model=model, training_data=training_data, EPOCH_COUNT=1, tokenizer=tokenizer)\n",
    "\n",
    "print(f\"After fine-tuning, Model has {get_prop_zeros(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, m in model.named_buffers():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input_prehooks import get_feature_storage_name\n",
    "import gc\n",
    "from SparseGPT_pruning import sparsegpt_prune\n",
    "layer_blacklist = ['model.decoder.embed_tokens.weight', 'model.decoder.embed_tokens.bias', 'model.decoder.embed_positions.weight']\n",
    "\n",
    "# Using calibration data (inputs to each intermediate weight layer)\n",
    "# Iterate through named parameters, calculate inverse hessian and calculate mask\n",
    "\n",
    "SPARSENESS_LIST = [0.5]#0.1, 0.2, 0.3, 0.5, 0.7, 0.9\n",
    "for i, SPARSENESS in enumerate(SPARSENESS_LIST):\n",
    "    \n",
    "    # Load model with pre-trained head\n",
    "    model = OPTForCausalLM.from_pretrained(model_name, output_attentions=True,\n",
    "                                           output_hidden_states=True).to(device=device) # type: ignore\n",
    "    \n",
    "    #storage_dir = f'tmp/{model_name}-{SPARSENESS}'\n",
    "    \n",
    "    # First, put in forward hooks\n",
    "    # Don't store inputs, instead store hessians (less data)\n",
    "    # Only store hessians once, as all models take the same hessians\n",
    "    print('------------')\n",
    "    if i == 0:\n",
    "        feature_hessians = {}\n",
    "        #put_input_hooks(model=model, features=feature_hessians, storage_dir=storage_dir, offload_freq=10000, feature_storage_device='cpu')\n",
    "        put_input_hooks(model=model, features=feature_hessians, feature_storage_device='cpu')\n",
    "        split_model_calibration(model)\n",
    "        torch.cuda.empty_cache()\n",
    "    print('-----------')\n",
    "    \n",
    "    sparsegpt_prune(model=model, feature_hessians=feature_hessians, # type: ignore\n",
    "    EPSILON=EPSILON, SPARSENESS=SPARSENESS, B=B, Bs=Bs)\n",
    "\n",
    "    pruned_model_name = f'{model_size}-test-{SPARSENESS}'\n",
    "    # make a dictionary to access module by name\n",
    "    module_lookup_dict = {}\n",
    "    for module_name, module_iter in model.named_modules():\n",
    "        module_lookup_dict[module_name] = module_iter\n",
    "\n",
    "    # without this\n",
    "    param_lookup_dict = {}\n",
    "    param_names = []\n",
    "    for name, param in model.named_parameters():\n",
    "        param_names.append(name)\n",
    "        param_lookup_dict[name] = param\n",
    "    \n",
    "    model.eval()\n",
    "    #model = torch.nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "    with torch.no_grad():\n",
    "        # for name in tqdm(param_names):\n",
    "\n",
    "    torch.save(model.state_dict(), f'pruned_models/{pruned_model_name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from save_pruned_model import load_unmasked_model, load_masked_model\n",
    "\n",
    "loaded_model = OPTForCausalLM.from_pretrained(f'facebook/{model_size}', output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "# loaded_model = torch.nn.DataParallel(loaded_model, device_ids=[0,1,2,3])\n",
    "load_unmasked_model(loaded_model, f'pruned_models/{model_size}-test-0.5.pt')\n",
    "\n",
    "loaded_model_2 = OPTForCausalLM.from_pretrained(f'facebook/{model_size}', output_attentions=True, output_hidden_states=True).to(device=device) # type: ignore\n",
    "load_masked_model(loaded_model_2, f'pruned_models/{model_size}-test-0.5.pt')\n",
    "\n",
    "def get_prop_zeros(model):\n",
    "    return torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) / (torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight == 0) + torch.sum(model.get_decoder().layers[0].self_attn.k_proj.weight != 0))\n",
    "\n",
    "print(get_prop_zeros(loaded_model))\n",
    "print(get_prop_zeros(model))\n",
    "print(get_prop_zeros(loaded_model_2))\n",
    "# loaded_model.eval()\n",
    "# _ = loaded_model(torch.randint(high=20, size=(1,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_list = []\n",
    "layer_blacklist = ['', 'module','module.model','module.model.decoder',\n",
    "                   'module.model.decoder.embed_tokens',\n",
    "                   'module.model.decoder.embed_tokens',\n",
    "                   'module.model.decoder.embed_positions']\n",
    "for name, module in model.named_modules():\n",
    "    # skip the embed layer or skip norms which have 1 dimension\n",
    "    if name in layer_blacklist or 'norm' in name or not isinstance(module, torch.nn.Module):\n",
    "        continue\n",
    "    prune_list.append((module, 'weight'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prune to 0s\n",
    "class ThresholdPruning(prune.BasePruningMethod):\n",
    "    PRUNING_TYPE = \"unstructured\"\n",
    "\n",
    "    # default threshold is 0, prunes weights that are already 0 (for training)\n",
    "    def __init__(self, threshold=1e-8):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def compute_mask(self, tensor, default_mask):\n",
    "        return torch.abs(tensor) >= self.threshold\n",
    "    \n",
    "prune.global_unstructured(prune_list,\n",
    "                          pruning_method=ThresholdPruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "01ca8e50707366aa71ffa41d1a1f13415d8b38eae741916c87480058f12910d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

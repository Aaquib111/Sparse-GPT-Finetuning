{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141ae67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "import torch\n",
    "#from parallelformers import parallelize\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, OPTForCausalLM, Trainer, TrainingArguments\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from utils.save_utils import load_masked_model, load_masked_model_single\n",
    "from utils.prehook_utils import remove_all_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52904c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step():\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    #accelerate = Accelerator()\n",
    "    \n",
    "    def encode_tok(examples):\n",
    "        return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "\n",
    "    #print(torch.cuda.is_initialized())\n",
    "    model_name='opt-1.3b'\n",
    "    EPOCH_COUNT=10\n",
    "    SPARSITY=0.3\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f'facebook/{model_name}', padding_side='left', model_max_length=1024)\n",
    "    #print(torch.cuda.is_initialized())\n",
    "    #stream c4, training split\n",
    "    training_data = load_dataset('c4', 'en', split='train', streaming=True)\n",
    "    #training_data = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train', streaming=True)\n",
    "    #IMPORTANT: process data while streaming -> remove unnecessary columns in batches\n",
    "    training_data = training_data.map(encode_tok, \n",
    "                                      batched=True,\n",
    "                                      remove_columns=[\"text\", \"timestamp\", \"url\"])\n",
    "    #set data to tensor mode\n",
    "    training_data = training_data.with_format(\"torch\")\n",
    "\n",
    "    #dataloader from dataloader (mlm=False when training without mask)\n",
    "    dataloader = DataLoader(training_data, \n",
    "                            collate_fn=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "                            batch_size=1, num_workers=16, pin_memory=True)\n",
    "    #print(torch.cuda.is_initialized())\n",
    "    loaded_model = OPTForCausalLM.from_pretrained(f'facebook/{model_name}',\n",
    "                                                      output_attentions=True,\n",
    "                                                      output_hidden_states=True).to(device=device)\n",
    "    #print(torch.cuda.is_initialized())\n",
    "    if SPARSITY != 1:\n",
    "        load_masked_model_single(loaded_model, f'pruned_models/{model_name}-{SPARSITY}.pt')\n",
    "    loaded_model = torch.nn.DataParallel(loaded_model, device_ids=[0,1,2,3])\n",
    "    #loaded_model = parallelize(loaded_model, num_gpus=4, fp16=True, verbose='detail')\n",
    "    loaded_model.train()\n",
    "    \n",
    "    FROZEN_LIMIT = 18\n",
    "    for n, p in loaded_model.named_parameters():\n",
    "        split_name = n.split('layers.')\n",
    "        if len(split_name) > 1 and int(split_name[-1].split('.')[0]) < FROZEN_LIMIT:\n",
    "            if p.requires_grad:\n",
    "                p.requires_grad = False\n",
    "    #print(torch.cuda.is_initialized())\n",
    "    t_optim = torch.optim.AdamW(params=loaded_model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "    #loaded_model, t_optim, dataloader = accelerate.prepare(loaded_model, t_optim, dataloader)\n",
    "    !nvidia-smi\n",
    "    t_optim = torch.optim.AdamW(params=loaded_model.parameters(), lr=1e-5)\n",
    "    #print(torch.cuda.is_initialized())\n",
    "    for epoch in tqdm(range(EPOCH_COUNT)):\n",
    "        training_data.set_epoch(epoch)\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            #print('TRAINING')\n",
    "            !nvidia-smi\n",
    "            #print(batch)\n",
    "            if i == 5:\n",
    "                break\n",
    "            #batch = {k: torch.tensor(v) for k, v in batch.items()}\n",
    "            #print(batch)\n",
    "            #print(batch['input_ids'].size())\n",
    "            #print(batch['attention_mask '].size())\n",
    "            outputs = loaded_model(**batch)\n",
    "            loss = outputs.loss\n",
    "            print(loss)\n",
    "            #accelerate.backward(loss)\n",
    "            loss.backward()\n",
    "            t_optim.step()\n",
    "            t_optim.zero_grad()\n",
    "    torch.save(loaded_model.state_dict(), f'pruned_models/{model_name}-{SPARSITY}-finetuned.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e51d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 26 17:22:57 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    56W / 250W |  17333MiB / 40960MiB |     42%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCI...  On   | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    37W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCI...  On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    34W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PCI...  On   | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    33W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     13331      C   ...l8/apps/conda3/bin/python    17331MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 26 17:24:06 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    35W / 250W |  17333MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCI...  On   | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    37W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCI...  On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    34W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PCI...  On   | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    33W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     13331      C   ...l8/apps/conda3/bin/python    17331MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.5384], device='cuda:0', grad_fn=<GatherBackward>)\n",
      "Sun Feb 26 17:24:13 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    54W / 250W |  38161MiB / 40960MiB |    100%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCI...  On   | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    40W / 250W |    421MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCI...  On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    37W / 250W |    421MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PCI...  On   | 00000000:C1:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    36W / 250W |    421MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     13331      C   ...l8/apps/conda3/bin/python    38159MiB |\n",
      "|    1   N/A  N/A     13331      C   ...l8/apps/conda3/bin/python      419MiB |\n",
      "|    2   N/A  N/A     13331      C   ...l8/apps/conda3/bin/python      419MiB |\n",
      "|    3   N/A  N/A     13331      C   ...l8/apps/conda3/bin/python      419MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [01:16<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_13331/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2753431802.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_13331/2753431802.py'</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_13331/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1029798185.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">65</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_13331/1029798185.py'</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_h <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">data_parallel.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">171</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device_ids) == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">169 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.module(*inputs[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>], **kwargs[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>])                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170 │   │   │   </span>replicas = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.replicate(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.module, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device_ids[:<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(inputs)])   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>171 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.parallel_apply(replicas, inputs, kwargs)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gather(outputs, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.output_device)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">173 │   </span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">174 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">replicate</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, module, device_ids):                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">data_parallel.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">181</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">parallel_apply</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">178 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> scatter_kwargs(inputs, kwargs, device_ids, dim=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dim)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">179 │   </span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">180 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">parallel_apply</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, replicas, inputs, kwargs):                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>181 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> parallel_apply(replicas, inputs, kwargs, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device_ids[:<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(replic <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">182 │   </span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">183 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">gather</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, outputs, output_device):                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">184 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> gather(outputs, output_device, dim=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dim)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">parallel_apply</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">89</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">parallel_apply</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">86 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(inputs)):                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">87 │   │   </span>output = results[i]                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">88 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(output, ExceptionWrapper):                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>89 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output.reraise()                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">90 │   │   </span>outputs.append(output)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">91 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> outputs                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">92 </span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">543</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">reraise</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">540 │   │   │   # If the exception takes multiple arguments, don't try to</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">541 │   │   │   # instantiate since we don't know how to</span>                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">542 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">RuntimeError</span>(msg) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">None</span>                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>543 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> exception                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">544 </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">545 </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">546 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_get_available_device_type</span>():                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>Caught OutOfMemoryError in replica <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> on device <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.\n",
       "Original Traceback <span style=\"font-weight: bold\">(</span>most recent call last<span style=\"font-weight: bold\">)</span>:\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/parallel_a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">pply.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, in _worker\n",
       "    output = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">module</span><span style=\"font-weight: bold\">(</span>*input, **kwargs<span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"</span>,\n",
       "line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1194</span>, in _call_impl\n",
       "    return <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">forward_call</span><span style=\"font-weight: bold\">(</span>*input, **kwargs<span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/transformers/models/opt/mode</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ling_opt.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">932</span>, in forward\n",
       "    outputs = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">self.model.decoder</span><span style=\"font-weight: bold\">(</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"</span>,\n",
       "line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1194</span>, in _call_impl\n",
       "    return <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">forward_call</span><span style=\"font-weight: bold\">(</span>*input, **kwargs<span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/transformers/models/opt/mode</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ling_opt.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">697</span>, in forward\n",
       "    layer_outputs = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">decoder_layer</span><span style=\"font-weight: bold\">(</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"</span>,\n",
       "line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1194</span>, in _call_impl\n",
       "    return <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">forward_call</span><span style=\"font-weight: bold\">(</span>*input, **kwargs<span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/transformers/models/opt/mode</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ling_opt.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span>, in forward\n",
       "    hidden_states, self_attn_weights, present_key_value = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">self.self_attn</span><span style=\"font-weight: bold\">(</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"</span>,\n",
       "line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1194</span>, in _call_impl\n",
       "    return <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">forward_call</span><span style=\"font-weight: bold\">(</span>*input, **kwargs<span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/transformers/models/opt/mode</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ling_opt.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">171</span>, in forward\n",
       "    query_states = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">self.q_proj</span><span style=\"font-weight: bold\">(</span>hidden_states<span style=\"font-weight: bold\">)</span> * self.scaling\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"</span>,\n",
       "line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1201</span>, in _call_impl\n",
       "    result = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">hook</span><span style=\"font-weight: bold\">(</span>self, input<span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/utils/prune.py\"</span>, \n",
       "line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33</span>, in __call__\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">setattr</span><span style=\"font-weight: bold\">(</span>module, self._tensor_name, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">self.apply_mask</span><span style=\"font-weight: bold\">(</span>module<span style=\"font-weight: bold\">))</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/utils/prune.py\"</span>, \n",
       "line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">74</span>, in apply_mask\n",
       "    pruned_tensor = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">mask.to</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">orig</span>.dtype<span style=\"font-weight: bold\">)</span> * orig\n",
       "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39.41</span> \n",
       "GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37.94</span> GiB already allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.50</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">38.10</span> GiB reserved in total \n",
       "by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid \n",
       "fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_13331/\u001b[0m\u001b[1;33m2753431802.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_13331/2753431802.py'\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_13331/\u001b[0m\u001b[1;33m1029798185.py\u001b[0m:\u001b[94m65\u001b[0m in \u001b[92mtraining_step\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_13331/1029798185.py'\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1194\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_h \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/\u001b[0m\u001b[1;33mdata_parallel.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mpy\u001b[0m:\u001b[94m171\u001b[0m in \u001b[92mforward\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(\u001b[96mself\u001b[0m.device_ids) == \u001b[94m1\u001b[0m:                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m169 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.module(*inputs[\u001b[94m0\u001b[0m], **kwargs[\u001b[94m0\u001b[0m])                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m│   │   │   \u001b[0mreplicas = \u001b[96mself\u001b[0m.replicate(\u001b[96mself\u001b[0m.module, \u001b[96mself\u001b[0m.device_ids[:\u001b[96mlen\u001b[0m(inputs)])   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m171 \u001b[2m│   │   │   \u001b[0moutputs = \u001b[96mself\u001b[0m.parallel_apply(replicas, inputs, kwargs)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.gather(outputs, \u001b[96mself\u001b[0m.output_device)                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m173 \u001b[0m\u001b[2m│   \u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mreplicate\u001b[0m(\u001b[96mself\u001b[0m, module, device_ids):                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/\u001b[0m\u001b[1;33mdata_parallel.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mpy\u001b[0m:\u001b[94m181\u001b[0m in \u001b[92mparallel_apply\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m178 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m scatter_kwargs(inputs, kwargs, device_ids, dim=\u001b[96mself\u001b[0m.dim)             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m179 \u001b[0m\u001b[2m│   \u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m180 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mparallel_apply\u001b[0m(\u001b[96mself\u001b[0m, replicas, inputs, kwargs):                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m181 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m parallel_apply(replicas, inputs, kwargs, \u001b[96mself\u001b[0m.device_ids[:\u001b[96mlen\u001b[0m(replic \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m182 \u001b[0m\u001b[2m│   \u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m183 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mgather\u001b[0m(\u001b[96mself\u001b[0m, outputs, output_device):                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m184 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m gather(outputs, output_device, dim=\u001b[96mself\u001b[0m.dim)                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/\u001b[0m\u001b[1;33mparallel_apply\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m89\u001b[0m in \u001b[92mparallel_apply\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m86 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(\u001b[96mlen\u001b[0m(inputs)):                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m87 \u001b[0m\u001b[2m│   │   \u001b[0moutput = results[i]                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m88 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(output, ExceptionWrapper):                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m89 \u001b[2m│   │   │   \u001b[0moutput.reraise()                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m90 \u001b[0m\u001b[2m│   │   \u001b[0moutputs.append(output)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m91 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m outputs                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m92 \u001b[0m                                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/\u001b[0m\u001b[1;33m_utils.py\u001b[0m:\u001b[94m543\u001b[0m in \u001b[92mreraise\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m540 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# If the exception takes multiple arguments, don't try to\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m541 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# instantiate since we don't know how to\u001b[0m                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m542 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(msg) \u001b[94mfrom\u001b[0m \u001b[96mNone\u001b[0m                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m543 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m exception                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m544 \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m545 \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m546 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_get_available_device_type\u001b[0m():                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mOutOfMemoryError: \u001b[0mCaught OutOfMemoryError in replica \u001b[1;36m0\u001b[0m on device \u001b[1;36m0\u001b[0m.\n",
       "Original Traceback \u001b[1m(\u001b[0mmost recent call last\u001b[1m)\u001b[0m:\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/parallel/parallel_a\u001b[0m\n",
       "\u001b[32mpply.py\"\u001b[0m, line \u001b[1;36m64\u001b[0m, in _worker\n",
       "    output = \u001b[1;35mmodule\u001b[0m\u001b[1m(\u001b[0m*input, **kwargs\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\u001b[0m,\n",
       "line \u001b[1;36m1194\u001b[0m, in _call_impl\n",
       "    return \u001b[1;35mforward_call\u001b[0m\u001b[1m(\u001b[0m*input, **kwargs\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/transformers/models/opt/mode\u001b[0m\n",
       "\u001b[32mling_opt.py\"\u001b[0m, line \u001b[1;36m932\u001b[0m, in forward\n",
       "    outputs = \u001b[1;35mself.model.decoder\u001b[0m\u001b[1m(\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\u001b[0m,\n",
       "line \u001b[1;36m1194\u001b[0m, in _call_impl\n",
       "    return \u001b[1;35mforward_call\u001b[0m\u001b[1m(\u001b[0m*input, **kwargs\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/transformers/models/opt/mode\u001b[0m\n",
       "\u001b[32mling_opt.py\"\u001b[0m, line \u001b[1;36m697\u001b[0m, in forward\n",
       "    layer_outputs = \u001b[1;35mdecoder_layer\u001b[0m\u001b[1m(\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\u001b[0m,\n",
       "line \u001b[1;36m1194\u001b[0m, in _call_impl\n",
       "    return \u001b[1;35mforward_call\u001b[0m\u001b[1m(\u001b[0m*input, **kwargs\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/transformers/models/opt/mode\u001b[0m\n",
       "\u001b[32mling_opt.py\"\u001b[0m, line \u001b[1;36m326\u001b[0m, in forward\n",
       "    hidden_states, self_attn_weights, present_key_value = \u001b[1;35mself.self_attn\u001b[0m\u001b[1m(\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\u001b[0m,\n",
       "line \u001b[1;36m1194\u001b[0m, in _call_impl\n",
       "    return \u001b[1;35mforward_call\u001b[0m\u001b[1m(\u001b[0m*input, **kwargs\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/transformers/models/opt/mode\u001b[0m\n",
       "\u001b[32mling_opt.py\"\u001b[0m, line \u001b[1;36m171\u001b[0m, in forward\n",
       "    query_states = \u001b[1;35mself.q_proj\u001b[0m\u001b[1m(\u001b[0mhidden_states\u001b[1m)\u001b[0m * self.scaling\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\"\u001b[0m,\n",
       "line \u001b[1;36m1201\u001b[0m, in _call_impl\n",
       "    result = \u001b[1;35mhook\u001b[0m\u001b[1m(\u001b[0mself, input\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/utils/prune.py\"\u001b[0m, \n",
       "line \u001b[1;36m33\u001b[0m, in __call__\n",
       "    \u001b[1;35msetattr\u001b[0m\u001b[1m(\u001b[0mmodule, self._tensor_name, \u001b[1;35mself.apply_mask\u001b[0m\u001b[1m(\u001b[0mmodule\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/nn/utils/prune.py\"\u001b[0m, \n",
       "line \u001b[1;36m74\u001b[0m, in apply_mask\n",
       "    pruned_tensor = \u001b[1;35mmask.to\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[35morig\u001b[0m.dtype\u001b[1m)\u001b[0m * orig\n",
       "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate \u001b[1;36m16.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m39.41\u001b[0m \n",
       "GiB total capacity; \u001b[1;36m37.94\u001b[0m GiB already allocated; \u001b[1;36m12.50\u001b[0m MiB free; \u001b[1;36m38.10\u001b[0m GiB reserved in total \n",
       "by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory try setting max_split_size_mb to avoid \n",
       "fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "110eb0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_2469/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2085411004.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_2469/2085411004.py'</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">launchers.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">136</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">notebook_launcher</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">133 │   │   │   │   </span>launcher = PrepareForLaunch(function, distributed_type=<span style=\"color: #808000; text-decoration-color: #808000\">\"MULTI_GPU\"</span>) <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">134 │   │   │   │   </span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">135 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Launching training on {</span>num_processes<span style=\"color: #808000; text-decoration-color: #808000\">} GPUs.\"</span>)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>136 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>start_processes(launcher, args=args, nprocs=num_processes, start_me <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">137 │   │   </span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">138 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">139 │   │   │   # No need for a distributed launch otherwise as it's either CPU, GPU or</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/multiprocessing/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">spawn.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">98</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">start_processes</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> context                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">196 │   </span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # Loop on join until it returns True or raises an exception.</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>198 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> context.join():                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">pass</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/multiprocessing/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">spawn.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">60</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">join</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">157 │   │   </span>original_trace = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.error_queues[error_index].get()                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">158 │   │   </span>msg = <span style=\"color: #808000; text-decoration-color: #808000\">\"\\n\\n-- Process %d terminated with the following error:\\n\"</span> % error_in <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">159 │   │   </span>msg += original_trace                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>160 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> ProcessRaisedException(msg, error_index, failed_process.pid)          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">161 </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">SpawnContext</span>(ProcessContext):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ProcessRaisedException: </span>\n",
       "\n",
       "-- Process <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> terminated with the following error:\n",
       "Traceback <span style=\"font-weight: bold\">(</span>most recent call last<span style=\"font-weight: bold\">)</span>:\n",
       "  File \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\"</span>, \n",
       "line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">69</span>, in _wrap\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">fn</span><span style=\"font-weight: bold\">(</span>i, *args<span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/accelerate/utils/launch.py\"</span>,\n",
       "line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">97</span>, in __call__\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">self.launcher</span><span style=\"font-weight: bold\">(</span>*args<span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/tmp/ipykernel_2469/1103207772.py\"</span>, line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, in training_step\n",
       "    accelerate = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Accelerator</span><span style=\"font-weight: bold\">()</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/accelerate/accelerator.py\"</span>, \n",
       "line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">323</span>, in __init__\n",
       "    self.state = <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AcceleratorState</span><span style=\"font-weight: bold\">(</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/accelerate/state.py\"</span>, line \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">169</span>, in __init__\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.cuda.set_device</span><span style=\"font-weight: bold\">(</span>self.device<span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/cuda/__init__.py\"</span>, \n",
       "line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span>, in set_device\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch._C._cuda_setDevice</span><span style=\"font-weight: bold\">(</span>device<span style=\"font-weight: bold\">)</span>\n",
       "  File <span style=\"color: #008000; text-decoration-color: #008000\">\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/cuda/__init__.py\"</span>, \n",
       "line <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">217</span>, in _lazy_init\n",
       "    raise <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RuntimeError</span><span style=\"font-weight: bold\">(</span>\n",
       "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with \n",
       "multiprocessing, you must use the <span style=\"color: #008000; text-decoration-color: #008000\">'spawn'</span> start method\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_2469/\u001b[0m\u001b[1;33m2085411004.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_2469/2085411004.py'\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/accelerate/\u001b[0m\u001b[1;33mlaunchers.py\u001b[0m:\u001b[94m136\u001b[0m in   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mnotebook_launcher\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m133 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlauncher = PrepareForLaunch(function, distributed_type=\u001b[33m\"\u001b[0m\u001b[33mMULTI_GPU\u001b[0m\u001b[33m\"\u001b[0m) \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m134 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m135 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLaunching training on \u001b[0m\u001b[33m{\u001b[0mnum_processes\u001b[33m}\u001b[0m\u001b[33m GPUs.\u001b[0m\u001b[33m\"\u001b[0m)               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m136 \u001b[2m│   │   │   │   \u001b[0mstart_processes(launcher, args=args, nprocs=num_processes, start_me \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m138 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# No need for a distributed launch otherwise as it's either CPU, GPU or\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/multiprocessing/\u001b[0m\u001b[1;33mspawn.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m98\u001b[0m in \u001b[92mstart_processes\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m context                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m196 \u001b[0m\u001b[2m│   \u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Loop on join until it returns True or raises an exception.\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m198 \u001b[2m│   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[95mnot\u001b[0m context.join():                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mpass\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/multiprocessing/\u001b[0m\u001b[1;33mspawn.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m60\u001b[0m in \u001b[92mjoin\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m157 \u001b[0m\u001b[2m│   │   \u001b[0moriginal_trace = \u001b[96mself\u001b[0m.error_queues[error_index].get()                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m158 \u001b[0m\u001b[2m│   │   \u001b[0mmsg = \u001b[33m\"\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m-- Process \u001b[0m\u001b[33m%d\u001b[0m\u001b[33m terminated with the following error:\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m % error_in \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m159 \u001b[0m\u001b[2m│   │   \u001b[0mmsg += original_trace                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m160 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m ProcessRaisedException(msg, error_index, failed_process.pid)          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m161 \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92mSpawnContext\u001b[0m(ProcessContext):                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mProcessRaisedException: \u001b[0m\n",
       "\n",
       "-- Process \u001b[1;36m2\u001b[0m terminated with the following error:\n",
       "Traceback \u001b[1m(\u001b[0mmost recent call last\u001b[1m)\u001b[0m:\n",
       "  File \n",
       "\u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\"\u001b[0m, \n",
       "line \u001b[1;36m69\u001b[0m, in _wrap\n",
       "    \u001b[1;35mfn\u001b[0m\u001b[1m(\u001b[0mi, *args\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/accelerate/utils/launch.py\"\u001b[0m,\n",
       "line \u001b[1;36m97\u001b[0m, in __call__\n",
       "    \u001b[1;35mself.launcher\u001b[0m\u001b[1m(\u001b[0m*args\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/tmp/ipykernel_2469/1103207772.py\"\u001b[0m, line \u001b[1;36m3\u001b[0m, in training_step\n",
       "    accelerate = \u001b[1;35mAccelerator\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/accelerate/accelerator.py\"\u001b[0m, \n",
       "line \u001b[1;36m323\u001b[0m, in __init__\n",
       "    self.state = \u001b[1;35mAcceleratorState\u001b[0m\u001b[1m(\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/accelerate/state.py\"\u001b[0m, line \n",
       "\u001b[1;36m169\u001b[0m, in __init__\n",
       "    \u001b[1;35mtorch.cuda.set_device\u001b[0m\u001b[1m(\u001b[0mself.device\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/cuda/__init__.py\"\u001b[0m, \n",
       "line \u001b[1;36m326\u001b[0m, in set_device\n",
       "    \u001b[1;35mtorch._C._cuda_setDevice\u001b[0m\u001b[1m(\u001b[0mdevice\u001b[1m)\u001b[0m\n",
       "  File \u001b[32m\"/gs/gsfs0/users/asyed/.local/lib/python3.9/site-packages/torch/cuda/__init__.py\"\u001b[0m, \n",
       "line \u001b[1;36m217\u001b[0m, in _lazy_init\n",
       "    raise \u001b[1;35mRuntimeError\u001b[0m\u001b[1m(\u001b[0m\n",
       "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with \n",
       "multiprocessing, you must use the \u001b[32m'spawn'\u001b[0m start method\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_launcher(training_step, args=(), num_processes=4, mixed_precision='bf16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c06da8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bf348a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torch\n",
    "from transformers import AutoTokenizer, OPTForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = load_dataset('c4', 'en', streaming=True)\n",
    "model_text = \"facebook/opt-125m\"\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_text)\n",
    "\n",
    "# Load model with pre-trained head\n",
    "model = OPTForCausalLM.from_pretrained(model_text, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "generator = pipeline('text-generation', model=model_text)\n",
    "\n",
    "calibration_data = []\n",
    "for i, data in enumerate(iter(dataset['train'])):\n",
    "    if i > 128:\n",
    "        break\n",
    "    tokenized = tokenizer.encode(data['text'], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=768)\n",
    "    calibration_data.append(tokenized)\n",
    "calibration_data = torch.squeeze(torch.stack(calibration_data)).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49669afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def inverse_hessian(X, epsilon=0.01):\n",
    "    \"\"\"\n",
    "    Calculate the inverse of a positive-definite matrix using the Cholesky decomposition.\n",
    "    Args:\n",
    "    - X (torch.Tensor): dxn tensor\n",
    "    - epsilon (float): small constant to prevent Hessian from being singular\n",
    "    Returns:\n",
    "    - torch.Tensor: inverted matrix\n",
    "    \"\"\"\n",
    "    X = X.float()\n",
    "    X_T = torch.transpose(X, 0, 1)\n",
    "    identity = torch.eye(X.shape[0], dtype=torch.float32)\n",
    "    H_inv = torch.inverse(2 * (X @ X_T + epsilon * identity))\n",
    "    #H_inv = torch.cholesky(H_inv).T\n",
    "    H_inv = torch.lu(H_inv)[0].T\n",
    "    \n",
    "    return H_inv\n",
    "\n",
    "# W is weights matrix for one layer\n",
    "# H_inv is inverse hessian for one layer\n",
    "# p is proportion of weights to 0\n",
    "# B is lazy block size, low B helps to reduce memory use\n",
    "# Bs is inverse of how often to make masks (e.g. when Bs is 4, make new masks with 20% sparseness every 4 columns)\n",
    "def calculate_mask(W, H_inv, p, B, Bs):\n",
    "    # Get the number of rows and columns in W\n",
    "    d_row, d_col = W.shape\n",
    "    \n",
    "    # Initialize the pruning mask M and block quantization errors E to all zeros\n",
    "    M = torch.zeros(d_row, d_col, dtype=torch.bool)\n",
    "    E = torch.zeros(d_row, B)\n",
    "\n",
    "    # only need to calculate w_square and h_square once\n",
    "    # w_square = torch.square(W)\n",
    "    # h_square = torch.square(H_inv)\n",
    "\n",
    "    # Loop over blocks of columns of W\n",
    "    for i in range(0, d_col, B):\n",
    "        # Loop over columns within a block\n",
    "        for j in range(i, min(i + B, d_col)):\n",
    "            # If j is a multiple of Bs, prune a portion of the weights\n",
    "            if j % Bs == 0:\n",
    "                # Get the mask for the largest (1 - p)% of weights based on squared value and inverse hessian\n",
    "\n",
    "                # prune_values is matrix of w^2/H^(-1)_cc\n",
    "                \n",
    "                w_square_section = torch.square(W[:, j:j+Bs])\n",
    "                h_square_section = torch.square(H_inv[j:j+Bs, j:j+Bs]).diag() # 1 dimensional vector\n",
    "\n",
    "                # print(\"weights squared and h_inv:\")\n",
    "                # print(w_square_section)\n",
    "                # print(h_square_section)\n",
    "\n",
    "                prune_values = w_square_section / h_square_section.unsqueeze(0)\n",
    "                # print(\"prune values: \")\n",
    "                # print(prune_values)\n",
    "\n",
    "                cutoff_value = torch.kthvalue(prune_values, int((1 - p) * d_row), dim=0)[0]\n",
    "                # print(\"cutoff value: \")\n",
    "                # print(cutoff_value)\n",
    "    \n",
    "                # print(\"mask: \")\n",
    "                mask = prune_values > cutoff_value\n",
    "            \n",
    "                M[:, j:j+Bs] = mask\n",
    "\n",
    "            # Calculate the pruning error for this column\n",
    "            E[:, j-i] = W[:, j] / H_inv[j, j]\n",
    "            # Freeze the weights that are not pruned by multiplying by the pruning mask\n",
    "            # Invert mask (~M equivalent to 1 - M)\n",
    "            E[:, j-i] = ~M[:, j] * E[:, j-i]\n",
    "            # Update the weights in this block based on the pruning error and inverse hessian information\n",
    "            W[:, j:i+B] -= torch.ger(E[:, j-i], H_inv[j, j:i+B])\n",
    "        # Update all remaining weights\n",
    "        W[:, i+B:] -= torch.matmul(E, H_inv[i:i+B, i+B:])\n",
    "    \n",
    "    # return mask\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faee467a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = torch.randn(512, 512, dtype=torch.float32)\n",
    "lmbda = 0.1\n",
    "#print(torch.transpose(calibration_data,0,1).shape)\n",
    "H_inv = inverse_hessian(torch.transpose(calibration_data,0,1), lmbda)\n",
    "#H_inv = inverse_hessian(X, lmbda)\n",
    "print(H_inv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e80748e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_inv.isnan().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f75d50",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a6ea75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a04e81",
   "metadata": {},
   "source": [
    "# Prune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5af3ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 768])\n",
      "model.decoder.embed_tokens.weight\n",
      "torch.Size([50272, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a view of a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(name)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(param\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 6\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_hess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 75\u001b[0m, in \u001b[0;36mcalculate_mask\u001b[1;34m(W, H_inv, p, B, Bs)\u001b[0m\n\u001b[0;32m     73\u001b[0m     E[:, j\u001b[38;5;241m-\u001b[39mi] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mM[:, j] \u001b[38;5;241m*\u001b[39m E[:, j\u001b[38;5;241m-\u001b[39mi]\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# Update the weights in this block based on the pruning error and inverse hessian information\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     W[:, j:i\u001b[38;5;241m+\u001b[39mB] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mger(E[:, j\u001b[38;5;241m-\u001b[39mi], H_inv[j, j:i\u001b[38;5;241m+\u001b[39mB])\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Update all remaining weights\u001b[39;00m\n\u001b[0;32m     77\u001b[0m W[:, i\u001b[38;5;241m+\u001b[39mB:] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(E, H_inv[i:i\u001b[38;5;241m+\u001b[39mB, i\u001b[38;5;241m+\u001b[39mB:])\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a view of a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "inv_hess = inverse_hessian(torch.transpose(calibration_data,0,1), 0.2)\n",
    "print(inv_hess.shape)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    param = calculate_mask(param, inv_hess, 0.5, 32, 32)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac281cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, my dog is cute and I love her. I'm a little nervous about her because she's a little bit shy and I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle it. I'm not sure if she's going to be able to handle\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "input2 = tokenizer(\"What the fuck did you just fucking say about me, you little bitch?\", return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "output = model.generate(input1.input_ids, max_length=100, num_return_sequences=1, temperature=0.5, top_p=0.95)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "07a6f8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('pyt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "01ca8e50707366aa71ffa41d1a1f13415d8b38eae741916c87480058f12910d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
